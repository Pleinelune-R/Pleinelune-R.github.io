<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>从Transformer到不同多模态工作</title>
    <link href="/2025/05/10/Transformer%E4%B8%8E%E5%A4%9A%E6%A8%A1%E6%80%81/"/>
    <url>/2025/05/10/Transformer%E4%B8%8E%E5%A4%9A%E6%A8%A1%E6%80%81/</url>
    
    <content type="html"><![CDATA[<h2 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h2><h3 id="Attention"><a href="#Attention" class="headerlink" title="Attention"></a>Attention</h3><p>Attention的结构如下：</p><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250513171957853.png" alt="Attention Structure" style="zoom:67%;" /><p>现在我们拥有Query、Key、Value三个向量矩阵。</p><p>注意力的作用可以看做对查询和键来计算余弦相似性。假设每行代表一个单词，其对于Q、K矩阵的一行Q<sub>1</sub>·K<sub>1</sub> &#x3D; |Q<sub>1</sub>|·|K<sub>1</sub>|·cosθ为一个数。Q<sub>1</sub>和K<sub>1</sub>的方向相同时（即它们的夹角接近0），cosθ 接近1，所以Q<sub>1</sub>·K<sub>1</sub>较大。这表明Q<sub>1</sub>和K<sub>1</sub>很相似。反之，当 Q<sub>1</sub>和K<sub>1</sub>的方向相反时（即它们的夹角接近180度），cos(θ) 接近 -1，所以Q<sub>1</sub>·K<sub>1</sub>较小，甚至为负。这表明Q<sub>1</sub>和K<sub>1</sub>不相似。</p><p>根据每行得出的相似度来计算得分，赋予权重，就完成了一次注意力计算。</p><p>具体来说，其公式为：</p><p>$$<br>\text { Attention }(Q, K, V)&#x3D;\operatorname{softmax}\left(\frac{Q K^{T}}{\sqrt{d_{k}}}\right) V<br>$$<br>也就是计算相似度得分后除以键向量维度的平方根，防止内积过大，使梯度更稳定。然后再进行softmax归一化后输出结果。输出的结果再去乘以V作为最后注意力输出的Z矩阵。</p><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250513172050816.png" alt="Attention Output" style="zoom: 50%; "/><h3 id="Self-Attention"><a href="#Self-Attention" class="headerlink" title="Self-Attention"></a>Self-Attention</h3><p>自注意力，顾名思义就是自己对自己做注意力机制。把我们仅有的初始矩阵X投影成Query、Key、Value三个向量矩阵（实际上是与可学习的矩阵做乘法），之后再做刚才的注意力机制操作。</p><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250513172124423.png" alt="self-attention QKV" style="zoom:50%;" align="center"/><p>自注意力是可以并行计算的，这与RNN逐个重复处理词元相比较，缺少了顺序信息。所以输入中要添加位置编码来注入绝对或相对位置信息。位置编码可以通过学习得到，也可以直接固定得到。例如在Transformer模型里，采用的正余弦位置编码就属于绝对位置编码。它利用正弦和余弦函数对不同位置进行编码，公式如下：</p><p>$$<br>PE_{t}^{(i)} &#x3D;<br>\begin{cases}<br>    \sin(w_{k}t), &amp; \text{if } i &#x3D; 2k\newline<br>    \cos(w_{k}t), &amp; \text{if } i &#x3D; 2k + 1<br>\end{cases}<br>$$<br>这里：<br>$$<br>\begin{cases}<br>    w_{k} &#x3D; \frac{1}{10000^{\frac{2k}{d_{model}}}} \newline<br>    \dot{t} &#x3D; 0, 1, 2, 3, \ldots, \frac{d_{model}}{2} - 1<br>\end{cases}<br>$$<br>把频率设为非常小的参数可以使最后和起始的位置不会靠得太近，sin、cos交替编码可以使其很容易转换成相对位置编码，k是维度。</p><p>假设有个512维度的Embedding，编码可视化就会像下面这样。越往后的位置，频率越小，波长越长，所以不同的t对最终的结果影响不大。</p><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250520155333892.png" alt="Position Encoding Vision" style="zoom:67%;" /><h3 id="Multihead-Self-Attention"><a href="#Multihead-Self-Attention" class="headerlink" title="Multihead Self-Attention"></a>Multihead Self-Attention</h3><p>实际上多头注意力就是多个Self-Attention的连结，拼接起来再去乘以权重矩阵W<sub>0</sub>，每个头可能会关注输入不同的部分。</p><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250513172140809.png" alt="Multihead Self-attention Output" style="zoom:67%;" align="center"/><p>在Transformer Decoder部分你还能看到Maked Multihead Attention，相较于多头注意力增加了一个掩码。包括：</p><ul><li>padding mask：由于输入序列长度不一样，给交短序列后填0，截取较长序列。mask做的就是把这些位置值加上非常大负数，使其softmax后几乎为0，Attention无法注意到。</li><li>causal（sequence） mask：由于Decoder时生成需要是因果的，t时刻后的输出不应该出现在输入内，所以用一个上三角矩阵作用在序列上。</li></ul><h3 id="Add-Normalize"><a href="#Add-Normalize" class="headerlink" title="Add &amp; Normalize"></a>Add &amp; Normalize</h3><p>经过多头注意力的输出Z还要进行Add和Norm两个操作。Add就是加上一个残差块，防止发生退化，而Layer Normalization则负责归一化加快收敛速度。</p><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250512152733146.png" alt="Transformer Encoder" style="zoom: 50%;" align="center" /><p>LN层针对同一样本的不同神经元进行归一化，在NLP中有用，而BN针对同一个batch不同样本同一位置神经元归一化，对NLP的词向量来说没有意义。</p><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250513141203365.png" alt="LN、BN、IN" style="zoom: 50%;" /><h3 id="Feed-Forward-Network"><a href="#Feed-Forward-Network" class="headerlink" title="Feed Forward Network"></a>Feed Forward Network</h3><p>全连接层FFN公式如下：<br>$$<br>F F N(x)&#x3D;\ ReLU (0, x W 1+b 1) W 2+b 2<br>$$<br>假设输入维度<code> [512，512]</code>，通过W<sub>1</sub><code>[512，2048]</code>拓展维度，能够稀疏特征分布，更易被ReLU非线性组合。W<sub>2</sub><code>[2048，512]</code>再去还原维度。</p><h3 id="整体结构"><a href="#整体结构" class="headerlink" title="整体结构"></a>整体结构</h3><p>Transformer整体结构就如下图所示，其中原文由6层Encoder和6层Decoder组成。</p><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250513161026879.png" alt="Transformer Overview" style="zoom:50%;" /><p>整个工作流程大概是</p><ul><li><p>输入Input Embedding（machine learing  eos&gt;）经过6个Encoder生成Z矩阵。</p></li><li><p>输入OutputEmbedding在掩码多头注意力后，QK用Encoder生成的矩阵（第二个Decoder没有做Self-attention而只是Attention），输出所推理的掩码下一个单词。（&lt;begin  -&gt; &lt;begin 机）</p></li></ul><h3 id="Vision-Transformer"><a href="#Vision-Transformer" class="headerlink" title="Vision Transformer"></a>Vision Transformer</h3><p>Transformer在NLP任务上的成功引起了广泛注意，其在视觉上的应用Vision Transformer很快便推出了，该模型由三个模块组成：</p><ul><li>Linear Projection of Flattened Patches</li><li>Transformer Encoder</li><li>MLP Head（最后用于分类）</li></ul><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250512163112813.png" alt="VIT Overview" style="zoom:67%;" /><p>标准的Transformer模块，要求输入的是token序列[num_token, token_dim]，而图像是[H, W, C]。假如我输入的是一个<code>[224，224 ，3]</code>的图像，我可以通过大小为<code>16x16</code>，步长为16，卷积核个数为768的卷积提取特征为<code>[14，14，768]</code>，然后把它平铺成<code>[196，768]</code>大小的二维矩阵，实现Transformer输入格式。</p><p>Linear Projection的操作可以表示为：<br>$$<br>Z0 &#x3D;\ X·W + b<br>$$<br><strong>W</strong> 是可学习的权重矩阵，形状为 <code>[768, d_model]</code>，<strong>b</strong> 是可学习的偏置向量，<strong>Z<sub>0</sub></strong> 是线性投影后的输出矩阵，形状为 <code>[196, d_model]</code>。</p><p>和图片上展示的一样，实际上16 x 16，步长为16的卷积就相当于把这个图片分为196块这个大小的块分别提取特征，这样后面加上位置信息就更有道理。在进入Encoder前，注意要把Position Embedding加入。<br>$$<br>Z0’ &#x3D;\ Z0 + P<br>$$<br><strong>P</strong> 是位置编码矩阵，**Z<sub>0</sub>**是添加位置编码后的矩阵，形状仍为 <code>[196, d_model]</code>。</p><p>原论文中向量处还加入了大小为<code>[1,d_model]</code>的[class]token，这个可训练参数和其他数据拼接在一起。<br>$$<br>\mathbf{Z}_0^{\prime\prime} &#x3D; \left[ \begin{array}{c} \mathbf{C} \newline<br>\mathbf{Z}_0^{\prime} \end{array} \right]<br>$$<br><strong>C</strong> 是 <code>[class]</code> token，<strong>Z<sub>0</sub>′′</strong> 是拼接后的矩阵，形状为 <code>[197, d_model]</code>。</p><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250512165338090.png" alt="Linear Projection of Flattened Patches" style="zoom:67%;" /><p>Transformer Encoder部分不再介绍，MLP层其实接近FFN。</p><p>经过Encoder后输出的仍是<code>[197，d_model]</code>大小的矩阵，我们把首位的[class]token对应的<code>[1,768]</code>调出来，然后通过MLP Head得到最后的分类结果，以免你不知道用哪个位置的token进行后面的MLP分类操作。</p><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250512165902633.png" alt="MLP Head" style="zoom:50%;" /><h2 id="CLIP及其改进"><a href="#CLIP及其改进" class="headerlink" title="CLIP及其改进"></a>CLIP及其改进</h2><h3 id="CLIP简介"><a href="#CLIP简介" class="headerlink" title="CLIP简介"></a>CLIP简介</h3><p>CLIP（Contrastive Language-Image Pre-Training）模型是OPENAI在2021年发布的多模态预训练神经网络，核心是用大量的图像和文本配对数据来预训练对齐。</p><p>CLIP包含的主要部分为：</p><ul><li>Text Encoder：文本-&gt;低维向量</li><li>Image Encoder：图像-&gt;低维向量</li></ul><h3 id="CLIP学习"><a href="#CLIP学习" class="headerlink" title="CLIP学习"></a>CLIP学习</h3><ul><li>优势：出色的zero-shot效果</li><li>劣势：需要大量数据，难以完成复杂任务。</li><li>可学习之处：文本-图像构建正负样本对及点积相似性分类，对下面文章的指导。</li></ul><h4 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h4><p>CLIP使用的Encoder可以是修改后的ResNet50（<strong>注意力池化机制</strong>替代<strong>全局平均池化层</strong>），通过单层的多头QKV注意力，Q基于图像的全局平均池化；也可以是<strong>修改后的ViT</strong>，在Transformer前对Patch+Possition Embedding进行了归一化处理。使用的Test Encoder则是经典的<strong>Transformer</strong>。</p><h4 id="训练过程"><a href="#训练过程" class="headerlink" title="训练过程"></a>训练过程</h4><p>由下图，CLIP实际上通过生成文本、图像的相似度来生成预测 $| I1 |·|T1|·cosθ$，一个文本和对应图片生成n个正样本和其他的负样本。</p><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250510214719681.png" alt="pre-training" style="zoom:67%;" /><p>为了达到更好的效果，不能直接把单个词语与图片输入，而是先做一个Prompt template之后输入。</p><p>Prompt template是指生成提示的可重复的方式，包含一个模板，例如图中的a photo of a {object}。Prompt把下游任务重构为模型熟悉的完形填空模式，可以激活预训练语言模型中的记忆，在few-shot上效果显著。</p><p>推理时把图片特征和文本特征计算相似性，输出分类结果。</p><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250510220213424.png" alt="Prompt Template & Predict" style="zoom:67%;" /><p>由于把文字和图像特征结合学习到了结构化的部分知识，CLIP一定程度上摆脱了categorical label的限制，可以跳出训练时给出的label，例如做物体检测时，你在训练时给出的蓝色玩具、绿色玩具可以被检测为大象玩具、鸭子玩具等等。</p><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250510221154785.png" alt="eg" style="zoom:67%;" /><h3 id="CLIP分割应用"><a href="#CLIP分割应用" class="headerlink" title="CLIP分割应用"></a>CLIP分割应用</h3><h4 id="LSeg"><a href="#LSeg" class="headerlink" title="LSeg"></a>LSeg</h4><p>N个需要分割类别：N x C Text矩阵</p><p>图片：H x W x C 在C维度上相乘 </p><p>结果：H x W x N</p><p>由于分割数据集一般较小，会把CLIP改为有监督训练，计算交叉熵，和CLIP实际上有区别。</p><h4 id="GroupViT"><a href="#GroupViT" class="headerlink" title="GroupViT"></a>GroupViT</h4><p>Image通过ViT的Transformer layer后进行Grouping。（重复此模块并且Group的类别逐渐减少）</p><p>由于是分类任务，最后还是有不同的类别，做Avg Pooling后MLP，和CLIP一样与Text的MLP结果做余弦相似得到Loss。推理分类时再把每个Group的特征和Text的特征做余弦相似，可以知道每个Group对应哪一类。</p><p>由于CLIP特性，很可能分离的Mask正确而类别错误，没有正确学习语义，且最多只能检测到最后一层Group数量的类。</p><img src="C:\Users\ForRiver\AppData\Roaming\Typora\typora-user-images\image-20250518191503395.png" alt="GroupVIT Overview" style="zoom:67%;" /><h2 id="多模态经典论文"><a href="#多模态经典论文" class="headerlink" title="多模态经典论文"></a>多模态经典论文</h2><p>以ViLT论文里展示的四种语言-视觉融合模型来说，CLIP处在B类。A、B两类的参数量更大，计算相似性部分则较轻，复杂情况可能难以得到好的结果。而C、D两种的训练时间长，要求高。</p><p>在多模态任务里，视觉特征是要大于文本特征的，所以Visual Embed应该不是很小，而Modality Interaction如果想处理更复杂的模型，应该也要较大，类似图中C类。</p><p>关于多模态任务使用的Loss，大概有Image Text Contrastive（CLIP中最大化NxN图像-文本对余弦相似度）、Mask Language Modeling（BERT中遮住一个词进行完形填空）、Image Text Matching（二分类判断生成的图像-文本对联合表示是否匹配）</p><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250512002336683.png" alt="four categories of vision-and-language models" style="zoom: 67%;" /><h3 id="ALBEF：Align-before-Fuse-Vision-and-Language-Representation-Learning-with-Momentum-Distillation"><a href="#ALBEF：Align-before-Fuse-Vision-and-Language-Representation-Learning-with-Momentum-Distillation" class="headerlink" title="ALBEF：Align before Fuse: Vision and Language Representation Learning with Momentum Distillation"></a>ALBEF：Align before Fuse: Vision and Language Representation Learning with Momentum Distillation</h3><h4 id="方法总结"><a href="#方法总结" class="headerlink" title="方法总结"></a>方法总结</h4><ul><li>优势：动量蒸馏处理噪声数据，标准化三个loss，训练成本相对亲民。</li><li>劣势：无法做多模态深度融合、难以完成复杂下游任务。</li><li>可学习之处：处理噪声数据、处理负样本太多导致ITM训练困难问题。</li></ul><h4 id="具体内容"><a href="#具体内容" class="headerlink" title="具体内容"></a>具体内容</h4><p>CLIP模型并没有学习更丰富的多模态交互，很多工作也容易对噪声文本进行过拟合。ALBEF旨在解决这些问题。</p><ul><li>Image Encoder：ViT</li><li>Text Encoder：BERT前六层</li><li>Multimodal Encoder：BERT后六层</li></ul><p>相对于CLIP，它的Text Encoder较轻而Modality Encoder较重，有点类似C类。</p><p>Loss：ITC（在多模态融合前先对齐) + ITM（负样本太多直接训练难，选择最接近正样本的负样本） + MLM</p><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250513162024567.png" alt="ALBEF Overview" style="zoom: 67%;" /><p>用动量蒸馏模型给图像-文本对比学习和掩码语言建模生成伪标签，不采用影响训练噪声数据（可能符合但不与Ground Truth相符），扩大有用数据集，one-hot to multi-hot。</p><p>动量蒸馏实际上就是先训练出一个版本的ALBEF，把它作为动量模型，用动量模型输出的伪目标作为额外的监督标准，在原本Loss的基础上和伪目标的Loss进行KL-发散加权组合。同时动量模型作为教师模型也随着ALBEF的训练动态EMA更新参数权重。</p><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250513162108984.png" alt="ITC/MIM examples" style="zoom: 67%;" /><h3 id="VLMo-Unified-Vision-Language-Pre-Training-with-Mixture-of-Modality-Experts"><a href="#VLMo-Unified-Vision-Language-Pre-Training-with-Mixture-of-Modality-Experts" class="headerlink" title="VLMo: Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts"></a>VLMo: Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts</h3><h4 id="方法总结-1"><a href="#方法总结-1" class="headerlink" title="方法总结"></a>方法总结</h4><ul><li>优势：Encoder使用灵活；可以使用单模态数据。</li><li>劣势：训练速度非常慢。</li><li>可学习之处：单模态训练流程优化、单塔模型及MoME。</li></ul><h4 id="具体内容-1"><a href="#具体内容-1" class="headerlink" title="具体内容"></a>具体内容</h4><p>CLIP、ALIGN等双塔模型，Image、Text单独抽取特征互不影响，但在一部分下游任务不如使用Fusion  Encoder的单塔模型。但是单塔模型在检索任务上又不适合大数据集，推理时间非常慢。</p><p>VLMo在Transformer中做了一部分改动：MoME Transformer。在Feed Forward Network中有区分Vision、Language和Vision-Language，在不同模态使用，self-attention可以共享参数。在使用中很灵活，但是由于要存储不同情况下的数据，要forward多次，速度非常慢，自己训练不现实。</p><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250513161725376.png" alt="VLMo Overview" style="zoom:80%;" /><p>训练时分阶段，使用单模态数据集单独预训练Vision、Language，大大减轻需要的多模态数据集。</p><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250513161754752.png" alt="Pre-training" style="zoom:80%;" /><p>使用的Loss也是ITC、ITM、MLM。</p><h3 id="BLIP：Bootstrapping-Language-Image-Pre-training-for-Unified-Vision-Language-Understanding-and-Generation"><a href="#BLIP：Bootstrapping-Language-Image-Pre-training-for-Unified-Vision-Language-Understanding-and-Generation" class="headerlink" title="BLIP：Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation"></a>BLIP：Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation</h3><h4 id="方法总结-2"><a href="#方法总结-2" class="headerlink" title="方法总结"></a>方法总结</h4><ul><li>优势：能处理噪声数据，扩大数据集；可以完成生成任务。</li><li>劣势：训练还是很慢，存在误分类问题（实际上还是CLIP带来的）。</li><li>可学习之处：对训练流程的优化。</li></ul><h4 id="具体内容-2"><a href="#具体内容-2" class="headerlink" title="具体内容"></a>具体内容</h4><p>视觉语言预训练（VLP）模型过去只可以单独做好理解任务或者是生成任务。而且，性能的提高大部分来自于从web上收集的大量的有噪声的数据。</p><p>BLIP提出了多模态混合编码-解码器，可以在三个功能中选择一个运行：</p><ul><li>单模态编码器</li><li>基于图像的文本编码器</li><li>基于图像的文本解码器</li></ul><p>以此来解决原来的模型无法生成任务的问题。</p><p>Image Encoder：VIT</p><p>Text Encoder&#x2F;Decoder：3种不同的用来算ITC、ITM（前两块比起ALBEF只是共享了self-attention参数）、Language Modeling（Decoder用的是Causal Self-Att做因果推理来完成生成任务）</p><p>文本端要做3次Forward，训练效率也偏慢。</p><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250512120226522.png" alt="BLIP-Overview" style="zoom:80%;" /><p>由于BLIP和ALBEF是同一个团队提出的，实际上训练细节差不多。而BLIP也致力于解决网络数据集噪声太多影响训练的情况，提出了Captioner Filter Model。</p><p>把训练好的BLIP模型前两部分拿出来，在无噪声数据集上微调后为Filter，再用微调后的Decoder生成的文本做Captioner补充新的数据集，过滤掉相对不匹配的提高图像文本对的质量。得到的图像文本对再返回去训练第二个BLIP模型。</p><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250512121417258.png" alt="Cap Filter Model" style="zoom:80%;" /><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250512122146514.png" alt="eg" style="zoom:80%;" /><h3 id="CoCa-Contrastive-Captioners-are-Image-Text-Foundation-Models"><a href="#CoCa-Contrastive-Captioners-are-Image-Text-Foundation-Models" class="headerlink" title="CoCa: Contrastive Captioners are Image-Text Foundation Models"></a>CoCa: Contrastive Captioners are Image-Text Foundation Models</h3><h4 id="方法总结-3"><a href="#方法总结-3" class="headerlink" title="方法总结"></a>方法总结</h4><ul><li><p>优势：训练较快，效果非常好，能适应不同任务。</p></li><li><p>劣势：规模史无前例，个人想训练几乎不可能。</p></li><li><p>可学习之处：对速度的优化，Pooling的更改。</p></li></ul><h4 id="具体内容-3"><a href="#具体内容-3" class="headerlink" title="具体内容"></a>具体内容</h4><p>顾名思义，用Contrastive loss 和 Captioning loss训练的模型。CoCa基本上是ALBEF直接的后续工作，跟ALBEF的区别是用了attentional pooling来学习不同任务的特征，以及放弃了ITM Loss。</p><p>另外文本特征端用的都是Decoder，这样一开始就是mask的，可以只做一次forward减少计算量，但是代价是用了巨大的数据集，非常多的参数量，scale远超之前所有的工作。</p><p>Image Encoder和Text Encoder用Contrastive Loss来训练，类似之前的ITC loss，而最后的多模态用的是Captioning Loss，也就是GPT用的Loss，基于交叉熵，用于预测被遮蔽的文本标记。</p><p><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250513164612673.png" alt="CoCa Overview"></p><p>效果和效果图都非常的好，这种类型的成果展示图被很多后来者用上。</p><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250513165158262.png" alt="比例取巧了，但是很吓人" style="zoom:50%;" /><h3 id="BEiT-3：Image-as-a-Foreign-Language-BEiT-Pretraining-for-All-Vision-and-Vision-Language-Tasks"><a href="#BEiT-3：Image-as-a-Foreign-Language-BEiT-Pretraining-for-All-Vision-and-Vision-Language-Tasks" class="headerlink" title="BEiT-3：Image as a Foreign Language: BEiT Pretraining for All Vision and Vision-Language Tasks"></a>BEiT-3：Image as a Foreign Language: BEiT Pretraining for All Vision and Vision-Language Tasks</h3><h4 id="方法总结-4"><a href="#方法总结-4" class="headerlink" title="方法总结"></a>方法总结</h4><ul><li>优势：没有像CoCa一样使用过多数据且均为Public，应用广泛。</li><li>可学习之处：损失函数设计优化</li></ul><h4 id="具体内容-4"><a href="#具体内容-4" class="headerlink" title="具体内容"></a>具体内容</h4><p>BieT的核心就是把所有图片化成Imagelish（和Text形式一样），和Text一起做Mask Modeling（完形填空）当做唯一的损失。而Pre-training部分和他们之前的工作VLMo是完全一致的MOE，可以根据任务来切换FFN。</p><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250513170238887.png" alt="Pre-training" style="zoom: 67%;" /><p>BEiT-3展示了它的不同模块可以完成不同任务，可以单独使用Vision Encoder、Language Encoder、模态融合做推理、分类、生成等，取决你哪种FFN的Encoder以及用不用Decoder。</p><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250513170356130.png" alt="BEiT Overview" style="zoom: 67%;" />]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>Segment Model Compare</title>
    <link href="/2025/04/15/ModelCompare/"/>
    <url>/2025/04/15/ModelCompare/</url>
    
    <content type="html"><![CDATA[<h1 id="SegMamba"><a href="#SegMamba" class="headerlink" title="SegMamba"></a>SegMamba</h1><h2 id="Datasets"><a href="#Datasets" class="headerlink" title="Datasets"></a>Datasets</h2><ul><li>CRC-500：500个3D结直肠癌CT</li><li><strong>BraTS 2023：1251个3D脑MRI</strong></li><li><strong>AIIB2023：120个纤维化肺病CT</strong></li><li>公开数据集加粗，下同</li></ul><h2 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h2><p>SegMamba具有三部分：</p><ul><li>1）具有多个三向空间Mamba（TSMamba）块的3D特征编码器，用于以不同尺度对全局信息进行建模（正向、反向、切片间）</li><li>2）基于卷积层的3D解码器，用于预测分割结果</li><li>3）特征级不确定性估计（FUE）的跳跃连接用于特征增强。</li><li>实际上就是把卷积块换成TSMamba块，逐个尺寸Res跳连。</li><li>主干：k：7x7x7    p：3x3x3    s：2x2x2  –&gt; 48 D&#x2F;2 H&#x2F;2 W&#x2F;2</li><li>代码中实际上把Stem作为下采样第一层，图示少了一次下采样。</li></ul><p><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250417170640235.png" alt="SegMambaOverview"></p><h3 id="TSMamba块"><a href="#TSMamba块" class="headerlink" title="TSMamba块"></a>TSMamba块</h3><ul><li><p>Gated Spatial Convolution<strong>（谨慎学习这块内容，基本没用，代码部分会提到）</strong>:weary:</p><p>类似于门机制的信息传输，在mamba前捕获空间信息</p></li></ul><p><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250421193131000.png" alt="GSC"></p><p>​       </p><ul><li><p>Tri-orientated Mamba</p><p>前向、反向、切片间，对高维特征的全局信息进行建模。</p></li></ul><p><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250421193035107.png" alt="ToM"></p><p><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250421194438637.png" alt="TSMamba"></p><h3 id="Feature-level-Uncertainty-Estimation（谨慎学习这块内容，作者没有给出）"><a href="#Feature-level-Uncertainty-Estimation（谨慎学习这块内容，作者没有给出）" class="headerlink" title="Feature-level Uncertainty Estimation（谨慎学习这块内容，作者没有给出）"></a>Feature-level Uncertainty Estimation<strong>（谨慎学习这块内容，作者没有给出）</strong></h3><p>​       计算通道维度上的平均值，并进行归一化，增强低不确定性特征。</p><p><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250421193241411.png" alt="FUE"></p><h3 id="Loss-Train"><a href="#Loss-Train" class="headerlink" title="Loss &amp; Train"></a>Loss &amp; Train</h3><p>参数：使用交叉熵损失，具有多项式学习率调度器的SGD优化器（初始学习率为1^e-2^，衰减为1^e-5^）。所有数据集运行1000个epoch</p><p>数据增强：加性亮度、伽马、旋转、缩放、镜像和弹性变形。</p><h2 id="Code"><a href="#Code" class="headerlink" title="Code"></a>Code</h2><p><a href="https://github.com/ge-xing/SegMamba">https://github.com/ge-xing/SegMamba</a></p><p>吐槽下作者代码里实际上根本没有点乘，而是直接相加，加的位置也不对，和画的图&#x2F;公式完全是俩玩意。其实就是基本两个3x3+1x1跳连的残差块，外边再1x1之后和原来的输入跳连，和门控不大沾边，浪费好久去看这个结构。git记录也是稍显抽象。然后FUE根本没给，在issue回复很模糊。</p><p><img src="C:\Users\ForRiver\AppData\Roaming\Typora\typora-user-images\image-20250422153207119.png" alt="GSC实际代码"></p><p><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250422153424533.png" alt="git"></p><p>可学习的点：3D切片上ToM的正序反序、切片间顺序思路。</p><h1 id="3D-ResUNet"><a href="#3D-ResUNet" class="headerlink" title="3D-ResUNet"></a>3D-ResUNet</h1><h2 id="Datasets-1"><a href="#Datasets-1" class="headerlink" title="Datasets"></a>Datasets</h2><ul><li>三个非洲爪蟾肾胚胎样本</li></ul><h2 id="Model-1"><a href="#Model-1" class="headerlink" title="Model"></a>Model</h2><p>和标准U-Net基本上一样。这篇比较早结构也比较简单，直接看代码吧。</p><p><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250422211547355.png" alt="3D-ResUNet"></p><h2 id="Code-1"><a href="#Code-1" class="headerlink" title="Code"></a>Code</h2><p>没有找到原论文代码，找到类似的3D-ResUNet</p><p><a href="https://github.com/safamathl/3D-ResUnet/tree/main/net">https://github.com/safamathl/3D-ResUnet/tree/main/net</a></p><p>16→32→64→128，每级包含2-3个卷积层，找到的这个代码通过dilation在3、4层扩大感受野。concat合并通道，特征融合后接3个卷积层进行信息整合。3D相对原来的U-Net在z轴维度上的变化也没有额外处理。</p><p>以图中128+256作为例子，进行信息整合的3个卷积层：</p><table><thead><tr><th>层级</th><th>输入通道</th><th>输出通道</th><th>kernel_size</th><th>stride</th><th>padding</th><th>dilation</th><th>激活函数</th><th>参数量</th><th>计算量（FLOPs）</th></tr></thead><tbody><tr><td>第1层</td><td>128+64&#x3D;192</td><td>128</td><td>3×3×3</td><td>1</td><td>1</td><td>1</td><td>PReLU</td><td>192×128×3³&#x3D;<strong>663,552</strong></td><td>128×192×D×H×W×27</td></tr><tr><td>第2层</td><td>128</td><td>128</td><td>3×3×3</td><td>1</td><td>1</td><td>1</td><td>PReLU</td><td>128×128×3³&#x3D;<strong>442,368</strong></td><td>128×128×D×H×W×27</td></tr><tr><td>第3层</td><td>128</td><td>128</td><td>3×3×3</td><td>1</td><td>1</td><td>2</td><td>PReLU</td><td>128×128×3³&#x3D;<strong>442,368</strong></td><td>128×128×D×H×W×27</td></tr></tbody></table><p>可学习的点：适当的使用dilation，其他当做一种方法和最后我们的结果进行比较就好。</p><h1 id="LISA"><a href="#LISA" class="headerlink" title="LISA"></a>LISA</h1><h2 id="Datasets-2"><a href="#Datasets-2" class="headerlink" title="Datasets"></a>Datasets</h2><ul><li><strong>Semantic Segmentation Dataset</strong></li><li><strong>Vanilla Referring Segmentation Dataset</strong></li><li><strong>Visual Question Answering Dataset</strong></li></ul><h2 id="Model-2"><a href="#Model-2" class="headerlink" title="Model"></a>Model</h2><p>LLM输出掩码，其中分割掩码表示为多边形序列，使分割掩码能够表示为纯文本。</p><p>仅需在现有LLM（如LLaVA）基础上新增一个 <code>&lt;SEG&gt;</code> 标记和简单的解码器结构，避免复杂模型重构。</p><p><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250417203159867.png" alt="LISAOverview"></p><p> 作为触发分割的指令信号，嵌入到LLM的词汇表中。当模型需要输出分割结果时，响应文本中会包含此标记。</p><p>有<SEG>-&gt;大模型理解文本需要提取的内容-&gt;投影层连接文字信息与图片特征信息-&gt;SAM分割-&gt;LLM输入掩码-&gt;叠加掩码生成分割结果</p><p><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250418132430684.png" alt="掩码生成过程"></p><h3 id="Loss-Train-1"><a href="#Loss-Train-1" class="headerlink" title="Loss &amp; Train"></a>Loss &amp; Train</h3><p>Ltxt是用于文本生成的自回归交叉熵损失，Lmask是掩码损失。</p><p><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250418132411758.png" alt="损失"></p><ul><li><p><IMAGE> Can you segment the {class name} in this image?  语义分割数据集，输入图像和目标对象的名称</p></li><li><p><IMAGE>Can you segment {description} in this image？Vanilla参考分割数据集，输入图像和目标对象的显式简短描述</p></li><li><p>视觉问题分类数据集，提高多模态LLM的问题分类（VQA）能力</p><p>训练参数：微调LoRA，Fdec，embed tokens，Ilm head，投影层</p><p>可以学习的点：LLM指导分割任务，LLM掩码处理。</p></li></ul><h2 id="Code-2"><a href="#Code-2" class="headerlink" title="Code"></a>Code</h2><p>也是SAM作为predictor</p><p>可学习的点：结合SAM</p><h1 id="HyperSeg"><a href="#HyperSeg" class="headerlink" title="HyperSeg"></a>HyperSeg</h1><h2 id="Datasets-3"><a href="#Datasets-3" class="headerlink" title="Datasets"></a>Datasets</h2><h3 id="图像分割"><a href="#图像分割" class="headerlink" title="图像分割"></a>图像分割</h3><ul><li><strong>COCO Panoptic</strong></li><li><strong>RefCOCO系列</strong></li><li>COCO-Interactive</li><li><strong>ReasonSeg</strong></li></ul><h3 id="视频分割"><a href="#视频分割" class="headerlink" title="视频分割"></a>视频分割</h3><ul><li><strong>DAVIS 2017</strong></li><li><strong>Ref-Youtube-VOS</strong></li><li><strong>YouTube-VIS 2019</strong></li><li>ReVOS</li></ul><h2 id="Model-3"><a href="#Model-3" class="headerlink" title="Model"></a>Model</h2><p>把Vision Token（VLLM的CLIP）、FVP精细分割后的Fine-grained Token、视觉和文字的Prompt Token输入大模型，LoRA微调。</p><p>Prompt Token、Fine-grained Token和语义识别后的Mask Token一起输入分割预测器。</p><p><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250422162428312.png" alt="HyperSegOverview"></p><h3 id="VLLM"><a href="#VLLM" class="headerlink" title="VLLM"></a>VLLM</h3><p>输入（V，P），由CLIP编码器得到特征f<del>v</del>，投影并与Fine-grained Token、Prompt Token级联得到输出。输出后的Hybrid Entity Reconition的语义增强掩码标记EQ是手动提取的。</p><p><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250422165119843.png" alt="VLLM Output"></p><h3 id="Prompt"><a href="#Prompt" class="headerlink" title="Prompt"></a>Prompt</h3><p>P<del>I</del>：文本提示 P<del>C</del>：图片提示 具体设计看上面的Overview就可以了</p><h3 id="Segmentation-predictor"><a href="#Segmentation-predictor" class="headerlink" title="Segmentation predictor"></a>Segmentation predictor</h3><p>m<del>j</del>是掩码建议，z<del>j</del>是分类得分，e<del>j</del>进针对视频用不上。</p><p><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250422171137406.png" alt="分割结果输出"></p><h3 id="CLIP和FVP区别"><a href="#CLIP和FVP区别" class="headerlink" title="CLIP和FVP区别"></a>CLIP和FVP区别</h3><p>CLIP单层简单结构，对细粒度感知效果不强。</p><p><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250422200535318.png" alt="CLIP/FVP块"></p><h2 id="Train-Loss"><a href="#Train-Loss" class="headerlink" title="Train &amp; Loss"></a>Train &amp; Loss</h2><p>该模型可以使用统一损失L在多个任务上联合训练。L<del>text</del>采用自回归交叉熵，L<del>mask</del>采用BCELoss和DiceLoss，L<del>cls</del>交叉熵来分类，L<del>ins</del>用于视频暂时也用不上。</p><p><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250422171444470.png" alt="Loss"></p><p>可学习的点：细粒度token的加入，prompt token的设计。</p><h1 id="LLMSeg"><a href="#LLMSeg" class="headerlink" title="LLMSeg"></a>LLMSeg</h1><h2 id="Datasets-4"><a href="#Datasets-4" class="headerlink" title="Datasets"></a>Datasets</h2><ul><li><p>981乳腺癌患者CT</p></li><li><p>内部验证集，提供了一小部分具有相似特征的受试者样本，而不是完整的患者数据集，用于验证目的</p></li><li><p><a href="https://github.com/tvseg/MM-LLMRO44%E3%80%82">https://github.com/tvseg/MM-LLMRO44。</a></p></li></ul><h2 id="Model-4"><a href="#Model-4" class="headerlink" title="Model"></a>Model</h2><p>大概思路和上面的LLM指导分割任务差不多，用的3D ResUNet + Llama2-7B-chat + sam</p><p><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250423142533886.png" alt="LLMSeg Overview"></p><p>主要有三个模块：</p><ul><li><p>text prompt tuning</p><p>v<del>n</del>：MxD矩阵，D与LLM的嵌入维度相同，进行随机初始化。</p><p>t &#x3D; v<del>n</del> + 诊断内容[TEXT] + 分割标记[SEG] (LISA也用到这个标记)，输出g<del>NxD</del></p></li><li><p>multimodal interactive alignment</p><p>对齐g与图像嵌入，图像嵌入f<del>l</del>为图像编码器第l层输出，维度H<del>l</del> x W<del>l</del> x S<del>l</del> x C<del>l</del> </p><p>逐层使用线性层对齐L与投影g，然后self-attention与cross-attention进行信息交互（这块就是SAM的双向注意力），最后得到f<del>l</del>^*^</p></li><li><p>CTV delineation</p><p>SAM预测，用CE和Dice来计算损失，softmax出y<del>hat</del></p></li></ul><p><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250423143907830.png" alt="LLMSeg Model"></p><h2 id="Train"><a href="#Train" class="headerlink" title="Train"></a>Train</h2><p>随机裁剪384 x 384 x 128的patch，训练冻结LLM，学习text prompt，对齐时的参数和SAM参数。</p><h2 id="Code-3"><a href="#Code-3" class="headerlink" title="Code"></a>Code</h2><p>代码中ResUnet和上文找到的ResUnet差不多，具体的大小如下：</p><p><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250423165327839.png" alt="ResUNet Param"></p><p>5个下采样 + g<del>NxD</del>信息，为了对齐通道会减去N：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs i">in_channels = feature_size * 4 - (args.n_prompts if args.align_score  else 0 if self.context  else 0)<br></code></pre></td></tr></table></figure><p>Encoder用的monai给的Unetr块，Decoder前四块为加了残差的上采样+卷积，最后一块简单的用转置卷积实现上采样。使用了SAM的双向注意力来交互文字与图片信息。</p><p>可学习的点：不再是直接把text和image连接进sam，two ways attention加在ResUNet跳连部分，接着再去做decoder。</p><h2 id="L-C"><a href="#L-C" class="headerlink" title="L&amp;C"></a>L&amp;C</h2><p>用4个MRI序列做多模态输入（T1、T1增强一组&#x2F;T2、DWI一组互注意力）。</p><p>3DResUnet+（顺序切片间（这样是不是2D更好？））做分割模型，高层使用自适应dilation不减小尺寸扩大感受野。</p><p>Two ways Attention可以加在跳连&#x2F;后处理门控？（加在Encoder、Decoder可能运算过大）</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>DeepS汇报</title>
    <link href="/2025/03/13/DeepS/"/>
    <url>/2025/03/13/DeepS/</url>
    
    <content type="html"><![CDATA[<h1 id="DeepS：通过深度神经网络加速3D质谱成像"><a href="#DeepS：通过深度神经网络加速3D质谱成像" class="headerlink" title="DeepS：通过深度神经网络加速3D质谱成像"></a>DeepS：通过深度神经网络加速3D质谱成像</h1><p><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250405185957411.png" alt="所汇报文献"></p><h2 id="研究目的"><a href="#研究目的" class="headerlink" title="研究目的"></a>研究目的</h2><p>3D-MSI相对于2D来说，可以绘制复杂生物结构的分子分布图。本文提出一种多尺度采样单元的系数采样策略，通过3D稀疏网络重建。并提出DeepS工作流程和3D-SSNet，旨在<strong>在20-30%的采样率</strong>下也能得到和全采样近似的结果，缩短3D MSI技术分析构建的时长。</p><h2 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h2><p><strong>3D阿尔兹海默症小鼠脂质数据集</strong></p><p>PDX胶质母瘤小鼠脑数据集</p><p>小鼠肾脏MALDI数据集</p><h2 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h2><h3 id="稀疏采样策略"><a href="#稀疏采样策略" class="headerlink" title="稀疏采样策略"></a>稀疏采样策略</h3><p>传统方法：集合S中按预定顺序选取1x1大小子集。</p><p>本文方法：定义S由多个采样单元（正方形区域）构成，H x W会调整为H&#x2F;k x W&#x2F;k，不能整除部分填0。而s由人工掩码（稀疏化全采样数据时）和操作掩码决定。</p><p><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250405185957412.png" alt="掩码处理"></p><p><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250406235949369.png" alt="不同k值图像示例"></p><p>所有<strong>未采样化为三通道（0,1,0）</strong>，采样复制值到三通道，以此区分未采样像素和低强度采样像素。</p><p>论文地址未提供randon_mask函数代码。</p><h3 id="3D-SSNet模型"><a href="#3D-SSNet模型" class="headerlink" title="3D-SSNet模型"></a>3D-SSNet模型</h3><p><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250405185957413.png" alt="SSNet模型"></p><p>Generator：稀疏图像重建。</p><p><strong>Unet：</strong>收缩：4x下采样-卷积 扩展：卷积 + 4x上采样。</p><p>Discriminator：区分真实、重建样本。</p><p>步长2的六块4x4卷积层。</p><p><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250405185957414.png" alt="UNet结构"></p><p>Loss：</p><figure class="highlight abnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs abnf"><span class="hljs-attribute">criterion</span> <span class="hljs-operator">=</span> nn.BCELoss()<br><span class="hljs-attribute">criterionL1</span> <span class="hljs-operator">=</span> nn.L1Loss()<br></code></pre></td></tr></table></figure><p><strong>netD:</strong></p><p><strong>errD_real</strong>：判别器对真实图像的判断损失</p><ul><li>使用二元交叉熵（<code>BCELoss</code>），标签为<code>real_label=1</code>。</li><li>目标：最大化判别器对真实图像的识别能力（输出接近1）。</li></ul><p><strong>errD_fake</strong>：判别器对生成图像的判断损失</p><ul><li>同样使用<code>BCELoss</code>，标签为<code>fake_label=0</code>。</li><li>目标：最大化判别器对生成图像的识别能力（输出接近0）。</li></ul><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs ini"><span class="hljs-attr">output</span> = netD(input_real)<br><span class="hljs-attr">errD_real</span> = criterion(output, label)<br><span class="hljs-attr">output</span> = netD(fake.detach())<br><span class="hljs-attr">errD_fake</span> = criterion(output, label)<br><span class="hljs-attr">errD</span> = errD_real + errD_fake<br></code></pre></td></tr></table></figure><p><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250407000048202.png" alt="Ld"></p><p><strong>netG:</strong></p><p><strong>errG_D</strong>：生成器对抗损失</p><ul><li>使用<code>BCELoss</code>，但标签为<code>real_label=1</code>（欺骗判别器）。</li><li>目标：生成图像使判别器输出接近1（误判为真）。</li></ul><p><strong>errG_l1</strong>：L1重建损失</p><ul><li>使用<code>L1Loss</code>，计算生成图像与真实图像的像素级差异。</li><li>目标：约束生成图像与真实图像的结构一致性。</li></ul><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs ini"><span class="hljs-attr">errG_D</span> = criterion(output, label)<br><br>        <br>        <span class="hljs-attr">errG_l1</span> = criterionL1(fake, input_real)<br>        <span class="hljs-attr">errG_l1</span> = errG_l1.mean()<br><br>        <span class="hljs-attr">errG</span> = (<span class="hljs-number">1</span> - wtl2) * errG_D + wtl2 * errG_l1<br></code></pre></td></tr></table></figure><p><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250405185957415.png" alt="Lg"></p><h3 id="训练过程"><a href="#训练过程" class="headerlink" title="训练过程"></a>训练过程</h3><ul><li><p>从集合{1，2，4，8，16}中选择一个值作为每个离子图像的采样单元的大小，生成一个随机的人工掩码，并执行模拟稀疏采样，遮挡80%。为每个epoch生成新的人工掩码，使3DSSNet了解截面的结构特征、适应不同大小的采样单元。</p></li><li><p>计算真实图像和生成图像的判别损失（<code>errD_real</code>&#x2F;<code>errD_fake</code>），反向传播更新<code>netD</code>。</p></li><li><p>生成图像（<code>fake = netG(input_cropped)</code>），计算对抗损失（<code>errG_D</code>）和L1损失（<code>errG_l1</code>），反向传播更新<code>netG</code>。</p></li><li><p>如果有提升则保存参数，每五个epoch学习率衰减20% 。</p></li><li><p><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250408171608959.png" alt="训练过程"></p></li></ul><p>在测试阶段，每个部分只生成一个操作掩码（M#2），由来自相同截面的所有离子图像共享。不需要使用相同的k值，k可以根据偏好设置。在此阶段，使用生成器网络，并丢弃了递归网络。</p><p>为了在数据多样性和数据收集成本之间取得平衡，DeepS利用三个切片的全采样数据作为训练数据集，根据沿着Z轴的等距标准进行选择。剩余的组织切片被稀疏采样并用于测试。</p><p><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250408173223616.png" alt="DeepS总流程"></p><h2 id="训练结果"><a href="#训练结果" class="headerlink" title="训练结果"></a>训练结果</h2><p><strong>Dataset：3D阿尔茨海默氏病小鼠脂质数据集</strong></p><p>通过对采样率的对比实验表明，在模型训练过程中，<strong>采样率越低，重建效果越好</strong>。训练参数默认设置如下：50个训练时期，20%的模拟稀疏采样率，批量大小为8，剩余的组织切片被稀疏地采样以评估具有各种采样单元尺寸和采样比的模型的重建性能。</p><p>在海马区选择一个10 × 10的区域来评估。计算了特定区域的平均质谱，总体平均绝对误差（MAE）为5.861 × 10−6，与原始光谱高度可比。</p><p>固定k值，模型可以有效地恢复不同采样率下的离子图像，几乎所有的离子图像在50%的采样率下都被很好地重建，这由所有离子图像的PSNR值大于30和SSIM值大于0.8来证明。在20%的采样率下，超过90%的离子图像被良好地重建，PSNR和SSIM值分别高于27和0.7。</p><p><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250405185957417.png" alt="不同采样率重建效果"></p><p>固定20%采样率，在k&#x3D;32后PSNR、SSIM数值显著减小。</p><p><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250405185957418.png" alt="不同掩码大小重建效果"></p><p>模型迁移到胶质母细胞瘤PDX小鼠脑模型，该模型对不同质荷比和不同组织切片具有较高的重建精度。</p><p>PDX和正常小鼠脑数据集在空间特征上存在差异，但该模型对两者都具有较好的重建效果。</p><p><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250405185957419.png" alt="不同质荷比重建效果"></p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>用git管理repo</title>
    <link href="/2025/03/06/%E7%94%A8Git%E7%AE%A1%E7%90%86Repo/"/>
    <url>/2025/03/06/%E7%94%A8Git%E7%AE%A1%E7%90%86Repo/</url>
    
    <content type="html"><![CDATA[<p>过去我只简单的使用过git push、git clone以及创建博客时的hexo等操作，为了更加系统性管理我的代码，花点时间学习git时有必要的。本文单纯是复习笔记，更详细的用法可以查看廖雪峰老师的git教程。</p><h2 id="什么是git"><a href="#什么是git" class="headerlink" title="什么是git"></a>什么是git</h2><p>git是由linus开发的一种分布式开发控制系统，可以方便的管理个人或是公司的代码版本，查看每个版本进行的改动。</p><h3 id="集中式？分布式？"><a href="#集中式？分布式？" class="headerlink" title="集中式？分布式？"></a>集中式？分布式？</h3><p>过去人们常用的SVN等集中式管理系统存在一些问题，比如必须联网、需要担心中央服务器的正常运行等等。而在分布式系统中，每个人都有完整的版本库，就不存在上述问题。</p><h2 id="基础操作"><a href="#基础操作" class="headerlink" title="基础操作"></a>基础操作</h2><h3 id="创建repo"><a href="#创建repo" class="headerlink" title="创建repo"></a>创建repo</h3><p>在你所需要创建仓库的地方打开gitbash或cd至你想要创建仓库的地方，用git init来初始化仓库。仓库中出现.git的文件夹说明创建成功。不要轻易的更改.git里的文件。</p><p>当然你也可以从github上git clone一份已经存在的repo，推荐使用ssh方式。</p><h3 id="工作区域与文件状态"><a href="#工作区域与文件状态" class="headerlink" title="工作区域与文件状态"></a>工作区域与文件状态</h3><p>除了你的本地仓库（工作区）、远程仓库（版本库）以外，.git文件夹中还有一个重要的区域——暂存区。你可以把他当做一个小推车，从仓库（工作区）取货时我们把需要的货物一个个git add进小推车里（暂存区），然后用小推车一次性git commit到商店（版本库）中。当然你也可以把商店里不需要的货物git reset HEAD到暂存区，然后再git checkout –file到工作区。当然，git status给你了随时查看工作区文件状态的可能。</p><p>为什么我在git checkout后要写完整代码？因为git checkout和git checkout –是两回事。后面我们切换分支时会讲到git checkout，不过你也有替代方法。</p><h3 id="版本差异与回退"><a href="#版本差异与回退" class="headerlink" title="版本差异与回退"></a>版本差异与回退</h3><p>今天你运了一天货，但是店老板突然告诉你那是修改后的货物清单，他把现在商店里的货物和他的清单git diff HEAD了一下给你看了看差异，其实他还想要你送来的第前100版。为了3500&#x2F;月工资，我忍了，我们知道HEAD表示这一版，HEAD^表示前一版，当然老板说的前100版不会是HEAD^^^^^^….^^，毕竟看到这么多^^你肯定也笑不出来，我们用HEAD~100来表示。</p><p>好在现在git给了我们时间回溯的可能，可以用git log来查看过去我们的存档，<del>用sl来回去打店老板一拳</del>然后git reset 版本号来进行时间回溯。等你回溯完老板不紧不慢打电话给你说：能不能改回你删掉的那版？以你专业人员的素养，你只能微笑着跟他说：有的兄弟有的，像这样的git reflog记录我还有一百条。然后git reset到reflog的版本号去。</p><h3 id="删除与忽略"><a href="#删除与忽略" class="headerlink" title="删除与忽略"></a>删除与忽略</h3><p>使用rm对本地文件进行删除后，你会发现暂存区和版本库内的内容仍然存在，事实上你可以把git add和git commit理解为一种状态，照样可以通过add和commit进行删除文件的更新。</p><p>对于.ignore来说，不建议你自己写这个文档，你可以从github上下载一份对应的.ignore文件，然后根据自己的需求进行小修改。</p><h3 id="远程仓库"><a href="#远程仓库" class="headerlink" title="远程仓库"></a>远程仓库</h3><p>在你的github绑定了ssh以后，可以通过git remote add origin 你的远程仓库（默认命名为origin，当然你也可以取一个新的名字）来绑定远程和本地仓库。此后只需要git pull&#x2F;push origin master来拉取和更新你的远程仓库。</p><h2 id="分支管理"><a href="#分支管理" class="headerlink" title="分支管理"></a>分支管理</h2><h3 id="创建、合并分支"><a href="#创建、合并分支" class="headerlink" title="创建、合并分支"></a>创建、合并分支</h3><p>现在我们的手头有一份工作，但是随意更改已经发布的版本是很危险的，这时候我们需要在一个新的分支进行工作。上文说到git checkout可以切换分支，由于一个指令有两种意思，我倾向于使用git switch -c 分支名来创建一个新的分支进行工作。</p><h3 id="bug分支"><a href="#bug分支" class="headerlink" title="bug分支"></a>bug分支</h3><h3 id="feature分支"><a href="#feature分支" class="headerlink" title="feature分支"></a>feature分支</h3><h3 id="多人协作"><a href="#多人协作" class="headerlink" title="多人协作"></a>多人协作</h3><h3 id="rebase"><a href="#rebase" class="headerlink" title="*rebase"></a>*rebase</h3>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>MSI数据处理流程</title>
    <link href="/2025/01/01/MSI%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%B5%81%E7%A8%8B/"/>
    <url>/2025/01/01/MSI%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%B5%81%E7%A8%8B/</url>
    
    <content type="html"><![CDATA[<h2 id="Version：2-0"><a href="#Version：2-0" class="headerlink" title="Version：2.0"></a>Version：2.0</h2><h3 id="更新了imzML文件转为csv的过程"><a href="#更新了imzML文件转为csv的过程" class="headerlink" title="更新了imzML文件转为csv的过程"></a>更新了imzML文件转为csv的过程</h3><h3 id="添加了Python预处理imzML文件的方法"><a href="#添加了Python预处理imzML文件的方法" class="headerlink" title="添加了Python预处理imzML文件的方法"></a>添加了Python预处理imzML文件的方法</h3><h1 id="一、质谱成像技术"><a href="#一、质谱成像技术" class="headerlink" title="一、质谱成像技术"></a>一、质谱成像技术</h1><p>MSI是一种将质谱分析与空间分辨相结合的分析技术，能够同时获得样品中分子的化学组成信息和空间分布信息，实现对样品表面分子的”可视化”分析。<br>MSI在空间代谢组学中已然成为热门技术，本文介绍的就是对MSI数据处理简要的流程。<br><img src="/img/MSI%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%B5%81%E7%A8%8B/1-1.jpg"></p><h1 id="二、MSI数据"><a href="#二、MSI数据" class="headerlink" title="二、MSI数据"></a>二、MSI数据</h1><h2 id="常用的数据类型有哪些？"><a href="#常用的数据类型有哪些？" class="headerlink" title="常用的数据类型有哪些？"></a>常用的数据类型有哪些？</h2><p>在质谱成像领域内，你大概率会见到的两种文件格式：<br>● Raw文件格式<br>● imzML原始数据文件格式<br>事实上不同厂家的质谱仪器生产出来的Raw质谱数据是不一样的，主流公司的数据格式如下表所示。<br><img src="/img/MSI%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%B5%81%E7%A8%8B/1-2.png"></p><h2 id="Raw和imzML分别含有哪些信息？"><a href="#Raw和imzML分别含有哪些信息？" class="headerlink" title="Raw和imzML分别含有哪些信息？"></a>Raw和imzML分别含有哪些信息？</h2><p>RAW保留更多原始信息：仪器状态日志、诊断信息、原始校准数据等等<br>mzML侧重核心数据：<br><img src="/img/MSI%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%B5%81%E7%A8%8B/1-3.jpg"><br>在mzML文件下，<strong>mzML</strong>标签通常来说明多数与质谱数据无关的参数及过程，<strong>run</strong>中的<strong>spectrum</strong>标签则包含着谱图信息。其中的m&#x2F;z array和intensity array就是最重要的数据。<br>imzML相比元数据包含x、y、z坐标，并且带有一个.ibd的二进制数据文件。从光谱维度，它的每一行对应一个空间位置的完整质谱图；从空间维度，它的每一列固定m&#x2F;z值在所有位置的强度，用于生成我们的离子图像。</p><h2 id="如何从Raw数据转换成imzML数据？"><a href="#如何从Raw数据转换成imzML数据？" class="headerlink" title="如何从Raw数据转换成imzML数据？"></a>如何从Raw数据转换成imzML数据？</h2><p>理论上我们只需要知道扫描斑点数信息，就可以直接从Raw文件中计算出质谱成像所需要的空间坐标信息。<br>本文侧重点在于对imzML文件的预处理，对于Raw文件可以用以下的链接提供的软件来进行Raw to imzML的转换，试着使用他给出的示例文件来进行练习。<br><a href="https://www.ms-imaging.org/imzml/software-tools/raw-to-imzml-converter/">https://www.ms-imaging.org/imzml/software-tools/raw-to-imzml-converter/</a><br>在后续更新中我会将这一步进行时的截图放上。</p><h2 id="MALDI？DESI？"><a href="#MALDI？DESI？" class="headerlink" title="MALDI？DESI？"></a>MALDI？DESI？</h2><p>我们常见到的数据可能由<strong>MALDI</strong>(基质辅助激光解吸电离)和<strong>DESI</strong>(解吸电喷雾电离)两种技术得出。MALDI的空间分辨率高，但有着复杂的样品制备流程以及需要真空电离的条件。DESI则直接在大气压中进行电离，从而减少样品准备过程，但相对的空间分辨率低于MALDI。 </p><h1 id="三、imzML数据预处理-Cardinal"><a href="#三、imzML数据预处理-Cardinal" class="headerlink" title="三、imzML数据预处理(Cardinal)"></a>三、imzML数据预处理(Cardinal)</h1><h2 id="Cardinal包安装"><a href="#Cardinal包安装" class="headerlink" title="Cardinal包安装"></a>Cardinal包安装</h2><p>imzML文件的数据处理有很多不同的方式，在熟悉流程之后，你可以自己编写脚本来实现。<br>本文使用Cardinal包来进行预处理。Cardinal包支持MALDI和DESI的MSI工作，可以对生物样品进行基于质谱实验的统计分析。<br>在开始前，你需要自己安装好RStudio或其他IDE。安装Cardinal包，使用以下命令：</p><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs cmake"><span class="hljs-keyword">if</span> (!require(<span class="hljs-string">&quot;BiocManager&quot;</span>, quietly = <span class="hljs-keyword">TRUE</span>))<br>    <span class="hljs-keyword">install</span>.packages(<span class="hljs-string">&quot;BiocManager&quot;</span>)<br><br>BiocManager::<span class="hljs-keyword">install</span>(<span class="hljs-string">&quot;Cardinal&quot;</span>)<br></code></pre></td></tr></table></figure><p>别忘了把刚安装好的包加载上：</p><figure class="highlight gauss"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs gauss"><span class="hljs-meta"># 加载包</span><br><span class="hljs-keyword">library</span>(Cardinal)<br></code></pre></td></tr></table></figure><p>到这里如果没有报错，说明安装成功了，你可以查看官方文档来进行进一步的了解。</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs stylus"><span class="hljs-function"><span class="hljs-title">browseVignettes</span><span class="hljs-params">(<span class="hljs-string">&quot;Cardinal&quot;</span>)</span></span><br></code></pre></td></tr></table></figure><p>或者参考：<div class="row">    <embed src="/pdf/MSI数据处理流程/Cardinal.pdf" width="100%" height="550" type="application/pdf"></div></p><h2 id="读取imzML文件"><a href="#读取imzML文件" class="headerlink" title="读取imzML文件"></a>读取imzML文件</h2><figure class="highlight clean"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs clean"># 读取continuous数据<br>path_continuous &lt;- file.path (<span class="hljs-string">&quot;C:&quot;</span>, <span class="hljs-string">&quot;Users&quot;</span>, <span class="hljs-string">&quot;你的文件地址&quot;</span>, fsep=<span class="hljs-string">&quot;\\&quot;</span>)<br>mse &lt;- readMSIData(path_continuous)<br></code></pre></td></tr></table></figure><p>我们可以读取一些数据出来查看：</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">mz</span>(mse)[<span class="hljs-number">1</span>:<span class="hljs-number">10</span>] # 提取前<span class="hljs-number">10</span>个mz<br><span class="hljs-attribute">run</span>(mse)[<span class="hljs-number">1</span>:<span class="hljs-number">10</span>]# 提取前<span class="hljs-number">10</span>个批次信息<br><span class="hljs-attribute">featureData</span>(mse)[<span class="hljs-number">1</span>:<span class="hljs-number">10</span>]# 提取前<span class="hljs-number">10</span>个feature信息<br><span class="hljs-attribute">pixelData</span>(mse)[<span class="hljs-number">1</span>:<span class="hljs-number">10</span>]# 提取前<span class="hljs-number">10</span>个像素信息<br><span class="hljs-attribute">plot</span>(mse, pixel=c(<span class="hljs-number">211</span>))# 可视化单个像素，纵轴为强度，横轴为m/z<br><span class="hljs-attribute">plot</span>(mse, pixel=c(<span class="hljs-number">211</span>, <span class="hljs-number">611</span>))# 可视化多个像素，纵轴为强度，横轴为m/z<br><span class="hljs-attribute">image</span>(mse,mz=<span class="hljs-number">1200</span>)# 单m/z空间分布可视化，注意此处m/z可能在你的数据中不存在，选取你数据中有的m/z，可以运行后参考RStudio给出的提示<br></code></pre></td></tr></table></figure><p>大致了解你手中的imzML数据后，可以开始进行数据预处理了。</p><h2 id="预处理流程"><a href="#预处理流程" class="headerlink" title="预处理流程"></a>预处理流程</h2><h3 id="高斯平滑去噪"><a href="#高斯平滑去噪" class="headerlink" title="高斯平滑去噪"></a>高斯平滑去噪</h3><p>高斯平滑可以减少随机噪声，提高信噪比。</p><figure class="highlight oxygene"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs oxygene">mse_smoothed &lt;- smoothSignal(mse,<br>                            <span class="hljs-keyword">method</span> = &quot;<span class="hljs-title function_">gaussian</span>&quot;,<br>                            <span class="hljs-title function_">window</span> = 5,<br>                            <span class="hljs-title function_">sd</span> = 2,<br>                            <span class="hljs-title function_">units</span> = &quot;<span class="hljs-title function_">ppm</span>&quot;)<br></code></pre></td></tr></table></figure><p>调整大小适中的窗口，可以有效去噪同时保留峰形</p><h3 id="谱对齐"><a href="#谱对齐" class="headerlink" title="谱对齐"></a>谱对齐</h3><p>由于仪器采集时条件不可能完全一致，产生一些偏移是必然的。初步对齐可以校正不同空间位置的质量偏移。</p><figure class="highlight jboss-cli"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs jboss-cli">mse_aligned &lt;- peakAlign<span class="hljs-params">(mse_smoothed,</span><br><span class="hljs-params">                         <span class="hljs-attr">tolerance</span> = 1500,</span><br><span class="hljs-params">                         <span class="hljs-attr">units</span> = &quot;ppm&quot;,</span><br><span class="hljs-params">                         <span class="hljs-attr">BPPARAM</span> = MulticoreParam()</span>)<br></code></pre></td></tr></table></figure><p>增大tolerance可提高匹配率，但可能引入错误匹配</p><h3 id="TIC标准化"><a href="#TIC标准化" class="headerlink" title="TIC标准化"></a>TIC标准化</h3><p>TIC标准化可以校正不同像素点之间的整体强度差异。</p><figure class="highlight oxygene"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs oxygene">mse_normalized &lt;- normalize(mse_aligned,<br>                           <span class="hljs-keyword">method</span> = &quot;<span class="hljs-title function_">tic</span>&quot;,<br>                           <span class="hljs-title function_">scale</span> = <span class="hljs-title function_">TRUE</span>)<br></code></pre></td></tr></table></figure><h3 id="peakpick选峰"><a href="#peakpick选峰" class="headerlink" title="peakpick选峰"></a>peakpick选峰</h3><p>一般来说，我们需要选取显著的质谱峰，减少数据维度。</p><figure class="highlight oxygene"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs oxygene">mse_peaks &lt;- peakPick(mse_normalized, <span class="hljs-keyword">method</span>=&quot;<span class="hljs-title function_">filter</span>&quot;, <span class="hljs-title function_">SNR</span>=5)<br></code></pre></td></tr></table></figure><p>增大SNR可减少假阳性，但可能丢失低丰度峰<br>调整peakWidth可适应不同的峰形特征</p><h3 id="峰合并"><a href="#峰合并" class="headerlink" title="峰合并"></a>峰合并</h3><p>合并相近的峰以减少冗余。</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs routeros">mse_done &lt;- bin(mse_peaks,<span class="hljs-attribute">spectra</span>=<span class="hljs-string">&quot;intensity&quot;</span>,index=&quot;mz&quot;,method=&quot;linear&quot;, <span class="hljs-attribute">resolution</span>=5, <span class="hljs-attribute">units</span>=<span class="hljs-string">&quot;ppm&quot;</span>)<br></code></pre></td></tr></table></figure><p>改变resolution可改变合并的精度，但可能丢失一些峰。需要你根据实际情况调整。</p><h3 id="查看处理结果"><a href="#查看处理结果" class="headerlink" title="查看处理结果"></a>查看处理结果</h3><p>现在，你可以使用image函数来比较预处理前后的图像结果。</p><h3 id="输出"><a href="#输出" class="headerlink" title="输出"></a>输出</h3><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs stylus"><span class="hljs-selector-id">#save</span><br>imzfile &lt;- <span class="hljs-built_in">tempfile</span>(fileext=<span class="hljs-string">&quot;你的文件名.imzML&quot;</span>)<br><span class="hljs-function"><span class="hljs-title">writeMSIData</span><span class="hljs-params">(mse_done, imzfile)</span></span><br>list<span class="hljs-selector-class">.files</span>(imzfile)<br></code></pre></td></tr></table></figure><h1 id="四、imzML转可处理文件"><a href="#四、imzML转可处理文件" class="headerlink" title="四、imzML转可处理文件"></a>四、imzML转可处理文件</h1><p>我们保存了预处理后的imzML文件，要进行后续的训练分析，则需要把这些文件转换为另一些可处理的文件。<br>这里我用了刘思扬师兄的Python代码来完成。</p><h2 id="加载需要的库"><a href="#加载需要的库" class="headerlink" title="加载需要的库"></a>加载需要的库</h2><figure class="highlight elm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs elm"><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><span class="hljs-title">from</span> scipy <span class="hljs-keyword">import</span> sparse<br><span class="hljs-title">from</span> scipy.ndimage <span class="hljs-keyword">import</span> gaussian_filter1d<br><span class="hljs-title">from</span> pyimzml.<span class="hljs-type">ImzMLParser</span> <span class="hljs-keyword">import</span> ImzMLParser<br><span class="hljs-title">from</span> pyimzml.<span class="hljs-type">ImzMLWriter</span> <span class="hljs-keyword">import</span> ImzMLWriter<br><span class="hljs-keyword">import</span> csv<br></code></pre></td></tr></table></figure><h2 id="设置路径及读取imzML文件"><a href="#设置路径及读取imzML文件" class="headerlink" title="设置路径及读取imzML文件"></a>设置路径及读取imzML文件</h2><p>首先要读取我们处理好的imzML文件，并设置好保存路径。</p><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs makefile"><span class="hljs-comment"># Define file paths</span><br>input_path = &#x27;/你的文件路径/xx.imzML&#x27;<br>output_path = &#x27;/你的保存路径/xx.imzML&#x27;<br>output_dir = <span class="hljs-string">&quot;/你的保存路径&quot;</span><br>sorted_peaklist_file = /你的peaklist保存路径/xx.txt&#x27;<br><br><span class="hljs-comment"># Create output directory if it doesn&#x27;t exist</span><br>if not os.path.exists(output_dir):<br>    os.makedirs(output_dir)<br><br>def __init__(self, mz, intensity):<br>        self.mz = mz<br>        self.intensity = intensity<br><br><span class="hljs-comment"># Process IMZML file</span><br>parser = ImzMLParser(input_path)<br>coordinates = []<br>mz_values_list = []<br>intensity_values_list = []<br></code></pre></td></tr></table></figure><h2 id="提取质谱峰数据并保存到csv文件"><a href="#提取质谱峰数据并保存到csv文件" class="headerlink" title="提取质谱峰数据并保存到csv文件"></a>提取质谱峰数据并保存到csv文件</h2><figure class="highlight scss"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs scss">def <span class="hljs-built_in">extract_imzml_peaks</span>(imzml_file, output_dir):<br>    parser = <span class="hljs-built_in">ImzMLParser</span>(imzml_file)<br>    for i, (x, y, z) in <span class="hljs-built_in">enumerate</span>(parser.coordinates, start=<span class="hljs-number">1</span>):<br>        mzs, intensities = parser.<span class="hljs-built_in">getspectrum</span>(i - <span class="hljs-number">1</span>)<br>        output_filename = os.path.<span class="hljs-built_in">join</span>(output_dir, f<span class="hljs-string">&quot;peaks_&#123;i:05d&#125;.csv&quot;</span>)<br><br>        with <span class="hljs-built_in">open</span>(output_filename, <span class="hljs-string">&#x27;w&#x27;</span>, newline=<span class="hljs-string">&#x27;&#x27;</span>) as csvfile:<br>            csvwriter = csv.<span class="hljs-built_in">writer</span>(csvfile)<br>            csvwriter.<span class="hljs-built_in">writerow</span>([<span class="hljs-string">&#x27;mz&#x27;</span>, <span class="hljs-string">&#x27;intensity&#x27;</span>])<br>            for mz, intensity in <span class="hljs-built_in">zip</span>(mzs, intensities):<br>                csvwriter.<span class="hljs-built_in">writerow</span>([mz, intensity])<br><br><span class="hljs-built_in">extract_imzml_peaks</span>(output_path, output_dir)<br></code></pre></td></tr></table></figure><h2 id="保存位置信息到csv文件"><a href="#保存位置信息到csv文件" class="headerlink" title="保存位置信息到csv文件"></a>保存位置信息到csv文件</h2><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs routeros"><span class="hljs-comment"># Save coordinates to CSV</span><br>imzml = ImzMLParser(output_path)<br>coordinates = np.array(imzml.coordinates)[:, :2]<br>np.savetxt(<span class="hljs-string">&#x27;/Users/ForRiver/OneDrive/Desktop/Pre/BIONET/preprocess/compare-neg/cood-gz4_5.csv&#x27;</span>, coordinates, <span class="hljs-attribute">delimiter</span>=<span class="hljs-string">&#x27;,&#x27;</span>)<br></code></pre></td></tr></table></figure><h2 id="csv文件根据坐标去零"><a href="#csv文件根据坐标去零" class="headerlink" title="csv文件根据坐标去零"></a>csv文件根据坐标去零</h2><figure class="highlight maxima"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs maxima"># Read <span class="hljs-built_in">sparse</span> data <span class="hljs-keyword">and</span> coordinates<br>data0 = <span class="hljs-built_in">sparse</span>.load_npz(&#x27;/Users/ForRiver/OneDrive/Desktop/Pre/BIONET/preprocess/<span class="hljs-built_in">compare</span>-neg/gz4_5.npz&#x27;).toarray()<br>data = data0[<span class="hljs-number">1</span>:, :]<br>coordinates = <span class="hljs-built_in">np</span>.loadtxt(&#x27;/Users/ForRiver/OneDrive/Desktop/Pre/BIONET/preprocess/<span class="hljs-built_in">compare</span>-neg/cood-gz4_5.csv&#x27;, delimiter=&#x27;,&#x27;)<br><br>x, y = coordinates[:, <span class="hljs-number">0</span>].astype(int), coordinates[:, <span class="hljs-number">1</span>].astype(int)<br>output_mat = <span class="hljs-built_in">np</span>.zeros((<span class="hljs-built_in">np</span>.<span class="hljs-built_in">max</span>(y) + <span class="hljs-number">1</span>, <span class="hljs-built_in">np</span>.<span class="hljs-built_in">max</span>(x) + <span class="hljs-number">1</span>, data.shape[<span class="hljs-number">1</span>]))<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(len(data)):<br>    output_mat[y[i], x[i]] = data[i]<br><br>output_mat = <span class="hljs-built_in">np</span>.<span class="hljs-built_in">array</span>([list(output_mat[i]) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(len(output_mat)) <span class="hljs-keyword">if</span> <span class="hljs-built_in">np</span>.<span class="hljs-built_in">sum</span>(output_mat[i]) != <span class="hljs-number">0</span>])<br>output_mat = <span class="hljs-built_in">np</span>.<span class="hljs-built_in">array</span>([list(output_mat[:, i]) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(len(output_mat[<span class="hljs-number">0</span>])) <span class="hljs-keyword">if</span> <span class="hljs-built_in">np</span>.<span class="hljs-built_in">sum</span>(output_mat[:, i]) != <span class="hljs-number">0</span>])<br>data = <span class="hljs-built_in">sparse</span>.bsr_matrix(output_mat.reshape(output_mat.shape[<span class="hljs-number">0</span>] * output_mat.shape[<span class="hljs-number">1</span>], output_mat.shape[<span class="hljs-number">2</span>]))<br><span class="hljs-built_in">sparse</span>.save_npz(&#x27;/Users/ForRiver/OneDrive/Desktop/Pre/BIONET/preprocess/<span class="hljs-built_in">compare</span>-neg/gz4_5_%d_%d.npz&#x27; <span class="hljs-symbol">%</span> (output_mat.shape[<span class="hljs-number">0</span>], output_mat.shape[<span class="hljs-number">1</span>]), data)<br><br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Data has been saved.&quot;</span>)<br></code></pre></td></tr></table></figure><p>这样，我们就保存好了需要的可处理文件。你可以在文件夹中找到一系列的peaks.csv文件、坐标文件和peaklist。</p><h1 id="五、Python进行预处理全流程"><a href="#五、Python进行预处理全流程" class="headerlink" title="五、Python进行预处理全流程"></a>五、Python进行预处理全流程</h1><p>这部分的代码由组里提供，包含了上述的全流程。你可以更换所有的path来进行处理，具体不再解释。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><span class="hljs-keyword">from</span> scipy <span class="hljs-keyword">import</span> sparse<br><span class="hljs-keyword">from</span> scipy.ndimage <span class="hljs-keyword">import</span> gaussian_filter1d<br><span class="hljs-keyword">from</span> pyimzml.ImzMLParser <span class="hljs-keyword">import</span> ImzMLParser<br><span class="hljs-keyword">from</span> pyimzml.ImzMLWriter <span class="hljs-keyword">import</span> ImzMLWriter<br><span class="hljs-keyword">import</span> csv<br><br><span class="hljs-comment"># Define file paths</span><br>input_path = <span class="hljs-string">&#x27;/Users/ForRiver/OneDrive/Desktop/Pre/BIONET/preprocess/slice_1.imzML&#x27;</span><br>output_path = <span class="hljs-string">&#x27;/Users/ForRiver/OneDrive/Desktop/Pre/BIONET/preprocess/compare-neg/gz4_5.imzML&#x27;</span><br>output_dir = <span class="hljs-string">&quot;/Users/ForRiver/OneDrive/Desktop/Pre/BIONET/preprocess/compare-neg/export-gz4_5/&quot;</span><br>sorted_peaklist_file = <span class="hljs-string">&#x27;/Users/ForRiver/OneDrive/Desktop/Pre/BIONET/preprocess/compare-neg/peaklist-neg-gz4_5-sort.txt&#x27;</span><br><br><span class="hljs-comment"># Create output directory if it doesn&#x27;t exist</span><br><span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> os.path.exists(output_dir):<br>    os.makedirs(output_dir)<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">SpectrumProcessor</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, mz, intensity</span>):<br>        <span class="hljs-variable language_">self</span>.mz = mz<br>        <span class="hljs-variable language_">self</span>.intensity = intensity<br><br><span class="hljs-meta">    @staticmethod</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">peak_picking</span>(<span class="hljs-params">mz_array, intensity_array, snr=<span class="hljs-number">1</span>, sigma=<span class="hljs-number">2.0</span></span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        Identify peaks in the intensity array based on a signal-to-noise ratio (SNR) threshold.</span><br><span class="hljs-string"></span><br><span class="hljs-string">        :param mz_array: Array of m/z values.</span><br><span class="hljs-string">        :param intensity_array: Array of intensity values corresponding to the m/z values.</span><br><span class="hljs-string">        :param snr: Signal-to-noise ratio threshold for peak detection.</span><br><span class="hljs-string">        :param sigma: Standard deviation for Gaussian kernel used in smoothing.</span><br><span class="hljs-string">        :return: Arrays of m/z values and intensities for detected peaks.</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-comment"># Smooth the intensity array to reduce noise</span><br>        smoothed_intensity = gaussian_filter1d(intensity_array, sigma=sigma)<br><br>        <span class="hljs-comment"># Calculate the mean and standard deviation of the smoothed intensity array</span><br>        mean_intensity = np.mean(smoothed_intensity)<br>        std_intensity = np.std(smoothed_intensity)<br><br>        <span class="hljs-comment"># Identify peaks where intensity is greater than mean + snr * std</span><br>        peaks = smoothed_intensity &gt; (mean_intensity + snr * std_intensity)<br><br>        <span class="hljs-comment"># Return the m/z values and intensities of the detected peaks</span><br>        <span class="hljs-keyword">return</span> mz_array[peaks], intensity_array[peaks]<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">binning</span>(<span class="hljs-params">self, Da</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        Binning algorithm for spectra.</span><br><span class="hljs-string">        :return: 1D numpy array, binned spectrum.</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        bin_size = <span class="hljs-built_in">int</span>(Da / (<span class="hljs-variable language_">self</span>.mz[<span class="hljs-number">1</span>] - <span class="hljs-variable language_">self</span>.mz[<span class="hljs-number">0</span>]))<br>        bin_spectrum = np.mean(<br>            np.pad(<span class="hljs-variable language_">self</span>.intensity, (<span class="hljs-number">0</span>, bin_size - <span class="hljs-built_in">len</span>(<span class="hljs-variable language_">self</span>.intensity) % bin_size), mode=<span class="hljs-string">&#x27;constant&#x27;</span>, constant_values=<span class="hljs-number">0</span>)<br>            .reshape(-<span class="hljs-number">1</span>, bin_size),<br>            axis=<span class="hljs-number">1</span>)<br><br>        mz_values1 = <span class="hljs-variable language_">self</span>.mz[::bin_size]<br>        <span class="hljs-keyword">return</span> mz_values1, bin_spectrum<br><br><br><span class="hljs-comment"># Process IMZML file</span><br>parser = ImzMLParser(input_path)<br>coordinates = []<br>mz_values_list = []<br>intensity_values_list = []<br><br><span class="hljs-keyword">for</span> idx, (x, y, z) <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(parser.coordinates):<br>    mz_array, intensity_array = parser.getspectrum(idx)<br>    mz_peaks, intensity_peaks = SpectrumProcessor.peak_picking(mz_array, intensity_array)<br>    coordinates.append((x, y, z))<br>    <span class="hljs-built_in">print</span>(idx)<br>    mz_values_list.append(mz_peaks)<br>    intensity_values_list.append(intensity_peaks)<br><br><span class="hljs-comment"># Write processed data to new IMZML file</span><br><span class="hljs-keyword">with</span> ImzMLWriter(output_path) <span class="hljs-keyword">as</span> writer:<br>    <span class="hljs-keyword">for</span> idx, (mz_peaks, intensity_peaks) <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(<span class="hljs-built_in">zip</span>(mz_values_list, intensity_values_list)):<br>        x, y, z = coordinates[idx]<br>        <span class="hljs-built_in">print</span>(idx)<br>        writer.addSpectrum(mz_peaks, intensity_peaks, (x, y, z))<br><br><br><span class="hljs-comment"># Extract peaks to CSV files</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">extract_imzml_peaks</span>(<span class="hljs-params">imzml_file, output_dir</span>):<br>    parser = ImzMLParser(imzml_file)<br>    <span class="hljs-keyword">for</span> i, (x, y, z) <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(parser.coordinates, start=<span class="hljs-number">1</span>):<br>        mzs, intensities = parser.getspectrum(i - <span class="hljs-number">1</span>)<br>        output_filename = os.path.join(output_dir, <span class="hljs-string">f&quot;peaks_<span class="hljs-subst">&#123;i:05d&#125;</span>.csv&quot;</span>)<br><br>        <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(output_filename, <span class="hljs-string">&#x27;w&#x27;</span>, newline=<span class="hljs-string">&#x27;&#x27;</span>) <span class="hljs-keyword">as</span> csvfile:<br>            csvwriter = csv.writer(csvfile)<br>            csvwriter.writerow([<span class="hljs-string">&#x27;mz&#x27;</span>, <span class="hljs-string">&#x27;intensity&#x27;</span>])<br>            <span class="hljs-keyword">for</span> mz, intensity <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(mzs, intensities):<br>                csvwriter.writerow([mz, intensity])<br><br><br>extract_imzml_peaks(output_path, output_dir)<br><br><span class="hljs-comment"># Save coordinates to CSV</span><br>imzml = ImzMLParser(output_path)<br>coordinates = np.array(imzml.coordinates)[:, :<span class="hljs-number">2</span>]<br>np.savetxt(<span class="hljs-string">&#x27;/Users/ForRiver/OneDrive/Desktop/Pre/BIONET/preprocess/compare-neg/cood-gz4_5.csv&#x27;</span>, coordinates, delimiter=<span class="hljs-string">&#x27;,&#x27;</span>)<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">peak_filter</span>(<span class="hljs-params">PATH, min_frequency=<span class="hljs-number">0.005</span>, <span class="hljs-built_in">type</span>=<span class="hljs-string">&#x27;MATLAB&#x27;</span>, file=<span class="hljs-string">&#x27; &#x27;</span></span>):<br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">type</span> == <span class="hljs-string">&#x27;MATLAB&#x27;</span>:<br>        path = PATH<br><br>        m = <span class="hljs-built_in">len</span>(os.listdir(path))<br>        mz = []<br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, m + <span class="hljs-number">1</span>):<br>            data = pd.read_csv(path + <span class="hljs-built_in">str</span>(i), header=<span class="hljs-literal">None</span>)<br>            data = data.values<br>            mz.extend(<span class="hljs-built_in">list</span>(data[:, <span class="hljs-number">0</span>]))<br>        mzs = <span class="hljs-built_in">sorted</span>(<span class="hljs-built_in">set</span>(np.around(mz, <span class="hljs-number">4</span>)))<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;all mz number is &#x27;</span>, <span class="hljs-built_in">len</span>(mzs))<br>        dict_total = &#123;&#125;<br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> mzs:<br>            dict_total[i] = <span class="hljs-number">0</span><br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(m):<br>            data = pd.read_csv(path, <span class="hljs-built_in">str</span>(i + <span class="hljs-number">1</span>), header=<span class="hljs-literal">None</span>)<br>            data = data.values<br>            _u = np.around(data[:, <span class="hljs-number">0</span>], <span class="hljs-number">4</span>)  <span class="hljs-comment"># 4 significant digits</span><br>            <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> _u:<br>                dict_total[j] += <span class="hljs-number">1</span><br><br>        i = <span class="hljs-number">0</span><br>        peak_list = []<br>        <span class="hljs-keyword">for</span> key, value <span class="hljs-keyword">in</span> dict_total.items():<br>            <span class="hljs-keyword">if</span> value &gt; m * min_frequency:<br>                i += <span class="hljs-number">1</span><br>                peak_list.append(key)<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;filter peak is &#x27;</span>, <span class="hljs-built_in">len</span>(peak_list))<br>        np.savetxt(<span class="hljs-string">&#x27;peak_list3&#x27;</span>, peak_list)<br><br>        output_mat = np.zeros((m, <span class="hljs-built_in">len</span>(peak_list)))<br><br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(m):<br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;processing &#x27;</span>, i, <span class="hljs-string">&#x27;piex&#x27;</span>)<br>            data = pd.read_csv(path, <span class="hljs-built_in">str</span>(i + <span class="hljs-number">1</span>), header=<span class="hljs-literal">None</span>)<br>            data = data.values<br>            _u = np.around(data[:, <span class="hljs-number">0</span>], <span class="hljs-number">4</span>)  <span class="hljs-comment"># 4 significant digits</span><br>            <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(_u)):<br>                <span class="hljs-keyword">if</span> _u[j] <span class="hljs-keyword">in</span> peak_list:<br>                    output_mat[i, peak_list.index(_u[j])] = data[j, <span class="hljs-number">1</span>]<br>        <span class="hljs-keyword">return</span> output_mat<br><br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">type</span> == <span class="hljs-string">&#x27;R&#x27;</span>:<br>        path = PATH<br><br>        m = <span class="hljs-built_in">len</span>(os.listdir(path))<br>        mz = []<br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, m):<br>            <span class="hljs-keyword">if</span> i % <span class="hljs-number">100</span> == <span class="hljs-number">0</span>:<br>                <span class="hljs-built_in">print</span>(i)<br>            k = <span class="hljs-string">&#x27;&#123;:0&gt;5d&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(i)  <span class="hljs-comment"># &#x27;&#123;:0&gt;5d&#125;&#x27;</span><br>            data = pd.read_csv(path + file + k + <span class="hljs-string">&#x27;.csv&#x27;</span>, header=<span class="hljs-literal">None</span>)<br>            data = data.values[<span class="hljs-number">1</span>:]<br>            mz.extend(<span class="hljs-built_in">list</span>(data[:, <span class="hljs-number">0</span>]))<br>        mz = [<span class="hljs-built_in">float</span>(x) <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> mz]<br>        mzs = <span class="hljs-built_in">list</span>(<span class="hljs-built_in">set</span>(mz))<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;all mz number is &#x27;</span>, <span class="hljs-built_in">len</span>(mzs))<br>        dict_total = &#123;&#125;<br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> mzs:<br>            dict_total[i] = <span class="hljs-number">0</span><br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, m):<br>            <span class="hljs-keyword">if</span> i % <span class="hljs-number">100</span> == <span class="hljs-number">0</span>:<br>                <span class="hljs-built_in">print</span>(i)<br>            k = <span class="hljs-string">&#x27;&#123;:0&gt;5d&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(i)  <span class="hljs-comment"># &#123;:0&gt;5d&#125;</span><br>            data = pd.read_csv(path + file + k + <span class="hljs-string">&#x27;.csv&#x27;</span>, header=<span class="hljs-literal">None</span>)<br>            data = data.values[<span class="hljs-number">1</span>:]<br>            dataint = [<span class="hljs-built_in">float</span>(x) <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> data[:, <span class="hljs-number">0</span>]]<br>            dataint2 = [<span class="hljs-built_in">float</span>(x) <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> data[:, <span class="hljs-number">1</span>]]<br>            _u = dataint<br>            threhold = np.percentile(dataint2, <span class="hljs-number">95</span>) * <span class="hljs-number">0.001</span><br>            <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(_u)):<br>                <span class="hljs-keyword">if</span> dataint2[j] &gt;= threhold:<br>                    dict_total[_u[j]] += <span class="hljs-number">1</span><br><br>        i = <span class="hljs-number">0</span><br>        peak_list = []<br>        <span class="hljs-keyword">for</span> key, value <span class="hljs-keyword">in</span> dict_total.items():<br>            <span class="hljs-keyword">if</span> value &gt; m * min_frequency:<br>                i += <span class="hljs-number">1</span><br>                peak_list.append(key)<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;after filter peak is &#x27;</span>, <span class="hljs-built_in">len</span>(peak_list))<br><br>        output_mat = np.zeros((m, <span class="hljs-built_in">len</span>(peak_list)))<br><br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, m):<br>            k = <span class="hljs-string">&#x27;&#123;:0&gt;5d&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(i)<br>            <span class="hljs-keyword">if</span> i % <span class="hljs-number">100</span> == <span class="hljs-number">0</span>:<br>                <span class="hljs-built_in">print</span>(i)<br>            data = pd.read_csv(path + file + k + <span class="hljs-string">&#x27;.csv&#x27;</span>, header=<span class="hljs-literal">None</span>)<br>            data = data.values[<span class="hljs-number">1</span>:]<br>            dataint = [<span class="hljs-built_in">float</span>(x) <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> data[:, <span class="hljs-number">0</span>]]<br>            _u = dataint<br>            <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(_u)):<br>                <span class="hljs-keyword">if</span> _u[j] <span class="hljs-keyword">in</span> peak_list:<br>                    output_mat[i, peak_list.index(_u[j])] = data[j, <span class="hljs-number">1</span>]<br>        <span class="hljs-keyword">return</span> output_mat, peak_list<br><br><br><span class="hljs-comment"># Filter peaks and save</span><br>output_mat, peak_list = peak_filter(output_dir, min_frequency=<span class="hljs-number">0.05</span>, <span class="hljs-built_in">type</span>=<span class="hljs-string">&#x27;R&#x27;</span>, file=<span class="hljs-string">&#x27;peaks_&#x27;</span>)<br>spa_data = sparse.bsr_matrix(output_mat)<br>sparse.save_npz(<span class="hljs-string">&#x27;/Users/ForRiver/OneDrive/Desktop/Pre/BIONET/preprocess/compare-neg/gz4_5.npz&#x27;</span>, spa_data)<br>np.savetxt(<span class="hljs-string">&#x27;/Users/ForRiver/OneDrive/Desktop/Pre/BIONET/preprocess/compare-neg/peaklist-gz4_5&#x27;</span>, peak_list)<br><br><span class="hljs-comment"># Read sparse data and coordinates</span><br>data0 = sparse.load_npz(<span class="hljs-string">&#x27;/Users/ForRiver/OneDrive/Desktop/Pre/BIONET/preprocess/compare-neg/gz4_5.npz&#x27;</span>).toarray()<br>data = data0[<span class="hljs-number">1</span>:, :]<br>coordinates = np.loadtxt(<span class="hljs-string">&#x27;/Users/ForRiver/OneDrive/Desktop/Pre/BIONET/preprocess/compare-neg/cood-gz4_5.csv&#x27;</span>, delimiter=<span class="hljs-string">&#x27;,&#x27;</span>)<br><br>x, y = coordinates[:, <span class="hljs-number">0</span>].astype(<span class="hljs-built_in">int</span>), coordinates[:, <span class="hljs-number">1</span>].astype(<span class="hljs-built_in">int</span>)<br>output_mat = np.zeros((np.<span class="hljs-built_in">max</span>(y) + <span class="hljs-number">1</span>, np.<span class="hljs-built_in">max</span>(x) + <span class="hljs-number">1</span>, data.shape[<span class="hljs-number">1</span>]))<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(data)):<br>    output_mat[y[i], x[i]] = data[i]<br><br>output_mat = np.array([<span class="hljs-built_in">list</span>(output_mat[i]) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(output_mat)) <span class="hljs-keyword">if</span> np.<span class="hljs-built_in">sum</span>(output_mat[i]) != <span class="hljs-number">0</span>])<br>output_mat = np.array([<span class="hljs-built_in">list</span>(output_mat[:, i]) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(output_mat[<span class="hljs-number">0</span>])) <span class="hljs-keyword">if</span> np.<span class="hljs-built_in">sum</span>(output_mat[:, i]) != <span class="hljs-number">0</span>])<br>data = sparse.bsr_matrix(output_mat.reshape(output_mat.shape[<span class="hljs-number">0</span>] * output_mat.shape[<span class="hljs-number">1</span>], output_mat.shape[<span class="hljs-number">2</span>]))<br>sparse.save_npz(<span class="hljs-string">&#x27;/Users/ForRiver/OneDrive/Desktop/Pre/BIONET/preprocess/compare-neg/gz4_5_%d_%d.npz&#x27;</span> % (output_mat.shape[<span class="hljs-number">0</span>], output_mat.shape[<span class="hljs-number">1</span>]), data)<br><br><span class="hljs-comment"># Sort and save peak list</span><br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;/Users/ForRiver/OneDrive/Desktop/Pre/BIONET/preprocess/compare-neg/peaklist-gz4_5&#x27;</span>, <span class="hljs-string">&#x27;r&#x27;</span>) <span class="hljs-keyword">as</span> file:<br>    lines = file.readlines()<br>lines = [line.strip() <span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> lines]<br>lines.sort()<br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(sorted_peaklist_file, <span class="hljs-string">&#x27;w&#x27;</span>) <span class="hljs-keyword">as</span> file:<br>    <span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> lines:<br>        file.write(<span class="hljs-string">f&quot;<span class="hljs-subst">&#123;line&#125;</span>\n&quot;</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;peaklist_saved&quot;</span>)<br><br><span class="hljs-comment"># Reorder data columns based on sorted peak list</span><br>data0 = sparse.load_npz(<br>    <span class="hljs-string">&#x27;/Users/ForRiver/OneDrive/Desktop/Pre/BIONET/preprocess/compare-neg/gz4_5_%d_%d.npz&#x27;</span> % (output_mat.shape[<span class="hljs-number">0</span>], output_mat.shape[<span class="hljs-number">1</span>])).toarray()<br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;/Users/ForRiver/OneDrive/Desktop/Pre/BIONET/preprocess/compare-neg/peaklist-gz4_5&#x27;</span>, <span class="hljs-string">&#x27;r&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>    reference = [<span class="hljs-built_in">float</span>(x.strip()) <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> f.readlines()]<br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(sorted_peaklist_file, <span class="hljs-string">&#x27;r&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>    target = [<span class="hljs-built_in">float</span>(x.strip()) <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> f.readlines()]<br><br>indices = [target.index(value) <span class="hljs-keyword">if</span> value <span class="hljs-keyword">in</span> target <span class="hljs-keyword">else</span> -<span class="hljs-number">1</span> <span class="hljs-keyword">for</span> value <span class="hljs-keyword">in</span> reference]<br>data = np.vstack([data0[:, idx] <span class="hljs-keyword">for</span> idx <span class="hljs-keyword">in</span> indices <span class="hljs-keyword">if</span> idx != -<span class="hljs-number">1</span>]).T<br>np.save(<span class="hljs-string">&#x27;/Users/ForRiver/OneDrive/Desktop/Pre/BIONET/preprocess/compare-neg/gz4_5&#x27;</span>, data)<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Data has been saved.&quot;</span>)<br></code></pre></td></tr></table></figure><h1 id="写在最后"><a href="#写在最后" class="headerlink" title="写在最后"></a>写在最后</h1><p>这篇文章主要用于自己学习MSI预处理步骤以及对BIONET新成员介绍imzML文件的预处理流程。目前处理过的数据还很少，大家在测试时候也可以自己多试试几个参数看看效果（虽然效果并不是特别明显）。如果有错误的烦请指正。</p>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
