<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title></title>
    <link href="/2025/11/12/medseg/"/>
    <url>/2025/11/12/medseg/</url>
    
    <content type="html"><![CDATA[<img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20251019221120392.png" alt="MAE Overview" style="zoom:67%;" /><ul><li><p>预训练：</p><p>训练集：医院+公开，约   <code>800x（3~4）x256x256x80</code></p><p>1、单模态resize -&gt; <code>160x160x80</code></p><p>2、crop -&gt; 0.4 mask -&gt; 0.75</p><p>3、mae3d输入，通道改为80，单个patch大小为<code>160/patch_size，160/patch_size，80/patch_size</code> 保存encoder权重。</p><p>4、encoder输出为<code>batch_size，160/patch_size x 160/patch_size x 80/patch_size，embed</code></p></li></ul><p><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20251112184515361.png" alt="image-20251112184515236"></p><ul><li><p>训练：</p><p>训练集：医院数据+labels</p><p>1、单模态resize -&gt; <code>160x160x80</code></p><p>2、crop -&gt; 0.4 mask -&gt; 0</p><p>3、encoder采用预训练权重，encoder输出<code>batch_size，160/patch_size x 160/patch_size x 80/patch_size，embed</code> 经过unpatch第二维度恢复至<code>20,20,10</code>三维。</p><p>4、decoder改为CNN Decoder对<code>20,20,10</code>进行上采样，计算与label损失。</p></li><li><p>手写研究思路</p></li></ul>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>丝线</title>
    <link href="/2025/10/20/%E6%80%BB%E5%92%8C/"/>
    <url>/2025/10/20/%E6%80%BB%E5%92%8C/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>Revisiting MAE pre-training for 3D medical image segmentation</title>
    <link href="/2025/10/19/3DMAE/"/>
    <url>/2025/10/19/3DMAE/</url>
    
    <content type="html"><![CDATA[<p><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20251019152757630.png" alt="title"></p><p>本文发表在CVPR-25。</p><h2 id="研究目的"><a href="#研究目的" class="headerlink" title="研究目的"></a>研究目的</h2><p>自监督训练在3D医学影像里主要受三个限制：</p><ul><li><strong>预训练数据集规模过小</strong>：我们项目中有足够的数据。</li><li><strong>使用的架构过时或不适合3D医学影像</strong>：重点研究以适配项目。</li><li><strong>评估不充分</strong></li></ul><p>本文旨在对这三部分进行标准化改进来使MAE预训练适合3D医学影像病显著提升3D-CNN分割模型的性能。</p><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20251020122110753.png" alt="Compare" style="zoom:80%;" /><h2 id="Datasets"><a href="#Datasets" class="headerlink" title="Datasets"></a>Datasets</h2><h3 id="预训练数据集"><a href="#预训练数据集" class="headerlink" title="预训练数据集"></a>预训练数据集</h3><p>专有的大脑MRI数据集，该数据集来自超过44个中心，10多种不同MR扫描仪，包含超过9000名患者，约44000张MRI扫描（最后剩余39168张）。</p><ul><li><p>保留<strong>T1、T2、T1 FLAIR</strong>和<strong>T2 FLAIR</strong>四个序列的图像。</p></li><li><p>丢弃任何轴上视野&lt;50 mm的图像，在任何方向上间距&gt; 6.5 mm的图像和文件大小&lt;200 kb的图像（空图像）。</p></li></ul><h3 id="训练数据集"><a href="#训练数据集" class="headerlink" title="训练数据集"></a>训练数据集</h3><ul><li><p>多发性硬化（MS）病变数据集：MS FLAIR（T2W）</p></li><li><p>脑转移肿瘤数据集：Brain Mets（T1、T2F）</p></li><li><p>海马体数据集：Hippocampus （T1W）</p></li><li><p>卒中后病变数据集：Atlas22（T1W）</p></li><li><p>耳道内和耳道外前庭神经鞘瘤肿瘤数据集：CrossModa（T1W）</p></li></ul><p>训练：验证：测试  &#x3D; <code>64：16：20</code></p><h3 id="测试数据集"><a href="#测试数据集" class="headerlink" title="测试数据集"></a>测试数据集</h3><p>8个数据集包含颈动脉血管壁、风险器官（OAR）的分割、缺血性卒中病变、前口咽癌和转移性淋巴结、尼日利亚胶质母细胞瘤和高级别胶质瘤成像、脑动脉瘤及其周围脑组织等等。</p><p>测试数据集被用来评估当训练微调后分割其他目标结构时，学习的表征的功效。</p><p><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20251020132315125.png" alt="Distribution"></p><h2 id="研究内容"><a href="#研究内容" class="headerlink" title="研究内容"></a>研究内容</h2><h3 id="MAE"><a href="#MAE" class="headerlink" title="MAE"></a>MAE</h3><p><strong>Masked Autoencoders Are Scalable Vision Learners</strong></p><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20251019221120392.png" alt="MAE Overview" style="zoom:67%;" /><p>MAE采用Transformer架构，实际上就是Mask 掉输入图像的随机的 patches 并进行重建（推荐掩码率75%），但是编码器和解码器是<strong>非对称</strong>的。</p><p>MAE 的具体实现方法是：</p><ul><li><p>首先通过 Linear Projection 和<strong>位置编码</strong>得到 image tokens。</p></li><li><p>随机 shuffle 这些 tokens，按照 masking ratio 扔掉最后的一部分。</p></li><li><p>把 <strong>unmasked patches 输出到 Encoder</strong> 中，得到这些 tokens 的表征。</p></li><li><p>把 Encoder 的输出，结合 masked tokens（视为此处需要恢复，后续也是计算mask部分loss），执行 unshuffle操作恢顺序，再一起输入到 Decoder 中。</p><p>3DMAE即把VIT改为3D-VIT，每个patch变为<code>nxnxn</code>大小后平铺输入到transformer中。相较于常用于分割的3D-UNet相关的网络，ViT结构是比较好做MAE的。</p></li></ul><h3 id="3D-nnU-Net"><a href="#3D-nnU-Net" class="headerlink" title="3D nnU-Net"></a>3D nnU-Net</h3><p>  <img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20251020182221943.png" alt="nnUNet"></p><p>nnU-Net实际上使用简单的U-Net架构，主要将复杂的手动方法配置流程系统化为固定参数、基于数据集属性的规则参数以及最少的经验参数进行优化，其处理方法流程如下：</p><ul><li>从训练数据中提取数据集特征，包括图像大小、体素间距、模态等信息。</li><li>根据数据集特征,使用一系列经验规则自动配置分割流水线的参数，如预处理、网络拓扑结构、训练策略等。</li><li>将未涵盖的少量参数(如模型选择、后处理)设置为在训练中经验优化。</li><li>使用默认的U-Net网络架构模板训练多个模型。</li><li>通过交叉验证,从训练好的多个模型中经验选择表现最好的单模型或模型组合。</li><li>如果后处理能提高验证性能,则应用后处理。</li></ul><h3 id="MAE-3D-nnU-Net"><a href="#MAE-3D-nnU-Net" class="headerlink" title="MAE 3D nnU-Net"></a>MAE 3D nnU-Net</h3><p>为了在3D-CNN架构使用MAE训练，本文采取了几个措施：</p><ul><li><p>训练前，所有3D MRI图像都会被重采样到统一的物理空间间距 <strong><code>[1x1x1]</code>毫米</strong>，并进行Z-score归一化（减均值，除标准差），每个样本固定使用**<code>[160x160x160]</code>大小**的输入。</p></li><li><p><strong>针对CNN的掩码与稀疏化处理</strong>：CNN需要规则的网格输入，无法像ViT那样直接丢弃掩码token。为此，论文引入了一套<strong>稀疏化处理</strong>流程：</p><p><strong>稀疏卷积：</strong>在编码器部分，使用稀疏卷积进行操作，卷积核在计算时，其覆盖范围内可能同时包含真实像素和被掩码的零值区域。随着网络层数的加深，这些零值会通过感受野“污染”有效特征的计算。 所以需要在每一层卷积操作之后，都<strong>重新应用输入时生成的掩码</strong>。</p><p><strong>掩码令牌</strong>：对被掩码的区域填充可学习的<strong>掩码令牌</strong>，以保持空间结构信息。</p><p><strong>致密化卷积</strong>：在编码器和解码器之间，应用一个小的**<code>3x3x3</code>卷积**。（除了最高分辨率外）利用周围有效像素的信息，对掩码令牌所在的区域进行初步的、基于上下文的“填充”或“平滑”，使特征图在空间上更加连续。</p><h3 id="消融结果"><a href="#消融结果" class="headerlink" title="消融结果"></a>消融结果</h3><p><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20251020125742826.png" alt="Sparsification"></p><p>新增的稀疏卷积、掩码令牌和致密化卷积三部进行消融实验，三个部分全部应用后有一定的效果。</p></li></ul><h2 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h2><h3 id="对比结果"><a href="#对比结果" class="headerlink" title="对比结果"></a>对比结果</h3><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20251020124506294.png" alt="Fine-Tuning" style="zoom:80%;" /><ul><li><p>冻结编码器权重是有害的</p></li><li><p>应该适当减少学习率</p></li><li><p>Warm-up是必不可少的</p></li></ul><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20251020125832081.png" alt="Mask ratio" style="zoom:80%;" /><p>和原文推荐的mask ratio差不多，都在60%到75%之间效果最好，本文采用60%-90%的动态mask ratio，和75%相差不大。</p><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20251020130037449.png" alt="image-20251020130037418" style="zoom:80%;" /><p>S3D超过了development datasets所有的Baseline方法。</p><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20251020130428399.png" alt="all datasets" style="zoom:80%;" /><p>最终评估采用Dice相似系数和归一化表面距离来评估，在development和test datasets中的表现（no Dyn.是单独适应每个下游数据集的原始nnU-Net，No Fixed表示使用<strong>固定配置</strong>进行从零开始训练的nnU-Net）</p><p><strong>No Fixed vs No Dyn.<strong>：体现了</strong>nnU-Net自动化配置流程本身的价值有多大</strong></p><p><strong>No Dyn. vs S3D</strong>：体现了<strong>自监督预训练是否能带来额外的的性能提升</strong></p><p>对于动态nnU-Net表现不佳的大多数数据集中，预训练有助于恢复性能。然而，在某些情况下，例如D5，固定配置的nnU-Net仍然是最好的。</p><h3 id="ABCD数据集训练结果"><a href="#ABCD数据集训练结果" class="headerlink" title="ABCD数据集训练结果"></a>ABCD数据集训练结果</h3><p><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20251020152317051.png" alt="pre-training in ABCD"></p><p>使用ABCD dataset（青少年大脑认知发展多模态MRI数据）训练的模型平均DSC低0.6%以及平均NSD低0.6%，说明虽然MAE在一般计算机视觉任务中可以拓展，但由于医学影像领域的独特性，迁移到下游分割任务时可能泛化能力下降</p><p><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20251020152538235.png" alt="fine-tuning"></p><p>S3D使用40个左右小样本进行微调就和所有样本从头开始训练的效果差距不大（D4在这里差距比较大，因为样本更多）</p><p>D1<code>38/10/12</code>，D2<code>67/17/21</code>，D3<code>166/42/52</code>，D4<code>419/105/131</code>，D5<code>134/34/42</code></p><p><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20251020152924220.png" alt="pre-training length"></p><p>更长的预训练并不会导致性能的提高，在250k steps时反而取得最好效果。</p><h3 id="五折验证结果"><a href="#五折验证结果" class="headerlink" title="五折验证结果"></a>五折验证结果</h3><p><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20251020154127812.png" alt="5 fold"></p><p>复制预训练的权重并在解码器预热阶段期间冻结结果最稳定且同样很好。</p><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20251020154511289.png" alt="training length" style="zoom:80%;" /><p>在不到正常训练时间的15%之后，就可以实现最终性能的很大一部分。</p><h2 id="原文"><a href="#原文" class="headerlink" title="原文"></a>原文</h2>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>用git管理repo</title>
    <link href="/2025/08/05/%E7%94%A8Git%E7%AE%A1%E7%90%86Repo/"/>
    <url>/2025/08/05/%E7%94%A8Git%E7%AE%A1%E7%90%86Repo/</url>
    
    <content type="html"><![CDATA[<p>过去我只简单的使用过git push、git clone以及创建博客时的hexo等操作，为了更加系统性管理我的代码，花点时间学习git时有必要的。本文单纯是复习笔记，更详细的用法可以查看廖雪峰老师的git教程。</p><h2 id="什么是git"><a href="#什么是git" class="headerlink" title="什么是git"></a>什么是git</h2><p>git是由linus开发的一种分布式开发控制系统，可以方便的管理个人或是公司的代码版本，查看每个版本进行的改动。</p><h3 id="集中式？分布式？"><a href="#集中式？分布式？" class="headerlink" title="集中式？分布式？"></a>集中式？分布式？</h3><p>过去人们常用的SVN等集中式管理系统存在一些问题，比如必须联网、需要担心中央服务器的正常运行等等。而在分布式系统中，每个人都有完整的版本库，就不存在上述问题。</p><h2 id="基础操作"><a href="#基础操作" class="headerlink" title="基础操作"></a>基础操作</h2><h3 id="创建repo"><a href="#创建repo" class="headerlink" title="创建repo"></a>创建repo</h3><p>在你所需要创建仓库的地方打开gitbash或cd至你想要创建仓库的地方，用git init来初始化仓库。仓库中出现.git的文件夹说明创建成功。不要轻易的更改.git里的文件。</p><p>当然你也可以从github上git clone一份已经存在的repo，推荐使用ssh方式。</p><h3 id="工作区域与文件状态"><a href="#工作区域与文件状态" class="headerlink" title="工作区域与文件状态"></a>工作区域与文件状态</h3><p>除了你的本地仓库（工作区）、远程仓库（版本库）以外，.git文件夹中还有一个重要的区域——暂存区。你可以把他当做一个小推车，从仓库（工作区）取货时我们把需要的货物一个个git add进小推车里（暂存区），然后用小推车一次性git commit到商店（版本库）中。当然你也可以把商店里不需要的货物git reset HEAD到暂存区，然后再git checkout –file到工作区。当然，git status给你了随时查看工作区文件状态的可能。</p><p>为什么我在git checkout后要写完整代码？因为git checkout和git checkout –是两回事。后面我们切换分支时会讲到git checkout，不过你也有替代方法。</p><h3 id="版本差异与回退"><a href="#版本差异与回退" class="headerlink" title="版本差异与回退"></a>版本差异与回退</h3><p>今天你运了一天货，但是店老板突然告诉你那是修改后的货物清单，他把现在商店里的货物和他的清单git diff HEAD了一下给你看了看差异，其实他还想要你送来的第前100版。为了3500&#x2F;月工资，我忍了，我们知道HEAD表示这一版，HEAD^表示前一版，当然老板说的前100版不会是HEAD^^^^^^….^^，毕竟看到这么多^^你肯定也笑不出来，我们用HEAD~100来表示。</p><p>好在现在git给了我们时间回溯的可能，可以用git log来查看过去我们的存档，<del>用sl来回去打店老板一拳</del>然后git reset 版本号来进行时间回溯。等你回溯完老板不紧不慢打电话给你说：能不能改回你删掉的那版？以你专业人员的素养，你只能微笑着跟他说：有的兄弟有的，像这样的git reflog记录我还有一百条。然后git reset到reflog的版本号去。</p><h3 id="删除与忽略"><a href="#删除与忽略" class="headerlink" title="删除与忽略"></a>删除与忽略</h3><p>使用rm对本地文件进行删除后，你会发现暂存区和版本库内的内容仍然存在，事实上你可以把git add和git commit理解为一种状态，照样可以通过add和commit进行删除文件的更新。</p><p>对于.ignore来说，不建议你自己写这个文档，你可以从github上下载一份对应的.ignore文件，然后根据自己的需求进行小修改。</p><h3 id="远程仓库"><a href="#远程仓库" class="headerlink" title="远程仓库"></a>远程仓库</h3><p>在你的github绑定了ssh以后，可以通过git remote add origin 你的远程仓库（默认命名为origin，当然你也可以取一个新的名字）来绑定远程和本地仓库。此后只需要git pull&#x2F;push origin master来拉取和更新你的远程仓库。</p><h2 id="分支管理"><a href="#分支管理" class="headerlink" title="分支管理"></a>分支管理</h2><h3 id="创建、合并分支"><a href="#创建、合并分支" class="headerlink" title="创建、合并分支"></a>创建、合并分支</h3><p>现在我们的手头有一份工作，但是随意更改已经发布的版本是很危险的，这时候我们需要在一个新的分支进行工作。上文说到git checkout可以切换分支，由于一个指令有两种意思，我倾向于使用git switch -c 分支名来创建一个新的分支进行工作。</p><h3 id="bug分支"><a href="#bug分支" class="headerlink" title="bug分支"></a>bug分支</h3><h3 id="feature分支"><a href="#feature分支" class="headerlink" title="feature分支"></a>feature分支</h3><h3 id="多人协作"><a href="#多人协作" class="headerlink" title="多人协作"></a>多人协作</h3><h3 id="rebase"><a href="#rebase" class="headerlink" title="*rebase"></a>*rebase</h3>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>从Transformer到不同多模态工作</title>
    <link href="/2025/08/05/Transformer%E4%B8%8E%E5%A4%9A%E6%A8%A1%E6%80%81/"/>
    <url>/2025/08/05/Transformer%E4%B8%8E%E5%A4%9A%E6%A8%A1%E6%80%81/</url>
    
    <content type="html"><![CDATA[<h2 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h2><h3 id="Attention"><a href="#Attention" class="headerlink" title="Attention"></a>Attention</h3><p>Attention的结构如下：</p><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250513171957853.png" alt="Attention Structure" style="zoom:67%;" /><p>现在我们拥有Query、Key、Value三个向量矩阵。</p><p>注意力的作用可以看做对查询和键来计算余弦相似性。假设每行代表一个单词，其对于Q、K矩阵的一行Q<sub>1</sub>·K<sub>1</sub> &#x3D; |Q<sub>1</sub>|·|K<sub>1</sub>|·cosθ为一个数。Q<sub>1</sub>和K<sub>1</sub>的方向相同时（即它们的夹角接近0），cosθ 接近1，所以Q<sub>1</sub>·K<sub>1</sub>较大。这表明Q<sub>1</sub>和K<sub>1</sub>很相似。反之，当 Q<sub>1</sub>和K<sub>1</sub>的方向相反时（即它们的夹角接近180度），cos(θ) 接近 -1，所以Q<sub>1</sub>·K<sub>1</sub>较小，甚至为负。这表明Q<sub>1</sub>和K<sub>1</sub>不相似。</p><p>根据每行得出的相似度来计算得分，赋予权重，就完成了一次注意力计算。</p><p>具体来说，其公式为：</p><p>$$<br>\text { Attention }(Q, K, V)&#x3D;\operatorname{softmax}\left(\frac{Q K^{T}}{\sqrt{d_{k}}}\right) V<br>$$<br>也就是计算相似度得分后除以键向量维度的平方根，防止内积过大，使梯度更稳定。然后再进行softmax归一化后输出结果。输出的结果再去乘以V作为最后注意力输出的Z矩阵。</p><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250513172050816.png" alt="Attention Output" style="zoom: 50%; "/><h3 id="Self-Attention"><a href="#Self-Attention" class="headerlink" title="Self-Attention"></a>Self-Attention</h3><p>自注意力，顾名思义就是自己对自己做注意力机制。把我们仅有的初始矩阵X投影成Query、Key、Value三个向量矩阵（实际上是与可学习的矩阵做乘法），之后再做刚才的注意力机制操作。</p><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250513172124423.png" alt="self-attention QKV" style="zoom:50%;" align="center"/><p>自注意力是可以并行计算的，这与RNN逐个重复处理词元相比较，缺少了顺序信息。所以输入中要添加位置编码来注入绝对或相对位置信息。位置编码可以通过学习得到，也可以直接固定得到。例如在Transformer模型里，采用的正余弦位置编码就属于绝对位置编码。它利用正弦和余弦函数对不同位置进行编码，公式如下：</p><p>$$<br>PE_{t}^{(i)} &#x3D;<br>\begin{cases}<br>    \sin(w_{k}t), &amp; \text{if } i &#x3D; 2k\newline<br>    \cos(w_{k}t), &amp; \text{if } i &#x3D; 2k + 1<br>\end{cases}<br>$$<br>这里：<br>$$<br>\begin{cases}<br>    w_{k} &#x3D; \frac{1}{10000^{\frac{2k}{d_{model}}}} \newline<br>    \dot{t} &#x3D; 0, 1, 2, 3, \ldots, \frac{d_{model}}{2} - 1<br>\end{cases}<br>$$<br>把频率设为非常小的参数可以使最后和起始的位置不会靠得太近，sin、cos交替编码可以使其很容易转换成相对位置编码，k是维度。</p><p>假设有个512维度的Embedding，编码可视化就会像下面这样。越往后的位置，频率越小，波长越长，所以不同的t对最终的结果影响不大。</p><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250520155333892.png" alt="Position Encoding Vision" style="zoom:67%;" /><h3 id="Multihead-Self-Attention"><a href="#Multihead-Self-Attention" class="headerlink" title="Multihead Self-Attention"></a>Multihead Self-Attention</h3><p>实际上多头注意力就是多个Self-Attention的连结，拼接起来再去乘以权重矩阵W<sub>0</sub>，每个头可能会关注输入不同的部分。</p><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250513172140809.png" alt="Multihead Self-attention Output" style="zoom:67%;" align="center"/><p>在Transformer Decoder部分你还能看到Maked Multihead Attention，相较于多头注意力增加了一个掩码。包括：</p><ul><li>padding mask：由于输入序列长度不一样，给交短序列后填0，截取较长序列。mask做的就是把这些位置值加上非常大负数，使其softmax后几乎为0，Attention无法注意到。</li><li>causal（sequence） mask：由于Decoder时生成需要是因果的，t时刻后的输出不应该出现在输入内，所以用一个上三角矩阵作用在序列上。</li></ul><h3 id="Add-Normalize"><a href="#Add-Normalize" class="headerlink" title="Add &amp; Normalize"></a>Add &amp; Normalize</h3><p>经过多头注意力的输出Z还要进行Add和Norm两个操作。Add就是加上一个残差块，防止发生退化，而Layer Normalization则负责归一化加快收敛速度。</p><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250512152733146.png" alt="Transformer Encoder" style="zoom: 50%;" align="center" /><p>LN层针对同一样本的不同神经元进行归一化，在NLP中有用，而BN针对同一个batch不同样本同一位置神经元归一化，对NLP的词向量来说没有意义。</p><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250513141203365.png" alt="LN、BN、IN" style="zoom: 50%;" /><h3 id="Feed-Forward-Network"><a href="#Feed-Forward-Network" class="headerlink" title="Feed Forward Network"></a>Feed Forward Network</h3><p>全连接层FFN公式如下：<br>$$<br>F F N(x)&#x3D;\ ReLU (0, x W 1+b 1) W 2+b 2<br>$$<br>假设输入维度<code> [512，512]</code>，通过W<sub>1</sub><code>[512，2048]</code>拓展维度，能够稀疏特征分布，更易被ReLU非线性组合。W<sub>2</sub><code>[2048，512]</code>再去还原维度。</p><h3 id="整体结构"><a href="#整体结构" class="headerlink" title="整体结构"></a>整体结构</h3><p>Transformer整体结构就如下图所示，其中原文由6层Encoder和6层Decoder组成。</p><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250513161026879.png" alt="Transformer Overview" style="zoom:50%;" /><p>整个工作流程大概是</p><ul><li><p>输入Input Embedding（machine learing  （eos））经过6个Encoder生成Z矩阵。</p></li><li><p>输入OutputEmbedding在掩码多头注意力后，QK用Encoder生成的矩阵（第二个Decoder没有做Self-attention而只是Attention），输出所推理的掩码下一个单词。（（begin）  -&gt; （begin） 机）</p></li></ul><h3 id="Vision-Transformer"><a href="#Vision-Transformer" class="headerlink" title="Vision Transformer"></a>Vision Transformer</h3><p>Transformer在NLP任务上的成功引起了广泛注意，其在视觉上的应用Vision Transformer很快便推出了，该模型由三个模块组成：</p><ul><li>Linear Projection of Flattened Patches</li><li>Transformer Encoder</li><li>MLP Head（最后用于分类）</li></ul><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250512163112813.png" alt="VIT Overview" style="zoom:67%;" /><p>标准的Transformer模块，要求输入的是token序列[num_token, token_dim]，而图像是[H, W, C]。假如我输入的是一个<code>[224，224 ，3]</code>的图像，我可以通过大小为<code>16x16</code>，步长为16，卷积核个数为768的卷积提取特征为<code>[14，14，768]</code>，然后把它平铺成<code>[196，768]</code>大小的二维矩阵，实现Transformer输入格式。</p><p>Linear Projection的操作可以表示为：<br>$$<br>Z0 &#x3D;\ X·W + b<br>$$<br><strong>W</strong> 是可学习的权重矩阵，形状为 <code>[768, d_model]</code>，<strong>b</strong> 是可学习的偏置向量，<strong>Z<sub>0</sub></strong> 是线性投影后的输出矩阵，形状为 <code>[196, d_model]</code>。</p><p>和图片上展示的一样，实际上16 x 16，步长为16的卷积就相当于把这个图片分为196块这个大小的块分别提取特征，这样后面加上位置信息就更有道理。在进入Encoder前，注意要把Position Embedding加入。<br>$$<br>Z0’ &#x3D;\ Z0 + P<br>$$<br><strong>P</strong> 是位置编码矩阵，**Z<sub>0</sub>**是添加位置编码后的矩阵，形状仍为 <code>[196, d_model]</code>。</p><p>原论文中向量处还加入了大小为<code>[1,d_model]</code>的[class]token，这个可训练参数和其他数据拼接在一起。<br>$$<br>\mathbf{Z}_0^{\prime\prime} &#x3D; \left[ \begin{array}{c} \mathbf{C} \newline<br>\mathbf{Z}_0^{\prime} \end{array} \right]<br>$$<br><strong>C</strong> 是 <code>[class]</code> token，<strong>Z<sub>0</sub>′′</strong> 是拼接后的矩阵，形状为 <code>[197, d_model]</code>。</p><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250512165338090.png" alt="Linear Projection of Flattened Patches" style="zoom:67%;" /><p>Transformer Encoder部分不再介绍，MLP层其实接近FFN。</p><p>经过Encoder后输出的仍是<code>[197，d_model]</code>大小的矩阵，我们把首位的[class]token对应的<code>[1,768]</code>调出来，然后通过MLP Head得到最后的分类结果，以免你不知道用哪个位置的token进行后面的MLP分类操作。</p><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250512165902633.png" alt="MLP Head" style="zoom:50%;" /><h2 id="CLIP及其改进"><a href="#CLIP及其改进" class="headerlink" title="CLIP及其改进"></a>CLIP及其改进</h2><h3 id="CLIP简介"><a href="#CLIP简介" class="headerlink" title="CLIP简介"></a>CLIP简介</h3><p>CLIP（Contrastive Language-Image Pre-Training）模型是OPENAI在2021年发布的多模态预训练神经网络，核心是用大量的图像和文本配对数据来预训练对齐。</p><p>CLIP包含的主要部分为：</p><ul><li>Text Encoder：文本-&gt;低维向量</li><li>Image Encoder：图像-&gt;低维向量</li></ul><h3 id="CLIP学习"><a href="#CLIP学习" class="headerlink" title="CLIP学习"></a>CLIP学习</h3><ul><li>优势：出色的zero-shot效果</li><li>劣势：需要大量数据，难以完成复杂任务。</li><li>可学习之处：文本-图像构建正负样本对及点积相似性分类，对下面文章的指导。</li></ul><h4 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h4><p>CLIP使用的Encoder可以是修改后的ResNet50（<strong>注意力池化机制</strong>替代<strong>全局平均池化层</strong>），通过单层的多头QKV注意力，Q基于图像的全局平均池化；也可以是<strong>修改后的ViT</strong>，在Transformer前对Patch+Possition Embedding进行了归一化处理。使用的Test Encoder则是经典的<strong>Transformer</strong>。</p><h4 id="训练过程"><a href="#训练过程" class="headerlink" title="训练过程"></a>训练过程</h4><p>由下图，CLIP实际上通过生成文本、图像的相似度来生成预测 $| I1 |·|T1|·cosθ$，一个文本和对应图片生成n个正样本和其他的负样本。</p><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250510214719681.png" alt="pre-training" style="zoom:67%;" /><p>为了达到更好的效果，不能直接把单个词语与图片输入，而是先做一个Prompt template之后输入。</p><p>Prompt template是指生成提示的可重复的方式，包含一个模板，例如图中的a photo of a {object}。Prompt把下游任务重构为模型熟悉的完形填空模式，可以激活预训练语言模型中的记忆，在few-shot上效果显著。</p><p>推理时把图片特征和文本特征计算相似性，输出分类结果。</p><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250510220213424.png" alt="Prompt Template & Predict" style="zoom:67%;" /><p>由于把文字和图像特征结合学习到了结构化的部分知识，CLIP一定程度上摆脱了categorical label的限制，可以跳出训练时给出的label，例如做物体检测时，你在训练时给出的蓝色玩具、绿色玩具可以被检测为大象玩具、鸭子玩具等等。</p><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250510221154785.png" alt="eg" style="zoom:67%;" /><h3 id="CLIP分割应用"><a href="#CLIP分割应用" class="headerlink" title="CLIP分割应用"></a>CLIP分割应用</h3><h4 id="LSeg"><a href="#LSeg" class="headerlink" title="LSeg"></a>LSeg</h4><p>N个需要分割类别：N x C Text矩阵</p><p>图片：H x W x C 在C维度上相乘 </p><p>结果：H x W x N</p><p>由于分割数据集一般较小，会把CLIP改为有监督训练，计算交叉熵，和CLIP实际上有区别。</p><h4 id="GroupViT"><a href="#GroupViT" class="headerlink" title="GroupViT"></a>GroupViT</h4><p>Image通过ViT的Transformer layer后进行Grouping。（重复此模块并且Group的类别逐渐减少）</p><p>由于是分类任务，最后还是有不同的类别，做Avg Pooling后MLP，和CLIP一样与Text的MLP结果做余弦相似得到Loss。推理分类时再把每个Group的特征和Text的特征做余弦相似，可以知道每个Group对应哪一类。</p><p>由于CLIP特性，很可能分离的Mask正确而类别错误，没有正确学习语义，且最多只能检测到最后一层Group数量的类。</p><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250520172116758.png" alt="GroupVIT Overview" style="zoom:67%;" /><h2 id="多模态经典论文"><a href="#多模态经典论文" class="headerlink" title="多模态经典论文"></a>多模态经典论文</h2><p>以ViLT论文里展示的四种语言-视觉融合模型来说，CLIP处在B类。A、B两类的参数量更大，计算相似性部分则较轻，复杂情况可能难以得到好的结果。而C、D两种的训练时间长，要求高。</p><p>在多模态任务里，视觉特征是要大于文本特征的，所以Visual Embed应该不是很小，而Modality Interaction如果想处理更复杂的模型，应该也要较大，类似图中C类。</p><p>关于多模态任务使用的Loss，大概有Image Text Contrastive（CLIP中最大化NxN图像-文本对余弦相似度）、Mask Language Modeling（BERT中遮住一个词进行完形填空）、Image Text Matching（二分类判断生成的图像-文本对联合表示是否匹配）</p><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250512002336683.png" alt="four categories of vision-and-language models" style="zoom: 67%;" /><h3 id="ALBEF：Align-before-Fuse-Vision-and-Language-Representation-Learning-with-Momentum-Distillation"><a href="#ALBEF：Align-before-Fuse-Vision-and-Language-Representation-Learning-with-Momentum-Distillation" class="headerlink" title="ALBEF：Align before Fuse: Vision and Language Representation Learning with Momentum Distillation"></a>ALBEF：Align before Fuse: Vision and Language Representation Learning with Momentum Distillation</h3><h4 id="方法总结"><a href="#方法总结" class="headerlink" title="方法总结"></a>方法总结</h4><ul><li>优势：动量蒸馏处理噪声数据，标准化三个loss，训练成本相对亲民。</li><li>劣势：无法做多模态深度融合、难以完成复杂下游任务。</li><li>可学习之处：处理噪声数据、处理负样本太多导致ITM训练困难问题。</li></ul><h4 id="具体内容"><a href="#具体内容" class="headerlink" title="具体内容"></a>具体内容</h4><p>CLIP模型并没有学习更丰富的多模态交互，很多工作也容易对噪声文本进行过拟合。ALBEF旨在解决这些问题。</p><ul><li>Image Encoder：ViT</li><li>Text Encoder：BERT前六层</li><li>Multimodal Encoder：BERT后六层</li></ul><p>相对于CLIP，它的Text Encoder较轻而Modality Encoder较重，有点类似C类。</p><p>Loss：ITC（在多模态融合前先对齐) + ITM（负样本太多直接训练难，选择最接近正样本的负样本） + MLM</p><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250513162024567.png" alt="ALBEF Overview" style="zoom: 67%;" /><p>用动量蒸馏模型给图像-文本对比学习和掩码语言建模生成伪标签，不采用影响训练噪声数据（可能符合但不与Ground Truth相符），扩大有用数据集，one-hot to multi-hot。</p><p>动量蒸馏实际上就是先训练出一个版本的ALBEF，把它作为动量模型，用动量模型输出的伪目标作为额外的监督标准，在原本Loss的基础上和伪目标的Loss进行KL-发散加权组合。同时动量模型作为教师模型也随着ALBEF的训练动态EMA更新参数权重。</p><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250513162108984.png" alt="ITC/MIM examples" style="zoom: 67%;" /><h3 id="VLMo-Unified-Vision-Language-Pre-Training-with-Mixture-of-Modality-Experts"><a href="#VLMo-Unified-Vision-Language-Pre-Training-with-Mixture-of-Modality-Experts" class="headerlink" title="VLMo: Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts"></a>VLMo: Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts</h3><h4 id="方法总结-1"><a href="#方法总结-1" class="headerlink" title="方法总结"></a>方法总结</h4><ul><li>优势：Encoder使用灵活；可以使用单模态数据。</li><li>劣势：训练速度非常慢。</li><li>可学习之处：单模态训练流程优化、单塔模型及MoME。</li></ul><h4 id="具体内容-1"><a href="#具体内容-1" class="headerlink" title="具体内容"></a>具体内容</h4><p>CLIP、ALIGN等双塔模型，Image、Text单独抽取特征互不影响，但在一部分下游任务不如使用Fusion  Encoder的单塔模型。但是单塔模型在检索任务上又不适合大数据集，推理时间非常慢。</p><p>VLMo在Transformer中做了一部分改动：MoME Transformer。在Feed Forward Network中有区分Vision、Language和Vision-Language，在不同模态使用，self-attention可以共享参数。在使用中很灵活，但是由于要存储不同情况下的数据，要forward多次，速度非常慢，自己训练不现实。</p><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250513161725376.png" alt="VLMo Overview" style="zoom:80%;" /><p>训练时分阶段，使用单模态数据集单独预训练Vision、Language，大大减轻需要的多模态数据集。</p><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250513161754752.png" alt="Pre-training" style="zoom:80%;" /><p>使用的Loss也是ITC、ITM、MLM。</p><h3 id="BLIP：Bootstrapping-Language-Image-Pre-training-for-Unified-Vision-Language-Understanding-and-Generation"><a href="#BLIP：Bootstrapping-Language-Image-Pre-training-for-Unified-Vision-Language-Understanding-and-Generation" class="headerlink" title="BLIP：Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation"></a>BLIP：Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation</h3><h4 id="方法总结-2"><a href="#方法总结-2" class="headerlink" title="方法总结"></a>方法总结</h4><ul><li>优势：能处理噪声数据，扩大数据集；可以完成生成任务。</li><li>劣势：训练还是很慢，存在误分类问题（实际上还是CLIP带来的）。</li><li>可学习之处：对训练流程的优化。</li></ul><h4 id="具体内容-2"><a href="#具体内容-2" class="headerlink" title="具体内容"></a>具体内容</h4><p>视觉语言预训练（VLP）模型过去只可以单独做好理解任务或者是生成任务。而且，性能的提高大部分来自于从web上收集的大量的有噪声的数据。</p><p>BLIP提出了多模态混合编码-解码器，可以在三个功能中选择一个运行：</p><ul><li>单模态编码器</li><li>基于图像的文本编码器</li><li>基于图像的文本解码器</li></ul><p>以此来解决原来的模型无法生成任务的问题。</p><p>Image Encoder：VIT</p><p>Text Encoder&#x2F;Decoder：3种不同的用来算ITC、ITM（前两块比起ALBEF只是共享了self-attention参数）、Language Modeling（Decoder用的是Causal Self-Att做因果推理来完成生成任务）</p><p>文本端要做3次Forward，训练效率也偏慢。</p><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250512120226522.png" alt="BLIP-Overview" style="zoom:80%;" /><p>由于BLIP和ALBEF是同一个团队提出的，实际上训练细节差不多。而BLIP也致力于解决网络数据集噪声太多影响训练的情况，提出了Captioner Filter Model。</p><p>把训练好的BLIP模型前两部分拿出来，在无噪声数据集上微调后为Filter，再用微调后的Decoder生成的文本做Captioner补充新的数据集，过滤掉相对不匹配的提高图像文本对的质量。得到的图像文本对再返回去训练第二个BLIP模型。</p><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250512121417258.png" alt="Cap Filter Model" style="zoom:80%;" /><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250512122146514.png" alt="eg" style="zoom:80%;" /><h3 id="CoCa-Contrastive-Captioners-are-Image-Text-Foundation-Models"><a href="#CoCa-Contrastive-Captioners-are-Image-Text-Foundation-Models" class="headerlink" title="CoCa: Contrastive Captioners are Image-Text Foundation Models"></a>CoCa: Contrastive Captioners are Image-Text Foundation Models</h3><h4 id="方法总结-3"><a href="#方法总结-3" class="headerlink" title="方法总结"></a>方法总结</h4><ul><li><p>优势：训练较快，效果非常好，能适应不同任务。</p></li><li><p>劣势：规模史无前例，个人想训练几乎不可能。</p></li><li><p>可学习之处：对速度的优化，Pooling的更改。</p></li></ul><h4 id="具体内容-3"><a href="#具体内容-3" class="headerlink" title="具体内容"></a>具体内容</h4><p>顾名思义，用Contrastive loss 和 Captioning loss训练的模型。CoCa基本上是ALBEF直接的后续工作，跟ALBEF的区别是用了attentional pooling来学习不同任务的特征，以及放弃了ITM Loss。</p><p>另外文本特征端用的都是Decoder，这样一开始就是mask的，可以只做一次forward减少计算量，但是代价是用了巨大的数据集，非常多的参数量，scale远超之前所有的工作。</p><p>Image Encoder和Text Encoder用Contrastive Loss来训练，类似之前的ITC loss，而最后的多模态用的是Captioning Loss，也就是GPT用的Loss，基于交叉熵，用于预测被遮蔽的文本标记。</p><p><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250513164612673.png" alt="CoCa Overview"></p><p>效果和效果图都非常的好，这种类型的成果展示图被很多后来者用上。</p><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250513165158262.png" alt="比例取巧了，但是很吓人" style="zoom:50%;" /><h3 id="BEiT-3：Image-as-a-Foreign-Language-BEiT-Pretraining-for-All-Vision-and-Vision-Language-Tasks"><a href="#BEiT-3：Image-as-a-Foreign-Language-BEiT-Pretraining-for-All-Vision-and-Vision-Language-Tasks" class="headerlink" title="BEiT-3：Image as a Foreign Language: BEiT Pretraining for All Vision and Vision-Language Tasks"></a>BEiT-3：Image as a Foreign Language: BEiT Pretraining for All Vision and Vision-Language Tasks</h3><h4 id="方法总结-4"><a href="#方法总结-4" class="headerlink" title="方法总结"></a>方法总结</h4><ul><li>优势：没有像CoCa一样使用过多数据且均为Public，应用广泛。</li><li>可学习之处：损失函数设计优化</li></ul><h4 id="具体内容-4"><a href="#具体内容-4" class="headerlink" title="具体内容"></a>具体内容</h4><p>BieT的核心就是把所有图片化成Imagelish（和Text形式一样），和Text一起做Mask Modeling（完形填空）当做唯一的损失。而Pre-training部分和他们之前的工作VLMo是完全一致的MOE，可以根据任务来切换FFN。</p><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250513170238887.png" alt="Pre-training" style="zoom: 67%;" /><p>BEiT-3展示了它的不同模块可以完成不同任务，可以单独使用Vision Encoder、Language Encoder、模态融合做推理、分类、生成等，取决你哪种FFN的Encoder以及用不用Decoder。</p><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250513170356130.png" alt="BEiT Overview" style="zoom: 67%;" />]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>Pytorch lightning学习</title>
    <link href="/2025/08/05/Pytorch%20lightning%E5%AD%A6%E4%B9%A0/"/>
    <url>/2025/08/05/Pytorch%20lightning%E5%AD%A6%E4%B9%A0/</url>
    
    <content type="html"><![CDATA[<h2 id="基本操作"><a href="#基本操作" class="headerlink" title="基本操作"></a>基本操作</h2><h3 id="使用LightningModule"><a href="#使用LightningModule" class="headerlink" title="使用LightningModule"></a>使用LightningModule</h3><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs ruby"><span class="hljs-keyword">class</span> <span class="hljs-title class_">LitAutoEncoder</span>(L.<span class="hljs-title class_">LightningModule</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params"><span class="hljs-variable language_">self</span>, encoder, decoder</span>):<br>        <span class="hljs-variable language_">super</span>().__init__()<br>        <span class="hljs-variable language_">self</span>.encoder = encoder<br>        <span class="hljs-variable language_">self</span>.decoder = decoder<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">training_step</span>(<span class="hljs-params"><span class="hljs-variable language_">self</span>, batch, batch_idx</span>):<br>        <span class="hljs-comment"># training_step defines the train loop.</span><br>        x, _ = batch<br>        x = x.view(x.size(<span class="hljs-number">0</span>), -<span class="hljs-number">1</span>)<br>        z = <span class="hljs-variable language_">self</span>.encoder(x)<br>        x_hat = <span class="hljs-variable language_">self</span>.decoder(z)<br>        loss = F.mse_loss(x_hat, x)<br>        <span class="hljs-keyword">return</span> loss<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">configure_optimizers</span>(<span class="hljs-params"><span class="hljs-variable language_">self</span></span>):<br>        optimizer = torch.optim.<span class="hljs-title class_">Adam</span>(<span class="hljs-variable language_">self</span>.parameters(), lr=<span class="hljs-number">1e-3</span>)<br>        <span class="hljs-keyword">return</span> optimizer<br></code></pre></td></tr></table></figure><p>对设定的encoder和decoder自动执行下面的training_step，应包含batch导入，forward步骤，loss导入与计算。结束后自动使用optimizer。</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs routeros"><span class="hljs-comment"># model</span><br>autoencoder = LitAutoEncoder(Encoder(), Decoder())<br><br><span class="hljs-comment"># train model</span><br>trainer = L.Trainer()<br>trainer.fit(<span class="hljs-attribute">model</span>=autoencoder, <span class="hljs-attribute">train_dataloaders</span>=train_loader)<br></code></pre></td></tr></table></figure><p>定义好模型后使用trainer.fit进行训练，实际上后台执行的逻辑是：</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs stylus">autoencoder = <span class="hljs-built_in">LitAutoEncoder</span>(<span class="hljs-built_in">Encoder</span>(), <span class="hljs-built_in">Decoder</span>())<br>optimizer = autoencoder<span class="hljs-selector-class">.configure_optimizers</span>()<br><br><span class="hljs-keyword">for</span> batch_idx, batch <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(train_loader):<br>    loss = autoencoder<span class="hljs-selector-class">.training_step</span>(batch, batch_idx)<br><br>    loss<span class="hljs-selector-class">.backward</span>()<br>    optimizer<span class="hljs-selector-class">.step</span>()<br>    optimizer<span class="hljs-selector-class">.zero_grad</span>()<br></code></pre></td></tr></table></figure><h3 id="添加验证集及测试集"><a href="#添加验证集及测试集" class="headerlink" title="添加验证集及测试集"></a>添加验证集及测试集</h3><p>把一个数据集按预定比例拆分，分别作为训练集验证集。</p><figure class="highlight jboss-cli"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs jboss-cli"><span class="hljs-comment"># use 20% of training data for validation</span><br>train_<span class="hljs-keyword">set</span>_size = int<span class="hljs-params">(len(train_set)</span> * 0.8)<br>valid_<span class="hljs-keyword">set</span>_size = len<span class="hljs-params">(train_set)</span> - train_<span class="hljs-keyword">set</span>_size<br><br><span class="hljs-comment"># split the train set into two</span><br>seed = torch.Generator<span class="hljs-params">()</span><span class="hljs-string">.manual_seed</span><span class="hljs-params">(42)</span><br>train_<span class="hljs-keyword">set</span>, valid_<span class="hljs-keyword">set</span> = data.random_split<span class="hljs-params">(train_set, [train_set_size, valid_set_size], <span class="hljs-attr">generator</span>=seed)</span><br></code></pre></td></tr></table></figure><p>在原模型中添加你需要的验证步骤，注意将验证集传入.fit使用。</p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs ruby"><span class="hljs-keyword">class</span> <span class="hljs-title class_">LitAutoEncoder</span>(L.<span class="hljs-title class_">LightningModule</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">training_step</span>(<span class="hljs-params"><span class="hljs-variable language_">self</span>, batch, batch_idx</span>):<br>        ...<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">validation_step</span>(<span class="hljs-params"><span class="hljs-variable language_">self</span>, batch, batch_idx</span>):<br>        <span class="hljs-comment"># this is the validation loop</span><br>        x, _ = batch<br>        x = x.view(x.size(<span class="hljs-number">0</span>), -<span class="hljs-number">1</span>)<br>        z = <span class="hljs-variable language_">self</span>.encoder(x)<br>        x_hat = <span class="hljs-variable language_">self</span>.decoder(z)<br>        val_loss = F.mse_loss(x_hat, x)<br>        <span class="hljs-variable language_">self</span>.log(<span class="hljs-string">&quot;val_loss&quot;</span>, val_loss)<br>        <br>trainer = L.<span class="hljs-title class_">Trainer</span>()<br>trainer.fit(model, train_loader, valid_loader)<br></code></pre></td></tr></table></figure><p>测试集也一样，添加test_step步骤后，在.test中调用。</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs routeros">trainer.test(model, <span class="hljs-attribute">dataloaders</span>=DataLoader(test_set))<br></code></pre></td></tr></table></figure><h3 id="保存-加载模型"><a href="#保存-加载模型" class="headerlink" title="保存&#x2F;加载模型"></a>保存&#x2F;加载模型</h3><p>Lightning 会自动在您当前工作目录中为您保存一个检查点，其中包含您上一个训练 epoch 的状态。这可确保在训练中断时可以恢复训练。更改路径可修改Trainer中的default_root_dir参数。通过以下方法恢复保存的参数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">model = MyLightningModule.load_from_checkpoint(<span class="hljs-string">&quot;/path/to/checkpoint.ckpt&quot;</span>)<br></code></pre></td></tr></table></figure><h3 id="早停"><a href="#早停" class="headerlink" title="早停"></a>早停</h3><p>监控指标，在未观察到改善时停止训练。</p><figure class="highlight scss"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs scss">from lightning<span class="hljs-selector-class">.pytorch</span><span class="hljs-selector-class">.callbacks</span><span class="hljs-selector-class">.early_stopping</span> import EarlyStopping<br><br>class <span class="hljs-built_in">LitModel</span>(LightningModule):<br>    def <span class="hljs-built_in">validation_step</span>(self, batch, batch_idx):<br>        loss = ...<br>        self.<span class="hljs-built_in">log</span>(<span class="hljs-string">&quot;val_loss&quot;</span>, loss)<br><br><br>model = <span class="hljs-built_in">LitModel</span>()<br>trainer = <span class="hljs-built_in">Trainer</span>(callbacks=[<span class="hljs-built_in">EarlyStopping</span>(monitor=<span class="hljs-string">&quot;val_loss&quot;</span>, mode=<span class="hljs-string">&quot;min&quot;</span>)])<br>trainer.<span class="hljs-built_in">fit</span>(model)<br></code></pre></td></tr></table></figure><p>您可以通过更改回调的参数来自定义回调行为。</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs routeros">early_stop_callback = EarlyStopping(<span class="hljs-attribute">monitor</span>=<span class="hljs-string">&quot;val_accuracy&quot;</span>, <span class="hljs-attribute">min_delta</span>=0.00, <span class="hljs-attribute">patience</span>=3, <span class="hljs-attribute">verbose</span>=<span class="hljs-literal">False</span>, <span class="hljs-attribute">mode</span>=<span class="hljs-string">&quot;max&quot;</span>)<br>trainer = Trainer(callbacks=[early_stop_callback])<br></code></pre></td></tr></table></figure><p>停止训练的其他参数：</p><ul><li><code>stopping_threshold</code>：一旦监测的数量达到此阈值，立即停止训练。 当我们知道超过某个最佳值不会进一步使我们受益时，它就很有用。</li><li><code>divergence_threshold</code>：一旦监控的数量低于此阈值，就立即停止训练。 当达到如此糟糕的值时，我们认为模型无法再恢复，最好尽早停止并在不同的初始条件下运行。</li><li><code>check_finite</code>：开启后，如果监控的指标变为 NaN 或 infinite，它会停止训练。</li><li><code>check_on_train_epoch_end</code>：开启后，它会在训练 epoch 结束时检查指标。</li></ul><h3 id="预训练"><a href="#预训练" class="headerlink" title="预训练"></a>预训练</h3><p>对已经PATH训练过的模型，可以采取这种方式进行微调与新数据集的预测。</p><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs pgsql">model = ImagenetTransferLearning()<br>trainer = Trainer()<br>trainer.fit(model)<br>model = ImagenetTransferLearning.load_from_checkpoint(<span class="hljs-type">PATH</span>)<br>model.<span class="hljs-keyword">freeze</span>()<br><br>x = some_images_from_cifar10()<br>predictions = model(x)<br></code></pre></td></tr></table></figure><h3 id="调试模型"><a href="#调试模型" class="headerlink" title="调试模型"></a>调试模型</h3><p>设置断点：在此示例中，代码将在执行y &#x3D; x**2之前停止。</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">def</span> function_to_debug():<br>    <span class="hljs-attribute">x</span> = <span class="hljs-number">2</span><br><br>    <span class="hljs-comment"># set breakpoint</span><br>    <span class="hljs-attribute">breakpoint</span>()<br>    <span class="hljs-attribute">y</span> = x**<span class="hljs-number">2</span><br></code></pre></td></tr></table></figure><p>快速测试：此代码会运行5批训练、验证、测试数据，你也可以自定义数字。</p><figure class="highlight abnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs abnf"><span class="hljs-attribute">trainer</span> <span class="hljs-operator">=</span> Trainer(fast_dev_run<span class="hljs-operator">=</span>True)<br></code></pre></td></tr></table></figure><p>缩短epoch：也可以用具体数字而非百分比。</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">trainer</span> = Trainer(limit_train_batches=<span class="hljs-number">0</span>.<span class="hljs-number">1</span>, limit_val_batches=<span class="hljs-number">0</span>.<span class="hljs-number">01</span>)<br></code></pre></td></tr></table></figure><h3 id="发现训练瓶颈"><a href="#发现训练瓶颈" class="headerlink" title="发现训练瓶颈"></a>发现训练瓶颈</h3><p>advance暂时无法使用，simple可以打印每一步的用时，但是有点粗略可能没啥用。</p><figure class="highlight abnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs abnf"><span class="hljs-attribute">profiler</span><span class="hljs-operator">=</span><span class="hljs-string">&quot;simple&quot;</span><br></code></pre></td></tr></table></figure><p>可以检测下GPU&#x2F;TPU&#x2F;HPU容量，自行判断。</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs stylus">from lightning<span class="hljs-selector-class">.pytorch</span><span class="hljs-selector-class">.callbacks</span> import DeviceStatsMonitor<br><br>trainer = <span class="hljs-built_in">Trainer</span>(callbacks=<span class="hljs-selector-attr">[DeviceStatsMonitor()]</span>)<br></code></pre></td></tr></table></figure><h3 id="跟踪指标"><a href="#跟踪指标" class="headerlink" title="跟踪指标"></a>跟踪指标</h3><p>在训练过程记录value指标：</p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs ruby"><span class="hljs-keyword">def</span> <span class="hljs-title function_">training_step</span>(<span class="hljs-params"><span class="hljs-variable language_">self</span>, batch, batch_idx</span>):<br>        value = ...<br>        <span class="hljs-variable language_">self</span>.log(<span class="hljs-string">&quot;some_value&quot;</span>, value)<br></code></pre></td></tr></table></figure><p>如果要记录多个指标，需要使用self.log_dict：</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs routeros">values = &#123;<span class="hljs-string">&quot;loss&quot;</span>: loss, <span class="hljs-string">&quot;acc&quot;</span>: acc, <span class="hljs-string">&quot;metric_n&quot;</span>: metric_n&#125;  # <span class="hljs-built_in">add</span> more items <span class="hljs-keyword">if</span> needed<br>self.log_dict(values, <span class="hljs-attribute">prog_bar</span>=<span class="hljs-literal">True</span>)<br></code></pre></td></tr></table></figure><p>在命令行使用以下命令可以观察累计指标：</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs routeros">tensorboard <span class="hljs-attribute">--logdir</span>=lightning_logs/<br></code></pre></td></tr></table></figure><p><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250717202421758.png" alt="image-20250717202418931"></p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>NormAE：旨在消除基于液相色谱质谱的代谢组学数据中的批次效应的深度对抗学习模型</title>
    <link href="/2025/08/05/NormAE%E6%B1%87%E6%8A%A5/"/>
    <url>/2025/08/05/NormAE%E6%B1%87%E6%8A%A5/</url>
    
    <content type="html"><![CDATA[<h1 id="NormAE：旨在消除基于液相色谱质谱的代谢组学数据中的批次效应的深度对抗学习模型"><a href="#NormAE：旨在消除基于液相色谱质谱的代谢组学数据中的批次效应的深度对抗学习模型" class="headerlink" title="NormAE：旨在消除基于液相色谱质谱的代谢组学数据中的批次效应的深度对抗学习模型"></a>NormAE：旨在消除基于液相色谱质谱的代谢组学数据中的批次效应的深度对抗学习模型</h1><p><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250720190702146.png" alt="NormAE"></p><h2 id="研究目的"><a href="#研究目的" class="headerlink" title="研究目的"></a>研究目的</h2><h3 id="需要解决的问题"><a href="#需要解决的问题" class="headerlink" title="需要解决的问题"></a>需要解决的问题</h3><p>基于<strong>液相色谱-质谱（LC-MS）</strong>的非靶向代谢组学受<strong>非线性批次效应</strong>（样品在不同批次处理和测量时发生的系统性技术差异）的影响，掩盖了生物学效应，且难以校正，为解决该问题该文章在2020年提出了Normalization Autoencoder。</p><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250720205058185.png" alt="image-20250720205058131" style="zoom:80%;" /><h3 id="现存技术问题"><a href="#现存技术问题" class="headerlink" title="现存技术问题"></a>现存技术问题</h3><ul><li><table><thead><tr><th align="left">方法</th><th align="left">优点</th><th align="left">缺点</th></tr></thead><tbody><tr><td align="left">NormAE</td><td align="left">非线性建模，扩展性强，联合校正多种效应</td><td align="left"><strong>需大量数据，依赖批次标签</strong></td></tr><tr><td align="left">WaveICA</td><td align="left">适合非平稳信号，能分离复杂噪声</td><td align="left"><strong>实现复杂</strong>，参数敏感，可能损失信号</td></tr><tr><td align="left">ComBat</td><td align="left">经典方法，稳健，易实现，适合线性批次效应</td><td align="left"><strong>仅线性校正，难处理复杂效应</strong>，不能校正顺序偏差</td></tr><tr><td align="left">ICA</td><td align="left">能分离独立噪声，算法成熟</td><td align="left"><strong>独立性假设强</strong>，结果解释难，需人工判断批次成分</td></tr><tr><td align="left">QC-RLSC</td><td align="left">校正时间漂移，适合高通量数据，直观易用</td><td align="left"><strong>依赖QC样本</strong>，仅校正顺序漂移，<strong>对批次效应有限</strong></td></tr></tbody></table></li></ul><p>由于这些拟合模型中样本泛化能力差、数量少，难以避免过度拟合。其他组学领域解决批次效应的方式又依赖于部分领域内假设，不符合代谢组学。</p><h2 id="NormAE"><a href="#NormAE" class="headerlink" title="NormAE"></a>NormAE</h2><h3 id="样品"><a href="#样品" class="headerlink" title="样品"></a>样品</h3><p>两份LC_MS数据集均为医院采集：一个包含四批共644份血浆样品，<strong>85份QC</strong>，另一个包含四批共644份血浆样品，<strong>81份QC</strong>。（Quality control samples，来自生物受试者的每个样品的小等分试样组成的混合样品）</p><p>4个批次，分别由192、192、184和76个样品组成，酰胺数据集中有25、25、24和11个QC，T3数据集中分别有25、25、21和10个QC。</p><p>QC样本可参与训练，提升模型的识别能力，也可用于评估校正效果。但NormAE本身不依赖QC样本，只要有批次&#x2F;顺序标签即可训练。</p><h3 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h3><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250720205122698.png" alt="NormAE Overview(C)" style="zoom: 67%;" /><p>NormAE分为四个结构：Encoder、Decoder、F<sub>b</sub>、F<sub>o</sub>，都是MLP结构，其中的层数如下所示：</p><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250722023612806.png" alt="网络架构" style="zoom:67%;" /><p>NormAE的要求：<strong>数据必须存在批次标签</strong>。</p><p>假设数据存在批次标签Y^b^，定义一个有Encoder和Decoder的AutoCoder对该数据进行重构。Encoder训练目标是让F<sub>b</sub>分类效果差。Decoder则合并批标签和潜在信息以重建原始信息，要确保Encoder不丢失有用信息。</p><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250720224023657.png" alt="image-20250720224023589" style="zoom:50%;" /><p>这里x，y分别是原始峰值和批次标签。</p><p>其中loss分为两部分：重建损失（使用MAE）</p><p><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250722021302076.png"></p><p>以及用于对标签进行分类的损失函数：</p><p><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250722021459663.png"></p><p>训练时交替运行编码器和判别器，同时引入<strong>对抗网络</strong>：在自编码器旁边并行训练一个<strong>批次分类器</strong>（预测样品所属批次）和一个<strong>批内判别器</strong>（去除比批间标签更多的批效应，例如批内效应不能用批标签来表征，而是与注射顺序相关），以对抗性正则化驱动自编码器学习使不同批次难以区分的潜在表示。训练目标包括数据重构损失和对抗损失等，迫使解码器输出具有去除批次效应的数据。</p><p>加入F<del>o</del>之后需要优化的损失函数为：</p><p><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250722022135367.png"></p><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250722182347525.png" alt="image-20250722182347407" style="zoom:67%;" /><p>训练首先使用loss<sub>rec</sub>对编码器E和解码器D进行了1000个epoch的预训练，然后使用loss<sub>disc-b</sub>对鉴别器F<sub>b</sub>进行了10个epoch的预训练，使用loss<sub>disc−o</sub>对鉴别器F<sub>o</sub>进行了10个epoch的预训练。迭代阶段进行了700个epoch的训练。</p><h2 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h2><p>下图分别展示了处理后（以T3数据集为例)：</p><ul><li>PCA降维后的空间中，QC之间的欧氏距离的平均值。</li><li>QC两两之间的皮尔逊相关系数的平均值，aPCC越接近1，说明QC样本之间的表达谱非常相似</li><li>相对标准偏差（RSD, Relative Standard Deviation）在QC的15%和30%范围内的峰比例，pRSD说明小于该RSD的比例。</li></ul><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250722040811899.png" alt="image-20250722040811844" style="zoom:67%;" /><p>具体来看，不同方法处理后QC（实心圆）和所有数据（空心圆）PCA评分如下：</p><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250722042157370.png" alt="PCA" style="zoom: 80%;" /><p>QC处理前后的对PCC如下：</p><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250722042335917.png" alt="PCC" style="zoom:80%;" /><p>处理后不同注射顺序结果如下：</p><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250722042428480.png" alt="Injection" style="zoom:80%;" /><p>小样本下，NormAE方法效果并不好：</p><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250722042736296.png" alt="image-20250722042736202" style="zoom:50%;" /><h2 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h2><p>RALPS(2023)：</p><ul><li><p>也是对抗学习思想，但更强调健壮性和伪批次（pseudo-batch）的构建。</p></li><li><p>通过构造“伪批次”或“伪标签”，让模型在没有真实批次标签的情况下也能校正批次效应。</p></li></ul><p>这是由于其引入了变异损失（variation loss）：通常通过度量不同伪批次之间的分布距离（如均值、方差、MMD、KL散度等），并将其作为损失项加入总损失函数中。</p><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250722044217028.png" alt="RALPS" style="zoom: 50%;" />]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>R-Studio容器使用及MSI数据处理流程</title>
    <link href="/2025/08/05/R-Studio%E5%AE%B9%E5%99%A8%E4%BD%BF%E7%94%A8%E5%8F%8AMSI%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%B5%81%E7%A8%8B/"/>
    <url>/2025/08/05/R-Studio%E5%AE%B9%E5%99%A8%E4%BD%BF%E7%94%A8%E5%8F%8AMSI%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%B5%81%E7%A8%8B/</url>
    
    <content type="html"><![CDATA[<h2 id="Version：2-1"><a href="#Version：2-1" class="headerlink" title="Version：2.1"></a>Version：2.1</h2><ul><li>2025&#x2F;11&#x2F;19</li><li>更新R-Studio标准容器连接</li><li>更新过时的<em>Cardinal</em>包函数</li></ul><h1 id="一、MSI数据"><a href="#一、MSI数据" class="headerlink" title="一、MSI数据"></a>一、MSI数据</h1><h2 id="常用的数据类型有哪些？"><a href="#常用的数据类型有哪些？" class="headerlink" title="常用的数据类型有哪些？"></a>常用的数据类型有哪些？</h2><p>在质谱成像领域内，你大概率会见到的两种文件格式：<br>    ● Raw文件格式<br>    ● imzML原始数据文件格式<br>    事实上不同厂家的质谱仪器生产出来的Raw质谱数据是不一样的，主流公司的数据格式如下表所示。<br><img src="/img/MSI%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%B5%81%E7%A8%8B/1-2.png"></p><h2 id="如何从Raw数据转换成imzML数据？"><a href="#如何从Raw数据转换成imzML数据？" class="headerlink" title="如何从Raw数据转换成imzML数据？"></a>如何从Raw数据转换成imzML数据？</h2><p>理论上我们只需要知道扫描斑点数信息，就可以直接从Raw文件中计算出质谱成像所需要的空间坐标信息。  本文侧重点在于对imzML文件的预处理，对于Raw文件可以用以下的链接提供的软件来进行Raw to imzML的转换，试着使用他给出的示例文件来进行练习。<br>        <a href="https://www.ms-imaging.org/imzml/software-tools/raw-to-imzml-converter/">https://www.ms-imaging.org/imzml/software-tools/raw-to-imzml-converter/</a></p><h1 id="二、R包标准容器连接"><a href="#二、R包标准容器连接" class="headerlink" title="二、R包标准容器连接"></a>二、R包标准容器连接</h1><h2 id="R语言连接办法"><a href="#R语言连接办法" class="headerlink" title="R语言连接办法"></a>R语言连接办法</h2><p>R部署在服务器8787端口，登录账号密码为姓名，姓名+123，第一次登陆后就需要自己修改为更复杂的密码。</p><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20251120180600258.png" alt="image-20251120180600196" style="zoom: 67%;" /><p>进入后可以直接使用，其中文件的使用在下一部分阐述，使用后在右上角登出。</p><p><strong>注意：8787中R已经预装了cardinal，只需要library（cardinal）就可以正常使用了。</strong></p><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20251120180746424.png" alt="image-20251120180746366" style="zoom: 67%;" /><p>使用不同版本的R语言</p><p>容器的介绍及相关网站详见：<a href="https://neonexusx.github.io/2024/09/21/Bionet-Server01%E4%BD%BF%E7%94%A8%E8%AF%B4%E6%98%8E%20%E2%80%94%E2%80%94Docker%E7%AF%87/">https://neonexusx.github.io/2024/09/21/Bionet-Server01%E4%BD%BF%E7%94%A8%E8%AF%B4%E6%98%8E%20%E2%80%94%E2%80%94Docker%E7%AF%87/</a></p><p><strong>注意：本部分仅提供给要使用特定版本的R语言的同学，不需要可以跳过。</strong></p><p>这里可以看到已经存在一个R_EXAMPLE标准容器：</p><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20251120161522694.png" alt="img" style="zoom:80%;" /><p>要创建自己版本的容器，从标准容器中继续点Duplicate复制容器：</p><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20251120161705119.png" alt="img" style="zoom:80%;" /><p>这里要改动的有如下几点，其中文件夹位置具体看下方R的文件共享部分：</p><p><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20251120175018054.png" alt="img" style="zoom: 80%;" /><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20251121164320678.png" alt="img"></p><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20251121164320678.png" alt="img" style="zoom:80%;" /><p>进入R语言对应的版本的端口即可，注意下图的8789应为你所修改后的端口号：</p><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20251120175102340.png" alt="image-20251120175102258" style="zoom: 67%;" /><h2 id="R的文件共享"><a href="#R的文件共享" class="headerlink" title="R的文件共享"></a>R的文件共享</h2><p>R的工作目录默认下应该能看到如下内容：</p><img src="https://s2.loli.net/2024/10/23/JtnciVDZ435kx8O.png" alt="image-20241023230045769" style="zoom:67%;" /><p>host文件夹连接了主<strong>机的你个人的home文件夹</strong>。</p><img src="https://s2.loli.net/2024/10/23/QcDnkq6fMvNZJW1.png" alt="image-20241023233507171" style="zoom:67%;" /><p>假设你主机上在你的</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">/home/用户名/A（也就是～）<br></code></pre></td></tr></table></figure><p>文件夹下有文件A，你在R studio中调用的时候就是：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">/home/Neo/host/A<br></code></pre></td></tr></table></figure><p>简单来说就是将主机下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">/home/用户名<br></code></pre></td></tr></table></figure><p>映射到了</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">/home/Neo/host/<br></code></pre></td></tr></table></figure><p><strong>所有在主机上路径为<code>/home/用户名/</code>应替换为<code>/home/Neo/host/</code>。</strong></p><p>Datasets文件夹连接了NAS的数据中心，与主机上&#x2F;home&#x2F;Datasets文件夹是同一个文件夹。</p><p>假设你在主机上NAS数据中心路径是这样的：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">/home/Datasets/bionet/Dataset/A<br></code></pre></td></tr></table></figure><p>那么在R studio中，应该是：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">/home/个人用户名/Datasets/bionet/Dataset/A<br></code></pre></td></tr></table></figure><p>简单来讲明就是我们将主机上：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">/home/Datasets <br></code></pre></td></tr></table></figure><p>映射到了：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">/home/个人用户名/Datasets<br></code></pre></td></tr></table></figure><p>所有路径<code>/home/Datasets/应替换为</code>&#x2F;home&#x2F;个人用户名&#x2F;Datasets&#96;。</p><p>我们举个例子：</p><p>假设你上传了数据到了此路经下：</p><img src="https://s2.loli.net/2024/11/27/1xn5UVXD3p9NWH6.png" alt="image-20241127102259316" style="zoom:67%;" /><img src="https://s2.loli.net/2024/11/27/eDyZOpgkLwXjKIn.png" alt="image-20241127102246110" style="zoom:80%;" /><p>想要在R中引用，那么我们可以在R中看到这样的：</p><img src="https://s2.loli.net/2024/11/27/Em8BXquW9CUv5Td.png" alt="image-20241127102355410" style="zoom:67%;" /><p>那么在R_Studio中，代码调用的路径应该是：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">/home/个人用户名/Datasets/bionet/Dataset/gl_XU<br></code></pre></td></tr></table></figure><h1 id="三、imzML数据预处理-Cardinal"><a href="#三、imzML数据预处理-Cardinal" class="headerlink" title="三、imzML数据预处理(Cardinal)"></a>三、imzML数据预处理(Cardinal)</h1><p><em>Cardinal</em> 原生支持读写 imzML（包括 “continuous” 和 “processed” 类型），当然我们要处理的都是每个频谱具有不同的m&#x2F;z值的 “continuous” 类型数据。</p><p><em>Cardinal 3.6</em>引入了一套简单的新数据结构如下，用于组织MS成像实验的数据。</p><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20251120165051237.png" alt="image-20251119204732563" style="zoom:80%;" /><ul><li><p><code>MSImagingArrays</code>：<strong>Continuous (连续) 模式</strong>读取后格式，以下简称msa。</p><p><strong>数据存储</strong>: 在连续模式下，每个像素点的质谱数据（m&#x2F;z值和对应的强度值）都完整地、独立地存储。这意味着每个像素的光谱都是一个完整的数据列表。</p></li><li><p><code>MSImagingExperiment</code>：<strong>Processed (处理过) 模式</strong>读取后格式以下简称mse。</p><p><strong>数据存储</strong>: 在处理模式下，数据经过了“binning”处理。首先，会创建一个统一的、全局的m&#x2F;z轴（一个m&#x2F;z值的列表）。然后，每个像素的质谱数据不再存储具体的m&#x2F;z值，而是只存储与全局m&#x2F;z轴对应的强度值。如果某个像素在某个m&#x2F;z值上没有信号，强度值就记为零。 数据变得更加规整，像一个大的数据矩阵，其中行代表像素，列代表统一的m&#x2F;z通道。</p><p>理论上caldinal包提供了强制转换函数as()，但是会损失比较多的信息，所以需要预处理实现转换。</p></li></ul><h2 id="Cardinal包安装"><a href="#Cardinal包安装" class="headerlink" title="Cardinal包安装"></a>Cardinal包安装</h2><p>imzML文件的数据处理有很多不同的方式，在熟悉流程之后，你可以自己编写脚本来实现。<br>本文使用Cardinal包来进行预处理。Cardinal包支持MALDI和DESI的MSI工作，可以对生物样品进行基于质谱实验的统计分析。<br>    在开始前，你需要自己安装好RStudio或其他IDE。安装Cardinal包，使用以下命令：</p><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs cmake"><span class="hljs-keyword">if</span> (!require(<span class="hljs-string">&quot;BiocManager&quot;</span>, quietly = <span class="hljs-keyword">TRUE</span>))<br>    <span class="hljs-keyword">install</span>.packages(<span class="hljs-string">&quot;BiocManager&quot;</span>)<br><br>BiocManager::<span class="hljs-keyword">install</span>(<span class="hljs-string">&quot;Cardinal&quot;</span>)<br></code></pre></td></tr></table></figure><p>别忘了把刚安装好的包加载上：</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs stylus"><span class="hljs-function"><span class="hljs-title">library</span><span class="hljs-params">(Cardinal)</span></span><br></code></pre></td></tr></table></figure><p>到这里如果没有报错，说明安装成功了，你可以查看官方文档来进行进一步的了解。</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs stylus"><span class="hljs-function"><span class="hljs-title">browseVignettes</span><span class="hljs-params">(<span class="hljs-string">&quot;Cardinal&quot;</span>)</span></span><br></code></pre></td></tr></table></figure><h2 id="读取imzML文件"><a href="#读取imzML文件" class="headerlink" title="读取imzML文件"></a>读取imzML文件</h2><figure class="highlight clean"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs clean"># 读取continuous数据，注意替换你的文件地址，会自动一起读取ibd<br>path_continuous &lt;- <span class="hljs-string">&quot;C:/xxx/xxx.imzML&quot;</span><br>msa &lt;- readMSIData(path_continuous)<br></code></pre></td></tr></table></figure><p>通过spectraData函数你可以初步了解你的数据，一般情况下一个对象必须至少有两个为“mz”和“intensity”的数组，分别是m&#x2F;z数组和强度数组的列表。</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs stylus"><span class="hljs-function"><span class="hljs-title">spectraData</span><span class="hljs-params">(msa)</span></span><br></code></pre></td></tr></table></figure><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20251120165049155.png" alt="image-20251119212356657" style="zoom:80%;" /><p>我们可以读取一些数据出来查看：</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">mz</span>(msa)[<span class="hljs-number">1</span>:<span class="hljs-number">2</span>] # 提取前<span class="hljs-number">2</span>个mz数组<br><span class="hljs-attribute">intensity</span>(msa)[<span class="hljs-number">1</span>:<span class="hljs-number">2</span>] # 提取前<span class="hljs-number">2</span>个intensity数组<br></code></pre></td></tr></table></figure><p>你也可以查看实验元数据：</p><figure class="highlight scss"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs scss"><span class="hljs-built_in">pixelData</span>(msa)# 查看像素元数据，可以简写为pData<br></code></pre></td></tr></table></figure><p>运行后会给你提供三列，包含像素的x轴、y轴以及批次信息。</p><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20251120165046254.png" alt="image-20251119213149015" style="zoom:80%;" /><p>上面展示的也许不太直观，你可以通过plot来可视化查看质谱：</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs routeros">plot(msa, <span class="hljs-attribute">i</span>=c(1, 322) ,<span class="hljs-attribute">superpose</span>=<span class="hljs-literal">TRUE</span>)<br></code></pre></td></tr></table></figure><p>其中i后面跟的是像素，这里展示了第一个像素和第五个像素的图片，用superpose&#x3D;TRUE的参数重合对比。值得一提的是，根据刚才查到的元数据，图像是321x330大小，也就是说此时我的第322个像素等效于以下这种用像素坐标来指定的方式：</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs routeros">plot(msa, <span class="hljs-attribute">coord</span>=list(x=2, <span class="hljs-attribute">y</span>=1))<br></code></pre></td></tr></table></figure><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20251120165044240.png" alt="image-20251119215113394" style="zoom: 50%;" /><p>大致了解你手中的imzML数据后，可以开始进行数据预处理了。</p><h2 id="预处理流程"><a href="#预处理流程" class="headerlink" title="预处理流程"></a>预处理流程</h2><h3 id="归一化处理"><a href="#归一化处理" class="headerlink" title="归一化处理"></a>归一化处理</h3><p>支持的归一化方法包括：</p><ul><li><code>method=&quot;tic&quot;</code>执行全离子电流（TIC）归一化</li><li><code>method=&quot;rms&quot;</code>执行均方根（RMS）归一化</li><li><code>method=&quot;reference&quot;</code>将光谱归一化为参考特征</li></ul><figure class="highlight oxygene"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs oxygene">msa_nom &lt;- normalize(msa, <span class="hljs-keyword">method</span>=&quot;<span class="hljs-title function_">tic</span>&quot;)<br></code></pre></td></tr></table></figure><p>TIC归一化是最常见的方法。</p><h3 id="高斯平滑处理"><a href="#高斯平滑处理" class="headerlink" title="高斯平滑处理"></a>高斯平滑处理</h3><p>手册里提供了不同的平滑处理方式，使用gaussion就可以。</p><figure class="highlight oxygene"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs oxygene">msa_smo &lt;- smooth(msa_nom, <span class="hljs-keyword">method</span>=&quot;<span class="hljs-title function_">gaussian</span>&quot;)<br></code></pre></td></tr></table></figure><h3 id="基线校准"><a href="#基线校准" class="headerlink" title="基线校准"></a>基线校准</h3><p>校准基线到强度为0附近。</p><figure class="highlight oxygene"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs oxygene">msa_bac &lt;- reduceBaseline(msa_bac, <span class="hljs-keyword">method</span>=&quot;<span class="hljs-title function_">locmin</span>&quot;)<br></code></pre></td></tr></table></figure><h3 id="对齐校准"><a href="#对齐校准" class="headerlink" title="对齐校准"></a>对齐校准</h3><p>剖面光谱的对齐可以用来初步校准。</p><figure class="highlight oxygene"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs oxygene">peaks_drift &lt;- estimateReferencePeaks(msa_bac) #平均谱峰值<br><br>mse_nodrift &lt;- recalibrate(mse_drift, ref=peaks_drift,<br>                           <span class="hljs-keyword">method</span>=&quot;<span class="hljs-title function_">locmax</span>&quot;, <span class="hljs-title function_">tolerance</span>=1500, <span class="hljs-title function_">units</span>=&quot;<span class="hljs-title function_">ppm</span>&quot;)<br><span class="hljs-title function_">mse_nodrift</span> &lt;- <span class="hljs-title function_">process</span><span class="hljs-params">(mse_nodrift)</span> #用平均谱峰值进行处理<br></code></pre></td></tr></table></figure><p>这里process包含了（1）峰检测，（2）所有光谱上的峰值对齐，（3）滤波峰，实际使用中如果不需要其中的步骤可以根据手册只使用其中部分步骤。</p><h3 id="峰合并分组"><a href="#峰合并分组" class="headerlink" title="峰合并分组"></a>峰合并分组</h3><p>合并相近的峰以减少冗余。</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs routeros">mse_binned &lt;- bin(msa, <span class="hljs-attribute">resolution</span>=1, <span class="hljs-attribute">units</span>=<span class="hljs-string">&quot;mz&quot;</span>)<br></code></pre></td></tr></table></figure><p>改变resolution可改变合并的精度，但可能丢失一些峰。需要你根据实际情况调整。</p><h3 id="全流程处理"><a href="#全流程处理" class="headerlink" title="全流程处理"></a>全流程处理</h3><p>实际使用时cardinal包提供了队列化处理，你可以一次性提交整个流程：</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs routeros">mse_queue &lt;- msa |&gt;<br>    normalize() |&gt;<br>    smooth() |&gt;<br>    reduceBaseline() |&gt;<br>    process()<br><br><span class="hljs-comment"># preview processing</span><br>plot(mse_queue, <span class="hljs-attribute">coord</span>=list(x=16, <span class="hljs-attribute">y</span>=16), <span class="hljs-attribute">linewidth</span>=2<br></code></pre></td></tr></table></figure><h3 id="查看处理结果"><a href="#查看处理结果" class="headerlink" title="查看处理结果"></a>查看处理结果</h3><p>由于处理后已经由msa转变成mse，你可以使用image函数来比较预处理前后的图像结果了。</p><h3 id="输出"><a href="#输出" class="headerlink" title="输出"></a>输出</h3><p>注意修改mse_queue部分即可</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs stylus"><span class="hljs-selector-id">#save</span><br>imzfile &lt;- <span class="hljs-built_in">tempfile</span>(fileext=<span class="hljs-string">&quot;你的文件名.imzML&quot;</span>)<br><span class="hljs-function"><span class="hljs-title">writeMSIData</span><span class="hljs-params">(mse_queue, imzfile)</span></span><br>list<span class="hljs-selector-class">.files</span>(imzfile)<br></code></pre></td></tr></table></figure>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>Segment Model Compare</title>
    <link href="/2025/08/05/ModelCompare/"/>
    <url>/2025/08/05/ModelCompare/</url>
    
    <content type="html"><![CDATA[<h1 id="SegMamba"><a href="#SegMamba" class="headerlink" title="SegMamba"></a>SegMamba</h1><h2 id="Datasets"><a href="#Datasets" class="headerlink" title="Datasets"></a>Datasets</h2><ul><li>CRC-500：500个3D结直肠癌CT</li><li><strong>BraTS 2023：1251个3D脑MRI</strong></li><li><strong>AIIB2023：120个纤维化肺病CT</strong></li><li>公开数据集加粗，下同</li></ul><h2 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h2><p>SegMamba具有三部分：</p><ul><li>1）具有多个三向空间Mamba（TSMamba）块的3D特征编码器，用于以不同尺度对全局信息进行建模（正向、反向、切片间）</li><li>2）基于卷积层的3D解码器，用于预测分割结果</li><li>3）特征级不确定性估计（FUE）的跳跃连接用于特征增强。</li><li>实际上就是把卷积块换成TSMamba块，逐个尺寸Res跳连。</li><li>主干：k：7x7x7    p：3x3x3    s：2x2x2  –&gt; 48 D&#x2F;2 H&#x2F;2 W&#x2F;2</li><li>代码中实际上把Stem作为下采样第一层，图示少了一次下采样。</li></ul><p><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250417170640235.png" alt="SegMambaOverview"></p><h3 id="TSMamba块"><a href="#TSMamba块" class="headerlink" title="TSMamba块"></a>TSMamba块</h3><ul><li><p>Gated Spatial Convolution<strong>（谨慎学习这块内容，基本没用，代码部分会提到）</strong>:weary:</p><p>类似于门机制的信息传输，在mamba前捕获空间信息</p></li></ul><p><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250421193131000.png" alt="GSC"></p><p>​       </p><ul><li><p>Tri-orientated Mamba</p><p>前向、反向、切片间，对高维特征的全局信息进行建模。</p></li></ul><p><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250421193035107.png" alt="ToM"></p><p><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250421194438637.png" alt="TSMamba"></p><h3 id="Feature-level-Uncertainty-Estimation（谨慎学习这块内容，作者没有给出）"><a href="#Feature-level-Uncertainty-Estimation（谨慎学习这块内容，作者没有给出）" class="headerlink" title="Feature-level Uncertainty Estimation（谨慎学习这块内容，作者没有给出）"></a>Feature-level Uncertainty Estimation<strong>（谨慎学习这块内容，作者没有给出）</strong></h3><p>​       计算通道维度上的平均值，并进行归一化，增强低不确定性特征。</p><p><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250421193241411.png" alt="FUE"></p><h3 id="Loss-Train"><a href="#Loss-Train" class="headerlink" title="Loss &amp; Train"></a>Loss &amp; Train</h3><p>参数：使用交叉熵损失，具有多项式学习率调度器的SGD优化器（初始学习率为1^e-2^，衰减为1^e-5^）。所有数据集运行1000个epoch</p><p>数据增强：加性亮度、伽马、旋转、缩放、镜像和弹性变形。</p><h2 id="Code"><a href="#Code" class="headerlink" title="Code"></a>Code</h2><p><a href="https://github.com/ge-xing/SegMamba">https://github.com/ge-xing/SegMamba</a></p><p>吐槽下作者代码里实际上根本没有点乘，而是直接相加，加的位置也不对，和画的图&#x2F;公式完全是俩玩意。其实就是基本两个3x3+1x1跳连的残差块，外边再1x1之后和原来的输入跳连，和门控不大沾边，浪费好久去看这个结构。git记录也是稍显抽象。然后FUE根本没给，在issue回复很模糊。</p><p><img src="C:\Users\ForRiver\AppData\Roaming\Typora\typora-user-images\image-20250422153207119.png" alt="GSC实际代码"></p><p><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250422153424533.png" alt="git"></p><p>可学习的点：3D切片上ToM的正序反序、切片间顺序思路。</p><h1 id="3D-ResUNet"><a href="#3D-ResUNet" class="headerlink" title="3D-ResUNet"></a>3D-ResUNet</h1><h2 id="Datasets-1"><a href="#Datasets-1" class="headerlink" title="Datasets"></a>Datasets</h2><ul><li>三个非洲爪蟾肾胚胎样本</li></ul><h2 id="Model-1"><a href="#Model-1" class="headerlink" title="Model"></a>Model</h2><p>和标准U-Net基本上一样。这篇比较早结构也比较简单，直接看代码吧。</p><p><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250422211547355.png" alt="3D-ResUNet"></p><h2 id="Code-1"><a href="#Code-1" class="headerlink" title="Code"></a>Code</h2><p>没有找到原论文代码，找到类似的3D-ResUNet</p><p><a href="https://github.com/safamathl/3D-ResUnet/tree/main/net">https://github.com/safamathl/3D-ResUnet/tree/main/net</a></p><p>16→32→64→128，每级包含2-3个卷积层，找到的这个代码通过dilation在3、4层扩大感受野。concat合并通道，特征融合后接3个卷积层进行信息整合。3D相对原来的U-Net在z轴维度上的变化也没有额外处理。</p><p>以图中128+256作为例子，进行信息整合的3个卷积层：</p><table><thead><tr><th>层级</th><th>输入通道</th><th>输出通道</th><th>kernel_size</th><th>stride</th><th>padding</th><th>dilation</th><th>激活函数</th><th>参数量</th><th>计算量（FLOPs）</th></tr></thead><tbody><tr><td>第1层</td><td>128+64&#x3D;192</td><td>128</td><td>3×3×3</td><td>1</td><td>1</td><td>1</td><td>PReLU</td><td>192×128×3³&#x3D;<strong>663,552</strong></td><td>128×192×D×H×W×27</td></tr><tr><td>第2层</td><td>128</td><td>128</td><td>3×3×3</td><td>1</td><td>1</td><td>1</td><td>PReLU</td><td>128×128×3³&#x3D;<strong>442,368</strong></td><td>128×128×D×H×W×27</td></tr><tr><td>第3层</td><td>128</td><td>128</td><td>3×3×3</td><td>1</td><td>1</td><td>2</td><td>PReLU</td><td>128×128×3³&#x3D;<strong>442,368</strong></td><td>128×128×D×H×W×27</td></tr></tbody></table><p>可学习的点：适当的使用dilation，其他当做一种方法和最后我们的结果进行比较就好。</p><h1 id="LISA"><a href="#LISA" class="headerlink" title="LISA"></a>LISA</h1><h2 id="Datasets-2"><a href="#Datasets-2" class="headerlink" title="Datasets"></a>Datasets</h2><ul><li><strong>Semantic Segmentation Dataset</strong></li><li><strong>Vanilla Referring Segmentation Dataset</strong></li><li><strong>Visual Question Answering Dataset</strong></li></ul><h2 id="Model-2"><a href="#Model-2" class="headerlink" title="Model"></a>Model</h2><p>LLM输出掩码，其中分割掩码表示为多边形序列，使分割掩码能够表示为纯文本。</p><p>仅需在现有LLM（如LLaVA）基础上新增一个 <code>&lt;SEG&gt;</code> 标记和简单的解码器结构，避免复杂模型重构。</p><p><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250417203159867.png" alt="LISAOverview"></p><p> 作为触发分割的指令信号，嵌入到LLM的词汇表中。当模型需要输出分割结果时，响应文本中会包含此标记。</p><p>有<SEG>-&gt;大模型理解文本需要提取的内容-&gt;投影层连接文字信息与图片特征信息-&gt;SAM分割-&gt;LLM输入掩码-&gt;叠加掩码生成分割结果</p><p><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250418132430684.png" alt="掩码生成过程"></p><h3 id="Loss-Train-1"><a href="#Loss-Train-1" class="headerlink" title="Loss &amp; Train"></a>Loss &amp; Train</h3><p>Ltxt是用于文本生成的自回归交叉熵损失，Lmask是掩码损失。</p><p><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250418132411758.png" alt="损失"></p><ul><li><p><IMAGE> Can you segment the {class name} in this image?  语义分割数据集，输入图像和目标对象的名称</p></li><li><p><IMAGE>Can you segment {description} in this image？Vanilla参考分割数据集，输入图像和目标对象的显式简短描述</p></li><li><p>视觉问题分类数据集，提高多模态LLM的问题分类（VQA）能力</p><p>训练参数：微调LoRA，Fdec，embed tokens，Ilm head，投影层</p><p>可以学习的点：LLM指导分割任务，LLM掩码处理。</p></li></ul><h2 id="Code-2"><a href="#Code-2" class="headerlink" title="Code"></a>Code</h2><p>也是SAM作为predictor</p><p>可学习的点：结合SAM</p><h1 id="HyperSeg"><a href="#HyperSeg" class="headerlink" title="HyperSeg"></a>HyperSeg</h1><h2 id="Datasets-3"><a href="#Datasets-3" class="headerlink" title="Datasets"></a>Datasets</h2><h3 id="图像分割"><a href="#图像分割" class="headerlink" title="图像分割"></a>图像分割</h3><ul><li><strong>COCO Panoptic</strong></li><li><strong>RefCOCO系列</strong></li><li>COCO-Interactive</li><li><strong>ReasonSeg</strong></li></ul><h3 id="视频分割"><a href="#视频分割" class="headerlink" title="视频分割"></a>视频分割</h3><ul><li><strong>DAVIS 2017</strong></li><li><strong>Ref-Youtube-VOS</strong></li><li><strong>YouTube-VIS 2019</strong></li><li>ReVOS</li></ul><h2 id="Model-3"><a href="#Model-3" class="headerlink" title="Model"></a>Model</h2><p>把Vision Token（VLLM的CLIP）、FVP精细分割后的Fine-grained Token、视觉和文字的Prompt Token输入大模型，LoRA微调。</p><p>Prompt Token、Fine-grained Token和语义识别后的Mask Token一起输入分割预测器。</p><p><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250422162428312.png" alt="HyperSegOverview"></p><h3 id="VLLM"><a href="#VLLM" class="headerlink" title="VLLM"></a>VLLM</h3><p>输入（V，P），由CLIP编码器得到特征f<del>v</del>，投影并与Fine-grained Token、Prompt Token级联得到输出。输出后的Hybrid Entity Reconition的语义增强掩码标记EQ是手动提取的。</p><p><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250422165119843.png" alt="VLLM Output"></p><h3 id="Prompt"><a href="#Prompt" class="headerlink" title="Prompt"></a>Prompt</h3><p>P<del>I</del>：文本提示 P<del>C</del>：图片提示 具体设计看上面的Overview就可以了</p><h3 id="Segmentation-predictor"><a href="#Segmentation-predictor" class="headerlink" title="Segmentation predictor"></a>Segmentation predictor</h3><p>m<del>j</del>是掩码建议，z<del>j</del>是分类得分，e<del>j</del>进针对视频用不上。</p><p><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250422171137406.png" alt="分割结果输出"></p><h3 id="CLIP和FVP区别"><a href="#CLIP和FVP区别" class="headerlink" title="CLIP和FVP区别"></a>CLIP和FVP区别</h3><p>CLIP单层简单结构，对细粒度感知效果不强。</p><p><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250422200535318.png" alt="CLIP/FVP块"></p><h2 id="Train-Loss"><a href="#Train-Loss" class="headerlink" title="Train &amp; Loss"></a>Train &amp; Loss</h2><p>该模型可以使用统一损失L在多个任务上联合训练。L<del>text</del>采用自回归交叉熵，L<del>mask</del>采用BCELoss和DiceLoss，L<del>cls</del>交叉熵来分类，L<del>ins</del>用于视频暂时也用不上。</p><p><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250422171444470.png" alt="Loss"></p><p>可学习的点：细粒度token的加入，prompt token的设计。</p><h1 id="LLMSeg"><a href="#LLMSeg" class="headerlink" title="LLMSeg"></a>LLMSeg</h1><h2 id="Datasets-4"><a href="#Datasets-4" class="headerlink" title="Datasets"></a>Datasets</h2><ul><li><p>981乳腺癌患者CT</p></li><li><p>内部验证集，提供了一小部分具有相似特征的受试者样本，而不是完整的患者数据集，用于验证目的</p></li><li><p><a href="https://github.com/tvseg/MM-LLMRO44%E3%80%82">https://github.com/tvseg/MM-LLMRO44。</a></p></li></ul><h2 id="Model-4"><a href="#Model-4" class="headerlink" title="Model"></a>Model</h2><p>大概思路和上面的LLM指导分割任务差不多，用的3D ResUNet + Llama2-7B-chat + sam</p><p><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250423142533886.png" alt="LLMSeg Overview"></p><p>主要有三个模块：</p><ul><li><p>text prompt tuning</p><p>v<del>n</del>：MxD矩阵，D与LLM的嵌入维度相同，进行随机初始化。</p><p>t &#x3D; v<del>n</del> + 诊断内容[TEXT] + 分割标记[SEG] (LISA也用到这个标记)，输出g<del>NxD</del></p></li><li><p>multimodal interactive alignment</p><p>对齐g与图像嵌入，图像嵌入f<del>l</del>为图像编码器第l层输出，维度H<del>l</del> x W<del>l</del> x S<del>l</del> x C<del>l</del> </p><p>逐层使用线性层对齐L与投影g，然后self-attention与cross-attention进行信息交互（这块就是SAM的双向注意力），最后得到f<del>l</del>^*^</p></li><li><p>CTV delineation</p><p>SAM预测，用CE和Dice来计算损失，softmax出y<del>hat</del></p></li></ul><p><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250423143907830.png" alt="LLMSeg Model"></p><h2 id="Train"><a href="#Train" class="headerlink" title="Train"></a>Train</h2><p>随机裁剪384 x 384 x 128的patch，训练冻结LLM，学习text prompt，对齐时的参数和SAM参数。</p><h2 id="Code-3"><a href="#Code-3" class="headerlink" title="Code"></a>Code</h2><p>代码中ResUnet和上文找到的ResUnet差不多，具体的大小如下：</p><p><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250423165327839.png" alt="ResUNet Param"></p><p>5个下采样 + g<del>NxD</del>信息，为了对齐通道会减去N：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs i">in_channels = feature_size * 4 - (args.n_prompts if args.align_score  else 0 if self.context  else 0)<br></code></pre></td></tr></table></figure><p>Encoder用的monai给的Unetr块，Decoder前四块为加了残差的上采样+卷积，最后一块简单的用转置卷积实现上采样。使用了SAM的双向注意力来交互文字与图片信息。</p><p>可学习的点：不再是直接把text和image连接进sam，two ways attention加在ResUNet跳连部分，接着再去做decoder。</p><h2 id="L-C"><a href="#L-C" class="headerlink" title="L&amp;C"></a>L&amp;C</h2><p>用4个MRI序列做多模态输入（T1、T1增强一组&#x2F;T2、DWI一组互注意力）。</p><p>3DResUnet+（顺序切片间（这样是不是2D更好？））做分割模型，高层使用自适应dilation不减小尺寸扩大感受野。</p><p>Two ways Attention可以加在跳连&#x2F;后处理门控？（加在Encoder、Decoder可能运算过大）</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>DeepS汇报</title>
    <link href="/2025/08/05/DeepS/"/>
    <url>/2025/08/05/DeepS/</url>
    
    <content type="html"><![CDATA[<h1 id="DeepS：通过深度神经网络加速3D质谱成像"><a href="#DeepS：通过深度神经网络加速3D质谱成像" class="headerlink" title="DeepS：通过深度神经网络加速3D质谱成像"></a>DeepS：通过深度神经网络加速3D质谱成像</h1><p><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250405185957411.png" alt="所汇报文献"></p><h2 id="研究目的"><a href="#研究目的" class="headerlink" title="研究目的"></a>研究目的</h2><p>3D-MSI相对于2D来说，可以绘制复杂生物结构的分子分布图。本文提出一种多尺度采样单元的系数采样策略，通过3D稀疏网络重建。并提出DeepS工作流程和3D-SSNet，旨在<strong>在20-30%的采样率</strong>下也能得到和全采样近似的结果，缩短3D MSI技术分析构建的时长。</p><h2 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h2><p><strong>3D阿尔兹海默症小鼠脂质数据集</strong></p><p>PDX胶质母瘤小鼠脑数据集</p><p>小鼠肾脏MALDI数据集</p><h2 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h2><h3 id="稀疏采样策略"><a href="#稀疏采样策略" class="headerlink" title="稀疏采样策略"></a>稀疏采样策略</h3><p>传统方法：集合S中按预定顺序选取1x1大小子集。</p><p>本文方法：定义S由多个采样单元（正方形区域）构成，H x W会调整为H&#x2F;k x W&#x2F;k，不能整除部分填0。而s由人工掩码（稀疏化全采样数据时）和操作掩码决定。</p><p><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250405185957412.png" alt="掩码处理"></p><p><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250406235949369.png" alt="不同k值图像示例"></p><p>所有<strong>未采样化为三通道（0,1,0）</strong>，采样复制值到三通道，以此区分未采样像素和低强度采样像素。</p><p>论文地址未提供randon_mask函数代码。</p><h3 id="3D-SSNet模型"><a href="#3D-SSNet模型" class="headerlink" title="3D-SSNet模型"></a>3D-SSNet模型</h3><p><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250405185957413.png" alt="SSNet模型"></p><p>Generator：稀疏图像重建。</p><p><strong>Unet：</strong>收缩：4x下采样-卷积 扩展：卷积 + 4x上采样。</p><p>Discriminator：区分真实、重建样本。</p><p>步长2的六块4x4卷积层。</p><p><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250405185957414.png" alt="UNet结构"></p><p>Loss：</p><figure class="highlight abnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs abnf"><span class="hljs-attribute">criterion</span> <span class="hljs-operator">=</span> nn.BCELoss()<br><span class="hljs-attribute">criterionL1</span> <span class="hljs-operator">=</span> nn.L1Loss()<br></code></pre></td></tr></table></figure><p><strong>netD:</strong></p><p><strong>errD_real</strong>：判别器对真实图像的判断损失</p><ul><li>使用二元交叉熵（<code>BCELoss</code>），标签为<code>real_label=1</code>。</li><li>目标：最大化判别器对真实图像的识别能力（输出接近1）。</li></ul><p><strong>errD_fake</strong>：判别器对生成图像的判断损失</p><ul><li>同样使用<code>BCELoss</code>，标签为<code>fake_label=0</code>。</li><li>目标：最大化判别器对生成图像的识别能力（输出接近0）。</li></ul><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs ini"><span class="hljs-attr">output</span> = netD(input_real)<br><span class="hljs-attr">errD_real</span> = criterion(output, label)<br><span class="hljs-attr">output</span> = netD(fake.detach())<br><span class="hljs-attr">errD_fake</span> = criterion(output, label)<br><span class="hljs-attr">errD</span> = errD_real + errD_fake<br></code></pre></td></tr></table></figure><p><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250407000048202.png" alt="Ld"></p><p><strong>netG:</strong></p><p><strong>errG_D</strong>：生成器对抗损失</p><ul><li>使用<code>BCELoss</code>，但标签为<code>real_label=1</code>（欺骗判别器）。</li><li>目标：生成图像使判别器输出接近1（误判为真）。</li></ul><p><strong>errG_l1</strong>：L1重建损失</p><ul><li>使用<code>L1Loss</code>，计算生成图像与真实图像的像素级差异。</li><li>目标：约束生成图像与真实图像的结构一致性。</li></ul><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs ini"><span class="hljs-attr">errG_D</span> = criterion(output, label)<br><br>        <br>        <span class="hljs-attr">errG_l1</span> = criterionL1(fake, input_real)<br>        <span class="hljs-attr">errG_l1</span> = errG_l1.mean()<br><br>        <span class="hljs-attr">errG</span> = (<span class="hljs-number">1</span> - wtl2) * errG_D + wtl2 * errG_l1<br></code></pre></td></tr></table></figure><p><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250405185957415.png" alt="Lg"></p><h3 id="训练过程"><a href="#训练过程" class="headerlink" title="训练过程"></a>训练过程</h3><ul><li><p>从集合{1，2，4，8，16}中选择一个值作为每个离子图像的采样单元的大小，生成一个随机的人工掩码，并执行模拟稀疏采样，遮挡80%。为每个epoch生成新的人工掩码，使3DSSNet了解截面的结构特征、适应不同大小的采样单元。</p></li><li><p>计算真实图像和生成图像的判别损失（<code>errD_real</code>&#x2F;<code>errD_fake</code>），反向传播更新<code>netD</code>。</p></li><li><p>生成图像（<code>fake = netG(input_cropped)</code>），计算对抗损失（<code>errG_D</code>）和L1损失（<code>errG_l1</code>），反向传播更新<code>netG</code>。</p></li><li><p>如果有提升则保存参数，每五个epoch学习率衰减20% 。</p></li><li><p><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250408171608959.png" alt="训练过程"></p></li></ul><p>在测试阶段，每个部分只生成一个操作掩码（M#2），由来自相同截面的所有离子图像共享。不需要使用相同的k值，k可以根据偏好设置。在此阶段，使用生成器网络，并丢弃了递归网络。</p><p>为了在数据多样性和数据收集成本之间取得平衡，DeepS利用三个切片的全采样数据作为训练数据集，根据沿着Z轴的等距标准进行选择。剩余的组织切片被稀疏采样并用于测试。</p><p><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250408173223616.png" alt="DeepS总流程"></p><h2 id="训练结果"><a href="#训练结果" class="headerlink" title="训练结果"></a>训练结果</h2><p><strong>Dataset：3D阿尔茨海默氏病小鼠脂质数据集</strong></p><p>通过对采样率的对比实验表明，在模型训练过程中，<strong>采样率越低，重建效果越好</strong>。训练参数默认设置如下：50个训练时期，20%的模拟稀疏采样率，批量大小为8，剩余的组织切片被稀疏地采样以评估具有各种采样单元尺寸和采样比的模型的重建性能。</p><p>在海马区选择一个10 × 10的区域来评估。计算了特定区域的平均质谱，总体平均绝对误差（MAE）为5.861 × 10−6，与原始光谱高度可比。</p><p>固定k值，模型可以有效地恢复不同采样率下的离子图像，几乎所有的离子图像在50%的采样率下都被很好地重建，这由所有离子图像的PSNR值大于30和SSIM值大于0.8来证明。在20%的采样率下，超过90%的离子图像被良好地重建，PSNR和SSIM值分别高于27和0.7。</p><p><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250405185957417.png" alt="不同采样率重建效果"></p><p>固定20%采样率，在k&#x3D;32后PSNR、SSIM数值显著减小。</p><p><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250405185957418.png" alt="不同掩码大小重建效果"></p><p>模型迁移到胶质母细胞瘤PDX小鼠脑模型，该模型对不同质荷比和不同组织切片具有较高的重建精度。</p><p>PDX和正常小鼠脑数据集在空间特征上存在差异，但该模型对两者都具有较好的重建效果。</p><p><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250405185957419.png" alt="不同质荷比重建效果"></p>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
