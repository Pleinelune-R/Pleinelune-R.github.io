

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">

  <link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.png">
  <link rel="icon" href="/img/fluid.png">
  

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="Pleinelune">
  <meta name="keywords" content="Pleinelune">
  
    <meta name="description" content="TransformerAttentionAttention的结构如下：   现在我们拥有Query、Key、Value三个向量矩阵，假设我们在视频网站要查询所需要的内容，现在Q是你想要查询的内容Query，K是视频标签、关键词等Keywords，V就是Video本身了。 注意力的作用可以看做对查询和键来计算余弦相似性。假设每行代表一个单词，其对于Q、K矩阵的一行Q1·K1 &#x3D; |Q1|·">
<meta property="og:type" content="article">
<meta property="og:title" content="从Transformer到不同多模态工作">
<meta property="og:url" content="http://pleinelune-r.github.io/2025/05/10/Transformer%E4%B8%8E%E5%A4%9A%E6%A8%A1%E6%80%81/index.html">
<meta property="og:site_name" content="Pleineblog">
<meta property="og:description" content="TransformerAttentionAttention的结构如下：   现在我们拥有Query、Key、Value三个向量矩阵，假设我们在视频网站要查询所需要的内容，现在Q是你想要查询的内容Query，K是视频标签、关键词等Keywords，V就是Video本身了。 注意力的作用可以看做对查询和键来计算余弦相似性。假设每行代表一个单词，其对于Q、K矩阵的一行Q1·K1 &#x3D; |Q1|·">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250513171957853.png">
<meta property="og:image" content="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250513172050816.png">
<meta property="og:image" content="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250513172124423.png">
<meta property="og:image" content="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250513172140809.png">
<meta property="og:image" content="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250512152733146.png">
<meta property="og:image" content="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250513141203365.png">
<meta property="og:image" content="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250513161026879.png">
<meta property="og:image" content="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250512163112813.png">
<meta property="og:image" content="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250512165338090.png">
<meta property="og:image" content="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250512165902633.png">
<meta property="og:image" content="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250510215124317.png">
<meta property="og:image" content="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250510215607755.png">
<meta property="og:image" content="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250510214719681.png">
<meta property="og:image" content="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250510220213424.png">
<meta property="og:image" content="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250510221154785.png">
<meta property="og:image" content="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250512002336683.png">
<meta property="og:image" content="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250513162024567.png">
<meta property="og:image" content="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250513162108984.png">
<meta property="og:image" content="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250513161725376.png">
<meta property="og:image" content="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250513161754752.png">
<meta property="og:image" content="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250512120226522.png">
<meta property="og:image" content="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250512121417258.png">
<meta property="og:image" content="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250512122146514.png">
<meta property="og:image" content="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250513164612673.png">
<meta property="og:image" content="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250513165158262.png">
<meta property="og:image" content="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250513170238887.png">
<meta property="og:image" content="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250513170356130.png">
<meta property="article:published_time" content="2025-05-10T13:40:00.650Z">
<meta property="article:modified_time" content="2025-05-14T11:42:20.788Z">
<meta property="article:author" content="Pleinelune">
<meta property="article:tag" content="Pleinelune">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250513171957853.png">
  
  
  
  <title>从Transformer到不同多模态工作 - Pleineblog</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1749284_5i9bdhy70f8.css">



<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1736178_k526ubmyhba.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"pleinelune-r.github.io","root":"/","version":"1.9.8","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":{"measurement_id":null},"tencent":{"sid":null,"cid":null},"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false},"umami":{"src":null,"website_id":null,"domains":null,"start_time":"2024-01-01T00:00:00.000Z","token":null,"api_server":null}},"search_path":"/local-search.xml","include_content_in_search":true};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  


  <style>img { display: block; margin-left: auto; margin-right: auto; }</style>
<meta name="generator" content="Hexo 7.3.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>PleineBlog</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/" target="_self">
                <i class="iconfont icon-home-fill"></i>
                <span>首页</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/" target="_self">
                <i class="iconfont icon-archive-fill"></i>
                <span>归档</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/" target="_self">
                <i class="iconfont icon-category-fill"></i>
                <span>分类</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/" target="_self">
                <i class="iconfont icon-tags-fill"></i>
                <span>标签</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/" target="_self">
                <i class="iconfont icon-user-fill"></i>
                <span>关于</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/default.jpg') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="从Transformer到不同多模态工作"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2025-05-10 21:40" pubdate>
          2025年5月10日 晚上
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          3.2k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          27 分钟
        
      </span>
    

    
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <h1 id="seo-header">从Transformer到不同多模态工作</h1>
            
            
              <div class="markdown-body">
                
                <h2 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h2><h3 id="Attention"><a href="#Attention" class="headerlink" title="Attention"></a>Attention</h3><p>Attention的结构如下：</p>
<img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250513171957853.png" srcset="/img/loading.gif" lazyload alt="Attention Structure" style="zoom:67%;" />

<p>现在我们拥有Query、Key、Value三个向量矩阵，假设我们在视频网站要查询所需要的内容，现在Q是你想要查询的内容Query，K是视频标签、关键词等Keywords，V就是Video本身了。</p>
<p>注意力的作用可以看做对查询和键来计算余弦相似性。假设每行代表一个单词，其对于Q、K矩阵的一行Q<del>1</del>·K<del>1</del> &#x3D; |Q<del>1</del>|·|K<del>1</del>|·cosθ为一个数。Q<del>1</del>和K<del>1</del>的方向相同时（即它们的夹角接近0），cosθ 接近1，所以Q<del>1</del>·K<del>1</del>较大。这表明Q<del>1</del>和K<del>1</del>很相似。反之，当 Q<del>1</del>和K<del>1</del>的方向相反时（即它们的夹角接近180度），cos(θ) 接近 -1，所以Q<del>1</del>·K<del>1</del>较小，甚至为负。这表明Q<del>1</del>和K<del>1</del>不相似。</p>
<p>根据每行得出的相似度来计算得分，赋予权重，就完成了一次注意力计算。</p>
<p>具体来说，其公式为：</p>
<p>$$<br>\text { Attention }(Q, K, V)&#x3D;\operatorname{softmax}\left(\frac{Q K^{T}}{\sqrt{d_{k}}}\right) V<br>$$<br>也就是计算相似度得分后除以键向量维度的平方根，防止内积过大，使梯度更稳定。然后再进行softmax归一化后输出结果。输出的结果再去乘以V作为最后自注意力输出的Z矩阵。</p>
<img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250513172050816.png" srcset="/img/loading.gif" lazyload alt="Attention Output" style="zoom: 50%; "/>

<h3 id="Self-Attention"><a href="#Self-Attention" class="headerlink" title="Self-Attention"></a>Self-Attention</h3><p>自注意力，顾名思义就是自己对自己做注意力机制。把我们仅有的初始矩阵X投影成Query、Key、Value三个向量矩阵（实际上是与可学习的矩阵做乘法），之后再做刚才的注意力机制操作。</p>
<p>自注意力是可以并行计算的，这与RNN逐个重复处理词元相比较，缺少了顺序信息。所以输入中要添加位置编码来注入绝对或相对位置信息。这个位置编码可以自己编写，也可以学习得到。</p>
<img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250513172124423.png" srcset="/img/loading.gif" lazyload alt="self-attention QKV" style="zoom:50%;" align="center"/>

<h3 id="Multihead-Self-Attention"><a href="#Multihead-Self-Attention" class="headerlink" title="Multihead Self-Attention"></a>Multihead Self-Attention</h3><p>实际上多头注意力就是多个Self-Attention的连结，拼接起来再去乘以权重矩阵W<del>0</del>，每个头可能会关注输入不同的部分。</p>
<img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250513172140809.png" srcset="/img/loading.gif" lazyload alt="Multihead Self-attention Output" style="zoom:67%;" align="center"/>

<p>在Transformer Decoder部分你还能看到Maked Multihead Attention，相较于多头注意力增加了一个掩码。包括：</p>
<ul>
<li>padding mask：由于输入序列长度不一样，给交短序列后填0，截取较长序列。mask做的就是把这些位置值加上非常大负数，使其softmax后几乎为0，Attention无法注意到。</li>
<li>causal（sequence） mask：由于Decoder时生成需要是因果的，t时刻后的输出不应该出现在输入内，所以用一个上三角矩阵作用在序列上。</li>
</ul>
<h3 id="Add-Normalize"><a href="#Add-Normalize" class="headerlink" title="Add &amp; Normalize"></a>Add &amp; Normalize</h3><p>经过多头注意力的输出Z还要进行Add和Norm两个操作。Add就是加上一个残差块，防止发生退化，而Layer Normalization则负责归一化加快收敛速度。</p>
<img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250512152733146.png" srcset="/img/loading.gif" lazyload alt="Transformer Encoder" style="zoom: 50%;" align="center" />

<p>LN层针对同一样本的不同神经元进行归一化，在NLP中有用，而BN针对同一个batch不同样本同一位置神经元归一化，对NLP的词向量来说没有意义。</p>
<img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250513141203365.png" srcset="/img/loading.gif" lazyload alt="LN、BN、IN" style="zoom: 50%;" />

<h3 id="Feed-Forward-Network"><a href="#Feed-Forward-Network" class="headerlink" title="Feed Forward Network"></a>Feed Forward Network</h3><p>全连接层FFN公式如下：<br>$$<br>F F N(x)&#x3D;\max (0, x W 1+b 1) W 2+b 2<br>$$<br>先进行线性变换，ReLU后再线性变换，把输入映射到高位空间筛选后变回来。</p>
<p>&lt;大小描述&gt;</p>
<h3 id="整体结构"><a href="#整体结构" class="headerlink" title="整体结构"></a>整体结构</h3><p>Transformer整体结构就如下图所示，其中原文由6层Encoder和6层Decoder组成。</p>
<img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250513161026879.png" srcset="/img/loading.gif" lazyload alt="Transformer Overview" style="zoom:50%;" />

<p>整个工作流程大概是</p>
<ul>
<li><p>输入Input Embedding经过6个Encoder生成Z矩阵</p>
</li>
<li><p>输入OutputEmbedding在掩码多头注意力后，QK用Encoder生成的矩阵（第二个Decoder没有做Self-attention而只是Attention），输出所推理的掩码下一个单词。</p>
</li>
</ul>
<h3 id="Vision-Transformer"><a href="#Vision-Transformer" class="headerlink" title="Vision Transformer"></a>Vision Transformer</h3><p>Transformer在NLP任务上的成功引起了广泛注意，其在视觉上的应用Vision Transformer很快便推出了，该模型由三个模块组成：</p>
<ul>
<li>Linear Projection of Flattened Patches</li>
<li>Transformer Encoder</li>
<li>MLP Head（最后用于分类）</li>
</ul>
<img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250512163112813.png" srcset="/img/loading.gif" lazyload alt="VIT Overview" style="zoom:67%;" />

<p>标准的Transformer模块，要求输入的是token序列[num_token, token_dim]，而图像是[H, W, C]。假如我输入的是一个[224，224 ，3]的图像，我可以通过大小为16x16，步长为16，卷积核个数为768的卷积提取特征为[14，14，768]，然后把它平铺成[196，768]大小的二维矩阵，实现Transformer输入格式。在进入Encoder前，注意要把Position Embedding加入。原论文中再起是的向量处还加入了大小为[1,768]的[class]token，这个可训练参数和其他数据拼接在一起。</p>
<p>和图片上展示的一样，实际上16 x 16，步长为16的卷积就相当于把这个图片分为196块这个大小的块分别提取特征，这样后面加上位置信息就更有道理。</p>
<img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250512165338090.png" srcset="/img/loading.gif" lazyload alt="Linear Projection of Flattened Patches" style="zoom:67%;" />

<p>Transformer Encoder部分不再介绍，MLP层其实接近FFN。</p>
<p>经过Encoder后输出的仍是[197，768]大小的矩阵，我们把首位的[class]token对应的[1,768]调出来，然后通过MLP Head得到最后的分类结果，以免你不知道用哪个位置的token进行后面的MLP分类操作。</p>
<img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250512165902633.png" srcset="/img/loading.gif" lazyload alt="MLP Head" style="zoom:50%;" />



<h2 id="CLIP及其改进"><a href="#CLIP及其改进" class="headerlink" title="CLIP及其改进"></a>CLIP及其改进</h2><h3 id="CLIP简介"><a href="#CLIP简介" class="headerlink" title="CLIP简介"></a>CLIP简介</h3><p>CLIP（Contrastive Language-Image Pre-Training）模型是OPENAI在2021年发布的多模态预训练神经网络，核心是用大量的图像和文本配对数据来预训练对齐。</p>
<p>CLIP包含的主要部分为：</p>
<ul>
<li>Text Encoder：文本-&gt;低维向量</li>
<li>Image Encoder：图像-&gt;低维向量</li>
</ul>
<h3 id="CLIP学习"><a href="#CLIP学习" class="headerlink" title="CLIP学习"></a>CLIP学习</h3><ul>
<li>优势：出色的zero-shot效果</li>
<li>劣势：需要大量数据，难以完成复杂任务。</li>
<li>可学习之处：文本-图像构建正负样本对及点积相似性分类，对下面文章的指导。</li>
</ul>
<h3 id="Image-Encoder"><a href="#Image-Encoder" class="headerlink" title="Image Encoder"></a>Image Encoder</h3><p>CLIP使用的Encoder可以是修改后的ResNet50（<strong>注意力池化机制</strong>替代<strong>全局平均池化层</strong>），通过单层的多头QKV注意力，Q基于图像的全局平均池化；也可以是修改后的ViT，在Transformer前对Patch+Possition Embedding进行了归一化处理。</p>
<img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250510215124317.png" srcset="/img/loading.gif" lazyload alt="Image Encoder:VIT" style="zoom: 80%;" />

<h3 id="Text-Encoder"><a href="#Text-Encoder" class="headerlink" title="Text Encoder"></a>Text Encoder</h3><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250510215607755.png" srcset="/img/loading.gif" lazyload alt="Text Encoder：Transformer" style="zoom: 50%;" />

<p>由下图，CLIP实际上在训练中通过生成文本、图像的相似度来生成预测 $| I1 |·|T1|·cosθ$，一个文本和对应图片生成n个正样本和其他的负样本。</p>
<img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250510214719681.png" srcset="/img/loading.gif" lazyload alt="pre-training" style="zoom:67%;" />

<p>为了达到更好的效果，不能直接把单个词语与图片输入，而是先做一个Prompt template之后输入。</p>
<p>推理时把图片特征和文本特征计算相似性，输出分类结果。</p>
<img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250510220213424.png" srcset="/img/loading.gif" lazyload alt="Prompt Template & Predict" style="zoom:67%;" />

<p>由于把文字和图像特征结合学习到了结构化的部分知识，CLIP一定程度上摆脱了categorical label的限制，可以跳出训练时给出的label，例如做物体检测时，你在训练时给出的蓝色玩具、绿色玩具可以被检测为大象玩具、鸭子玩具等等。</p>
<img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250510221154785.png" srcset="/img/loading.gif" lazyload alt="eg" style="zoom:67%;" />

<h3 id="CLIP分割应用"><a href="#CLIP分割应用" class="headerlink" title="CLIP分割应用"></a>CLIP分割应用</h3><h4 id="LSeg"><a href="#LSeg" class="headerlink" title="LSeg"></a>LSeg</h4><p>N个需要分割类别：N x C Text矩阵</p>
<p>图片：H x W x C 在C维度上相乘 </p>
<p>结果：H x W x N</p>
<p>由于分割数据集一般较小，会把CLIP改为有监督训练，计算交叉熵，和CLIP实际上有区别。</p>
<h4 id="GroupViT"><a href="#GroupViT" class="headerlink" title="GroupViT"></a>GroupViT</h4><p>Image通过ViT的Transformer layer -  Group聚类（重复后减少）</p>
<p>Avg Pooling后MLP，和Text的MLP做余弦相似（CLIP）</p>
<p>返回去对每一个Group进行相似分类。</p>
<p>由于CLIP特性，很可能分离的Mask正确而类别错误，没有正确学习语义，且最多智能检测到最后一层Group类。</p>
<h2 id="多模态经典论文"><a href="#多模态经典论文" class="headerlink" title="多模态经典论文"></a>多模态经典论文</h2><p>以ViLT论文里展示的四种语言-视觉融合模型来说，CLIP处在B类。A、B两类的参数量更大，计算相似性部分则较轻，复杂情况可能难以得到好的结果。而C、D两种的训练时间长，要求高。</p>
<p>在多模态任务里，视觉特征是要大于文本特征的，所以Visual Embed应该不是很小，而Modality Interaction如果想处理更复杂的模型，应该也要较大，类似图中C类。</p>
<p>关于多模态任务使用的Loss，大概有Image Text Contrastive（CLIP中最大化N*N图像-文本对余弦相似度）、Mask Language Modeling（BERT中遮住一个词进行完形填空）、Image Text Matching（二分类判断生成的图像-文本对联合表示是否匹配）</p>
<img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250512002336683.png" srcset="/img/loading.gif" lazyload alt="four categories of vision-and-language models" style="zoom: 67%;" />

<h3 id="ALBEF：Align-before-Fuse-Vision-and-Language-Representation-Learning-with-Momentum-Distillation"><a href="#ALBEF：Align-before-Fuse-Vision-and-Language-Representation-Learning-with-Momentum-Distillation" class="headerlink" title="ALBEF：Align before Fuse: Vision and Language Representation Learning with Momentum Distillation"></a>ALBEF：Align before Fuse: Vision and Language Representation Learning with Momentum Distillation</h3><h4 id="方法总结"><a href="#方法总结" class="headerlink" title="方法总结"></a>方法总结</h4><ul>
<li>优势：动量蒸馏处理噪声数据，标准化三个loss，训练成本相对亲民。</li>
<li>劣势：无法做多模态深度融合、难以完成复杂下游任务。</li>
<li>可学习之处：处理噪声数据、处理负样本太多导致ITM训练困难问题。</li>
</ul>
<h4 id="具体内容"><a href="#具体内容" class="headerlink" title="具体内容"></a>具体内容</h4><p>CLIP模型并没有学习更丰富的多模态交互，很多工作也容易对噪声文本进行过拟合。ALBEF旨在解决这些问题。</p>
<ul>
<li>Image Encoder：ViT</li>
<li>Text Encoder：BERT前六层</li>
<li>Multimodal Encoder：BERT后六层</li>
</ul>
<p>相对于CLIP，它的Text Encoder较轻而Modality Encoder较重，有点类似C类。</p>
<p>Loss：ITC（在多模态融合前先对齐) + ITM（负样本太多直接训练难，选择最接近正样本的负样本） + MLM</p>
<img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250513162024567.png" srcset="/img/loading.gif" lazyload alt="ALBEF Overview" style="zoom: 67%;" />

<p>用动量蒸馏模型给图像-文本对比学习和掩码语言建模生成伪标签，不采用影响训练噪声数据（可能符合但不与Ground Truth相符），扩大有用数据集，one-hot and multi-hot。</p>
<img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250513162108984.png" srcset="/img/loading.gif" lazyload alt="ITC/MIM examples" style="zoom: 67%;" />

<h3 id="VLMo-Unified-Vision-Language-Pre-Training-with-Mixture-of-Modality-Experts"><a href="#VLMo-Unified-Vision-Language-Pre-Training-with-Mixture-of-Modality-Experts" class="headerlink" title="VLMo: Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts"></a>VLMo: Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts</h3><h4 id="方法总结-1"><a href="#方法总结-1" class="headerlink" title="方法总结"></a>方法总结</h4><ul>
<li>优势：Encoder使用灵活；可以使用单模态数据。</li>
<li>劣势：训练速度非常慢。</li>
<li>可学习之处：单模态训练流程优化、单塔模型及MoME。</li>
</ul>
<h4 id="具体内容-1"><a href="#具体内容-1" class="headerlink" title="具体内容"></a>具体内容</h4><p>CLIP、ALIGN等双塔模型，Image、Text单独抽取特征互不影响，但在一部分下游任务不如使用Fusion  Encoder的单塔模型。但是单塔模型在检索任务上又不适合大数据集，推理时间非常慢。</p>
<p>VLMo在Transformer中做了一部分改动：MoME Transformer。在Feed Forward Network中有区分Vision、Language和Vision-Language，在不同模态使用，self-attention可以共享参数。在使用中很灵活，但是由于要存储不同情况下的数据，要forward多次，速度非常慢，自己训练不现实。</p>
<p>使用Loss也是ITC、ITM、MLM。</p>
<img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250513161725376.png" srcset="/img/loading.gif" lazyload alt="VLMo Overview" style="zoom:80%;" />

<p>训练时分阶段，使用单模态数据集单独预训练Vision、Language，大大减轻需要的多模态数据集。</p>
<img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250513161754752.png" srcset="/img/loading.gif" lazyload alt="Pre-training" style="zoom:80%;" />

<h3 id="BLIP：Bootstrapping-Language-Image-Pre-training-for-Unified-Vision-Language-Understanding-and-Generation"><a href="#BLIP：Bootstrapping-Language-Image-Pre-training-for-Unified-Vision-Language-Understanding-and-Generation" class="headerlink" title="BLIP：Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation"></a>BLIP：Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation</h3><h4 id="方法总结-2"><a href="#方法总结-2" class="headerlink" title="方法总结"></a>方法总结</h4><ul>
<li>优势：能处理噪声数据，扩大数据集；可以完成生成任务。</li>
<li>劣势：训练还是很慢，存在误分类问题（实际上还是CLIP带来的）。</li>
<li>可学习之处：对训练流程的优化。</li>
</ul>
<h4 id="具体内容-2"><a href="#具体内容-2" class="headerlink" title="具体内容"></a>具体内容</h4><p>视觉语言预训练（VLP）模型过去只可以单独做好理解任务或者是生成任务。而且，性能的提高大部分来自于从web上收集的大量的有噪声的数据。</p>
<p>BLIP提出了多模态混合编码-解码器，可以在三个功能中选择一个运行：</p>
<ul>
<li>单模态编码器</li>
<li>基于图像的文本编码器</li>
<li>基于图像的文本解码器</li>
</ul>
<p>以此来解决原来的模型无法生成任务的问题。</p>
<p>Image Encoder：VIT</p>
<p>Text Encoder&#x2F;Decoder：3种不同的用来算ITC、ITM（前两块比起ALBEF只是共享了self-attention参数）、Language Modeling（Decoder用的是Causal Self-Att做因果推理来完成生成任务）</p>
<p>文本端要做3次Forward，训练效率也偏慢。</p>
<img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250512120226522.png" srcset="/img/loading.gif" lazyload alt="BLIP-Overview" style="zoom:80%;" />

<p>特殊之处：由于BLIP和ALBEF是同一个团队提出的，实际上训练细节差不多。而BLIP也致力于解决网络数据集噪声太多影响训练的情况，提出了Captioner Filter Model。</p>
<p>把训练好的BLIP模型前两部分拿出来，在无噪声数据集上微调后为Filter，再用微调后的Decoder生成的文本做Captioner补充新的数据集，过滤掉相对不匹配的提高图像文本对的质量。得到的图像文本对再返回去训练第二个BLIP模型。</p>
<img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250512121417258.png" srcset="/img/loading.gif" lazyload alt="Cap Filter Model" style="zoom:80%;" />

<img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250512122146514.png" srcset="/img/loading.gif" lazyload alt="eg" style="zoom:80%;" />

<h3 id="CoCa-Contrastive-Captioners-are-Image-Text-Foundation-Models"><a href="#CoCa-Contrastive-Captioners-are-Image-Text-Foundation-Models" class="headerlink" title="CoCa: Contrastive Captioners are Image-Text Foundation Models"></a>CoCa: Contrastive Captioners are Image-Text Foundation Models</h3><h4 id="方法总结-3"><a href="#方法总结-3" class="headerlink" title="方法总结"></a>方法总结</h4><ul>
<li><p>优势：训练较快，效果非常好，能适应不同任务。</p>
</li>
<li><p>劣势：规模史无前例，个人想训练几乎不可能。</p>
</li>
<li><p>可学习之处：对速度的优化，Pooling的更改。</p>
</li>
</ul>
<h4 id="具体内容-3"><a href="#具体内容-3" class="headerlink" title="具体内容"></a>具体内容</h4><p>顾名思义，用Contrastive loss 和 Captioning loss训练出来。Image Encoder和Text Encoder用Contrastive Loss来训练，而最后的多模态用的是Captioning Loss，也就是GPT用的Loss。</p>
<p><img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250513164612673.png" srcset="/img/loading.gif" lazyload alt="CoCa Overview"></p>
<p>跟ALBEF的区别是用了attentional pooling来学习不同任务的特征，放弃了ITM Loss。另外文本特征端用的都是Decoder，这样一开始就是mask的，可以只做一次forward减少计算量，但是代价是用了巨大的数据集，非常多的参数量，scale远超之前所有的工作。</p>
<img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250513165158262.png" srcset="/img/loading.gif" lazyload alt="比例取巧了，但是很吓人" style="zoom:50%;" />



<h3 id="BieT-3：Image-as-a-Foreign-Language-BEiT-Pretraining-for-All-Vision-and-Vision-Language-Tasks"><a href="#BieT-3：Image-as-a-Foreign-Language-BEiT-Pretraining-for-All-Vision-and-Vision-Language-Tasks" class="headerlink" title="BieT-3：Image as a Foreign Language: BEiT Pretraining for All Vision and Vision-Language Tasks"></a>BieT-3：Image as a Foreign Language: BEiT Pretraining for All Vision and Vision-Language Tasks</h3><h4 id="方法总结-4"><a href="#方法总结-4" class="headerlink" title="方法总结"></a>方法总结</h4><ul>
<li>优势：没有像CoCa一样使用过多数据且均为Public，应用广泛。</li>
<li>可学习之处：损失函数设计优化</li>
</ul>
<h4 id="具体内容-4"><a href="#具体内容-4" class="headerlink" title="具体内容"></a>具体内容</h4><p>BieT的核心就是把所有图片化成Imagelish，和Text一起做Mask Modeling当做唯一的损失。</p>
<p>Pre-training部分和他们之前的工作VLMo是完全一致的MOE，只是换了个名字。</p>
<img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250513170238887.png" srcset="/img/loading.gif" lazyload alt="Pre-training" style="zoom: 67%;" />

<p>BEIT-3展示了它的不同模块可以完成不同任务，并且也用了上面CoCa的类似性能展示图，又叠了一层。。。</p>
<img src="https://pleinielune.oss-cn-guangzhou.aliyuncs.com/20250513170356130.png" srcset="/img/loading.gif" lazyload alt="BEiT Overview" style="zoom: 67%;" />

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>从Transformer到不同多模态工作</div>
      <div>http://pleinelune-r.github.io/2025/05/10/Transformer与多模态/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>Pleinelune</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2025年5月10日</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>许可协议</div>
          <div>
            
              
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - 署名">
                    <i class="iconfont icon-cc-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2025/04/15/ModelCompare/" title="Segment Model Compare">
                        <span class="hidden-mobile">Segment Model Compare</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
  
  
    <article id="comments" lazyload>
      
  <div id="valine"></div>
  <script type="text/javascript">
    Fluid.utils.loadComments('#valine', function() {
      Fluid.utils.createScript('https://lib.baomitu.com/valine/1.5.1/Valine.min.js', function() {
        var options = Object.assign(
          {"appId":"NNENV0fwrFl3uHeK9BQlpNqY-gzGzoHsz","appKey":"G5p10JwJML1aH0T56ErRyozZ","path":"window.location.pathname","placeholder":"say something...","avatar":"retro","meta":["nick","mail","link"],"requiredFields":[],"pageSize":10,"lang":"zh-CN","highlight":false,"recordIP":false,"serverURLs":"https://nnenv0fw.lc-cn-n1-shared.com","emojiCDN":true,"emojiMaps":true,"enableQQ":true},
          {
            el: "#valine",
            path: window.location.pathname
          }
        )
        new Valine(options);
        Fluid.utils.waitElementVisible('#valine .vcontent', () => {
          var imgSelector = '#valine .vcontent img:not(.vemoji)';
          Fluid.plugins.imageCaption(imgSelector);
          Fluid.plugins.fancyBox(imgSelector);
        })
      });
    });
  </script>
  <noscript>Please enable JavaScript to view the comments</noscript>


    </article>
  


          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>目录</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  







    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>River</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Lune</span></a> 
    </div>
  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/5.0.0/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  <script  src="/js/local-search.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
</body>
</html>
